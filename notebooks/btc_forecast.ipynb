{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wAiMuX5g-zPD"
      },
      "outputs": [],
      "source": [
        "# dataops src\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "import yfinance as yf\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# etl src\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# model src\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# evaluation src\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# prediction src\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# --------------------------------\n",
        "\n",
        "# etl univariate scripts\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import yfinance as yf\n",
        "\n",
        "# etl multivariate scripts\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import yfinance as yf\n",
        "\n",
        "# multivariate current day pred scripts\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import yfinance as yf\n",
        "\n",
        "# multivariate full pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import yfinance as yf\n",
        "\n",
        "# multivariate train pred\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SRC"
      ],
      "metadata": {
        "id": "1jiSDcnbNj0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assets"
      ],
      "metadata": {
        "id": "jDNVO_NrORi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Global variables for window and horizon size\n",
        "WINDOW_SIZE_WEEK = 7\n",
        "WINDOW_SIZE_MONTH = 30\n",
        "WINDOW_SIZE_2_MONTH = 60\n",
        "\n",
        "HORIZON_DAY = 1\n",
        "HORIZON_WEEK = 7\n",
        "HORIZON_MONTH = 30\n",
        "\n",
        "# Timesteps into the future\n",
        "INTO_FUTURE_1_DAY = 1\n",
        "INTO_FUTURE_WEEK = 7\n",
        "INTO_FUTURE_2_WEEK = 14\n",
        "INTO_FUTURE_MONTH = 30\n",
        "INTO_FUTURE_2_MONTH = 60\n",
        "\n",
        "# Block reward values\n",
        "BLOCK_REWARD_1 = 50 # After 3 January 2009 (2009-01-03) - this block reward isn't accounted for in BTC prices\n",
        "BLOCK_REWARD_2 = 25 # After 28 November 2012\n",
        "BLOCK_REWARD_3 = 12.5 # After 9 July 2016\n",
        "BLOCK_REWARD_4 = 6.25 # After 11 May 2020\n",
        "BLOCK_REWARD_5 = 3.125 # After 20 April 2024\n",
        "\n",
        "# Block reward dates (datetime form of the above date stamps for block rewards)\n",
        "BLOCK_REWARD_1_DATETIME = pd.Timestamp('2009-01-03')\n",
        "BLOCK_REWARD_2_DATETIME = pd.Timestamp('2012-11-28')\n",
        "BLOCK_REWARD_3_DATETIME = pd.Timestamp('2016-07-09')\n",
        "BLOCK_REWARD_4_DATETIME = pd.Timestamp('2020-05-11')\n",
        "BLOCK_REWARD_5_DATETIME = pd.Timestamp('2024-04-20')\n",
        "\n",
        "# Batch sizes\n",
        "NBEATS_BATCH_SIZE = 1024\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# N-BEATS hyperparameters\n",
        "# Values from N-BEATS paper in Figure 1 and Table 18/Appendix D\n",
        "NBEATS_N_EPOCHS = 100\n",
        "NBEATS_N_NEURONS = 512\n",
        "NBEATS_N_LAYERS = 4\n",
        "NBEATS_N_STACKS = 30\n",
        "\n",
        "NBEATS_INPUT_SIZE = WINDOW_SIZE_WEEK * HORIZON_DAY # Lookback period in Appendix D\n",
        "NBEATS_THETA_SIZE = NBEATS_INPUT_SIZE + HORIZON_DAY\n",
        "\n",
        "# Ensemble models training hyperparameters\n",
        "ENSEMBLE_NUM_ITER = 10\n",
        "ENSEMBLE_NUM_ITER_TEST = 1\n",
        "ENSEMBLE_NUM_EPOCHS = 300\n",
        "ENSEMBLE_NUM_EPOCHS_TEST = 1"
      ],
      "metadata": {
        "id": "1IMCXmxWOTMQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataOps"
      ],
      "metadata": {
        "id": "Xo0X49XpG7X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Live training dataset loading\n",
        "\n",
        "# Function to generate historical training dataset\n",
        "def get_historical_btc_data(period_years=10, interval_days=1):\n",
        "  \"\"\"\n",
        "  Returns historical BTC data using a period in years and interval in days\n",
        "  Parameters\n",
        "  ----------\n",
        "  period_years: Period in years\n",
        "  interval_days: Interval in days\n",
        "  \"\"\"\n",
        "  df_btc_price = yf.download(tickers='BTC-USD', period=f'{period_years}y', interval=f'{interval_days}d')\n",
        "  print(f\"Head of BTC prices: \\n{df_btc_price.head()}\")\n",
        "  print(f\"BTC prices info: \\n{df_btc_price.info()}\")\n",
        "\n",
        "  # Only looking for the closing price for each record\n",
        "  df_btc_price_closing = pd.DataFrame(df_btc_price['Close']).rename(columns={'Close': 'Price'})\n",
        "  print(df_btc_price_closing.head())\n",
        "\n",
        "  return df_btc_price_closing\n",
        "\n",
        "# Function to generate future dates for prediction data\n",
        "def get_future_dates(start_date, into_future, offset=1):\n",
        "    \"\"\"\n",
        "    Returns array of datetime values ranging from start_date to start_date +\n",
        "    into_future (horizon).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_date: date to start range (np.datetime64).\n",
        "    into_future: number of days to add onto start date for range (int).\n",
        "    offset: number of days to offset start_date by (default = 1).\n",
        "    \"\"\"\n",
        "    start_date = start_date + np.timedelta64(offset, \"D\")\n",
        "    end_date = start_date + np.timedelta64(into_future, \"D\")\n",
        "\n",
        "    return np.arange(start_date, end_date, dtype=\"datetime64[D]\")"
      ],
      "metadata": {
        "id": "1jsPIrszBAlo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETL"
      ],
      "metadata": {
        "id": "oDSIFAyyOLN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to include block reward to training and testing dataset before windowing is in effect\n",
        "def include_block_reward(df_btc_price):\n",
        "    \"\"\"\n",
        "    Includes block reward to an existing dataframe containing BTC prices with\n",
        "    datetime indexes. Should be used before windowing of dataset is in effect.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_btc_price: DataFrame containing BTC prices with datetime indexes.\n",
        "    \"\"\"\n",
        "    # TODO: Need to change this function when block reward changes in the future and also need to better manage block reward for the future\n",
        "\n",
        "    block_reward_1_days = (BLOCK_REWARD_2_DATETIME - df_btc_price.index[0]).days\n",
        "    block_reward_2_days = (BLOCK_REWARD_3_DATETIME - df_btc_price.index[0]).days\n",
        "    block_reward_3_days = (BLOCK_REWARD_4_DATETIME - df_btc_price.index[0]).days\n",
        "    block_reward_4_days = (df_btc_price.index[-1] - BLOCK_REWARD_4_DATETIME).days\n",
        "\n",
        "    print(f\"Days from block reward 1: {block_reward_1_days}\\n\",\n",
        "          f\"Days from block reward 2: {block_reward_2_days}\\n\",\n",
        "          f\"Days from block reward 3: {block_reward_3_days}\\n\",\n",
        "          f\"Days from block reward 4: {block_reward_4_days}\\n\")\n",
        "\n",
        "    # Adding the block_reward column\n",
        "    df_btc_price_block_reward = df_btc_price.copy()\n",
        "    df_btc_price_block_reward[\"block_reward\"] = None\n",
        "\n",
        "    # Set values of block_reward column (it's the last column here hence -1 indexing using iloc)\n",
        "    df_btc_price_block_reward.iloc[:block_reward_2_days, -1] = BLOCK_REWARD_2\n",
        "    df_btc_price_block_reward.iloc[block_reward_2_days:block_reward_3_days, -1] = BLOCK_REWARD_3\n",
        "    df_btc_price_block_reward.iloc[block_reward_3_days:, -1] = BLOCK_REWARD_4\n",
        "\n",
        "    print(\"Head of dataframe with block reward included: \\n\", df_btc_price_block_reward.head())\n",
        "    print(\"Tail of dataframe with block reward included: \\n\", df_btc_price_block_reward.tail())\n",
        "\n",
        "    return df_btc_price_block_reward\n",
        "\n",
        "def get_labelled_windows(x, horizon=HORIZON_DAY):\n",
        "    \"\"\"\n",
        "    Create labels for windowed dataset.\n",
        "\n",
        "    E.g. if horizon is 1, then:\n",
        "    x: [0, 1, 2, 3, 4, 5, 6, 7] -> output: ([0, 1, 2, 3, 4, 5, 6], [7])\n",
        "    \"\"\"\n",
        "    return x[:, :-horizon], x[:, -horizon:]\n",
        "\n",
        "# View numpy arrays as windows\n",
        "def make_windows(x, window_size=WINDOW_SIZE_WEEK, horizon=HORIZON_DAY):\n",
        "    \"\"\"\n",
        "    Turns a 1D array into a 2D array of sequential labelled windows of\n",
        "    window_size with horizon size labels.\n",
        "\n",
        "    Returns both a 2D array containing full windowed X values with shape\n",
        "    (number of samples, window size), and a 2D array containing full\n",
        "    labelled y values with shape (number of samples, horizon size).\n",
        "    \"\"\"\n",
        "    # TODO: In the future, function could be implemented using https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array\n",
        "\n",
        "    # 1. Create a window of specific window_size (add the horizon on the end for labelling later)\n",
        "    window_step = np.expand_dims(np.arange(window_size + horizon), axis=0)\n",
        "\n",
        "    # 2. Create a 2D of multiple window steps (minus 1 to account for 0 indexing)\n",
        "    window_indexes = window_step + np.expand_dims(np.arange(len(x) - (window_size + horizon - 1)), axis=0).T # Create 2D array of windows of window size\n",
        "    # print(f\"Window indexes: \\n {window_indexes, window_indexes.shape}\")\n",
        "\n",
        "    windowed_array = x[window_indexes]\n",
        "    # print(windowed_array)\n",
        "\n",
        "    # 4. Get the labelled window\n",
        "    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "    return windows, labels\n",
        "\n",
        "# Windowing function for block reward (multivariate dataset)\n",
        "def make_windows_labels_multivariate(df_btc_price_block_reward, window=WINDOW_SIZE_WEEK):\n",
        "    \"\"\"\n",
        "    Returns windows and labels using a dataframe containing both BTC prices and\n",
        "    block reward.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_btc_price_block_reward: DataFrame containing BTC prices and block reward.\n",
        "    window: Window size.\n",
        "    \"\"\"\n",
        "    # Make a copy of the BTC historical data with block reward feature\n",
        "    df_btc_price_windowed = df_btc_price_block_reward.copy()\n",
        "\n",
        "    # Add windowed columns\n",
        "    for i in range(window):\n",
        "        df_btc_price_windowed[f\"Price{i + 1}\"] = df_btc_price_windowed[\"Price\"].shift(periods=i + 1)\n",
        "\n",
        "    print(\"Head of windowed dataframe: \\n\", df_btc_price_windowed.head(10))\n",
        "\n",
        "    # Create X & y, remove the NaNs and convert to float32 to prevent tensorflow errors\n",
        "    X = df_btc_price_windowed.dropna().drop('Price', axis=1).astype(np.float32)\n",
        "    y = df_btc_price_windowed.dropna()['Price'].astype(np.float32)\n",
        "\n",
        "    print(\"Head of X: \\n\", X.head())\n",
        "    print(\"Head of y: \\n\", y.head())\n",
        "    print(f\"Shape of X: {X.shape} \\n\", f\"Shape of y: {y.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Implementation of train/test split after windowing is in effect\n",
        "def make_train_test_splits(windows, labels, test_split=0.2):\n",
        "    \"\"\"\n",
        "    Splits matching pairs of windows and labels into train and test splits.\n",
        "    \"\"\"\n",
        "    split_size = int(len(windows) * (1 - test_split)) # This will default to 80% train and 20% test\n",
        "    train_windows = windows[:split_size]\n",
        "    train_labels = labels[:split_size]\n",
        "    test_windows = windows[split_size:]\n",
        "    test_labels = labels[split_size:]\n",
        "\n",
        "    print(train_windows.shape, test_windows.shape, train_labels.shape, test_labels.shape)\n",
        "\n",
        "    return train_windows, test_windows, train_labels, test_labels\n",
        "\n",
        "# Function to generate train and test datasets using the tf.data API\n",
        "def gen_train_test_datasets(X_train, y_train, X_test, y_test, batch_size=BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Returns the train and test datasets using the tf.data API from train and\n",
        "    test arrays.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train: Training windows.\n",
        "    y_train: Training labels.\n",
        "    X_test: Testing windows.\n",
        "    y_test: Testing labels.\n",
        "    batch_size: Batch size used for training.\n",
        "    \"\"\"\n",
        "    # 1. Turn train and test arrays into tensor Datasets\n",
        "    train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "    train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "\n",
        "    test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "    test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "    # 2. Combine features & labels\n",
        "    train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
        "    test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
        "\n",
        "    # 3. Batch and prefetch for optimal performance\n",
        "    train_dataset = train_dataset.batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_dataset = test_dataset.batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def save_train_test_data(train_windowed, train_labels, test_windowed, test_labels, features, save_path=None):\n",
        "    \"\"\"\n",
        "    Saves the train and test datasets to save_path.\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "    now_date_time = now.strftime('%d_%m_%Y_%H_%M_%S')\n",
        "\n",
        "    df_train_windows = pd.DataFrame(train_windowed, columns=features)\n",
        "    df_train_labels = pd.DataFrame(train_labels)\n",
        "    df_test_windows = pd.DataFrame(test_windowed, columns=features)\n",
        "    df_test_labels = pd.DataFrame(test_labels)\n",
        "\n",
        "    if save_path == None:\n",
        "        df_train_windows.to_csv(path_or_buf=os.path.join(\"train_windowed\" + \"_\" + now_date_time + \".csv\"))\n",
        "        df_train_labels.to_csv(path_or_buf=os.path.join(\"train_labels\" + \"_\" + now_date_time + \".csv\"))\n",
        "        df_test_windows.to_csv(path_or_buf=os.path.join(\"test_windowed\" + \"_\" + now_date_time + \".csv\"))\n",
        "        df_test_labels.to_csv(path_or_buf=os.path.join(\"test_labels\" + \"_\" + now_date_time + \".csv\"))\n",
        "    else:\n",
        "        df_train_windows.to_csv(path_or_buf=os.path.join(save_path, \"train_windowed\" + \"_\" + now_date_time + \".csv\"))\n",
        "        df_train_labels.to_csv(path_or_buf=os.path.join(save_path, \"train_labels\" + \"_\" + now_date_time + \".csv\"))\n",
        "        df_test_windows.to_csv(path_or_buf=os.path.join(save_path, \"test_windowed\" + \"_\" + now_date_time + \".csv\"))\n",
        "        df_test_labels.to_csv(path_or_buf=os.path.join(save_path, \"test_labels\" + \"_\" + now_date_time + \".csv\"))"
      ],
      "metadata": {
        "id": "OlEyMjptOMhp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "oPwITzppP-mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Modelling checkpoint\n",
        "def create_model_checkpoint(model_name, save_path=\"btc_predict_model\"):\n",
        "    \"\"\"\n",
        "    Uses the model_name passed and saves model and weights to save_path using\n",
        "    datetime.now().\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "    now_date_time = now.strftime('%d_%m_%Y_%H_%M_%S')\n",
        "    return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name + \"_\" + now_date_time),\n",
        "                                              verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "class ARModel(tf.keras.Model):\n",
        "    def __init__(self, shape):\n",
        "        \"\"\"\n",
        "        Initializes Autoregressive model using below parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape: Tuple of (N, M) where N is the number of data points\n",
        "        and M is the window size.\n",
        "        model_name: Model's name.\n",
        "        \"\"\"\n",
        "        super(ARModel, self).__init__()\n",
        "        in_shape = shape[0]\n",
        "        out_shape = shape[1]\n",
        "        w_init = tf.random.normal(shape, mean=0, stddev=0.01, dtype='float32')\n",
        "        self.w = tf.Variable(w_init, name='ar_w')\n",
        "\n",
        "    def call(self, input_X, training=False):\n",
        "        return tf.matmul(input_X, self.w) + tf.reduce_sum(self.w, axis=0)\n",
        "\n",
        "def get_AR_model(X_shape, y_shape):\n",
        "    layer_size = 30\n",
        "    input_0 = tf.keras.Input(shape=X_shape)\n",
        "    output_0 = ARModel((X_shape, layer_size))(input_0)\n",
        "    output_1 = ARModel((layer_size, y_shape))(output_0)\n",
        "    AR_model = tf.keras.Model(input_0, output_1)\n",
        "\n",
        "    return AR_model\n",
        "\n",
        "# Create N-BEATS custom layer\n",
        "class NBEATSBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size: int, theta_size: int, horizon: int, n_neurons: int, n_layers: int, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes NEATSBlock class using below parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_size: Size of backcast (window size).\n",
        "        theta_size: Number of neurons in output dense layer, backcast + forecast.\n",
        "        horizon: Size of horizon, forecast prediction.\n",
        "        n_neurons: Number of neurons in fully connected dense layer.\n",
        "        n_layers: Number of fully connected dense layers.\n",
        "        kwargs: Passed to tf.keras.layers.Layers\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_size = input_size\n",
        "        self.theta_size = theta_size\n",
        "        self.horizon = horizon\n",
        "        self.n_neurons = n_neurons\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # One block contains a stack of 4 fully connected dense layers, each has a ReLU activation function\n",
        "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for layer in range(n_layers)]\n",
        "\n",
        "        # Output of block is a dense layer of theta_size with linear activation\n",
        "        self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\")\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        Runs when the layer is called.\n",
        "        \"\"\"\n",
        "        x = inputs\n",
        "        for layer in self.hidden: # pass inputs through each hidden layer\n",
        "            x = layer(x)\n",
        "        theta = self.theta_layer(x)\n",
        "\n",
        "        # Output the backcast and forecast from theta\n",
        "        backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "def get_NBEATS_model(model_name=\"NBEATS_model\"):\n",
        "        \"\"\"\n",
        "        Returns a NBEATS model using the Functional API.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name: Model's name.\n",
        "        \"\"\"\n",
        "        # TODO: Adjust this function to take into consideration if more features are used for the model, if more features are\n",
        "        # used, then the NBEATS_INPUT_SIZE + 1 will need to change.\n",
        "        # 1. Setup NBEATSBlock layer\n",
        "        nbeats_block_layer = NBEATSBlock(input_size=NBEATS_INPUT_SIZE + 1, theta_size=NBEATS_THETA_SIZE, horizon=HORIZON_DAY,\n",
        "                                         n_neurons=NBEATS_N_NEURONS, n_layers=NBEATS_N_LAYERS, name=\"initial_block\")\n",
        "\n",
        "        # 2. Create input to stacks\n",
        "        stack_input = layers.Input(shape=(NBEATS_INPUT_SIZE + 1), name=\"stack_input\")\n",
        "\n",
        "        # 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\n",
        "        backcast, forecast = nbeats_block_layer(stack_input)\n",
        "        # Add in subtraction residual connections\n",
        "        residuals = layers.subtract([stack_input, backcast], name=f\"subtract_00\")\n",
        "\n",
        "        # 4. Create stacks of blocks\n",
        "        for i, _ in enumerate(range(NBEATS_N_STACKS - 1)):\n",
        "            # First stack is already created in # 3\n",
        "            # 5. Use NBEATSBlock to calculate the backcast as well as block's forecast\n",
        "            backcast, block_forecast = NBEATSBlock(\n",
        "                input_size=NBEATS_INPUT_SIZE + 1,\n",
        "                theta_size=NBEATS_THETA_SIZE,\n",
        "                horizon=HORIZON_DAY,\n",
        "                n_neurons=NBEATS_N_NEURONS,\n",
        "                n_layers=NBEATS_N_LAYERS,\n",
        "                name=f\"NBEATSBlock_{i}\"\n",
        "            )(residuals) # Pass in the residuals (the backcast)\n",
        "\n",
        "            # 6. Create the double residual stacking\n",
        "            residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\")\n",
        "            forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
        "\n",
        "        # 7. Put the stack model together\n",
        "        NBEATS_model = tf.keras.Model(inputs=stack_input, outputs=forecast, name=model_name)\n",
        "\n",
        "        return NBEATS_model\n",
        "\n",
        "# Implement a layer normalization to normalize across the features dimension\n",
        "# Also helps with unstable gradients\n",
        "class LNRNNCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes a Layer Normalization RNN cell using below parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        units: Dimensionality of the output space of the SimpleRNNCell.\n",
        "        activation: Activation function to use.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.rnn_cell = layers.SimpleRNNCell(units, activation=None)\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def __call__(self, inputs, states, training=True):\n",
        "        \"\"\"\n",
        "        Runs when layer is called.\n",
        "        \"\"\"\n",
        "        outputs, new_states = self.rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "\n",
        "        return norm_outputs, [norm_outputs]\n",
        "\n",
        "def get_LNRNN_model(window=WINDOW_SIZE_WEEK, horizon=HORIZON_DAY, model_name=\"LNRNN_model\"):\n",
        "    \"\"\"\n",
        "    Returns a LNRNN model using the LNRNNCell implementation, with the\n",
        "    Sequential API.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window: Window size.\n",
        "    horizon: Horizon size.\n",
        "    model_name: Model's name.\n",
        "    \"\"\"\n",
        "    # TODO: Adjust this function to take into consideration if more features are used for the model, if more features are\n",
        "    # used, then the window + 1 will need to change.\n",
        "    LNRNN_model = tf.keras.Sequential([\n",
        "        layers.Lambda(lambda x: tf.expand_dims(x, axis=1)),\n",
        "        layers.RNN(LNRNNCell(units=horizon), return_sequences=False, input_shape=[None, window + 1]),\n",
        "        layers.Dense(horizon)\n",
        "    ], name=model_name)\n",
        "\n",
        "    return LNRNN_model\n",
        "\n",
        "class LSTMModel():\n",
        "    def __init__(self, window=WINDOW_SIZE_WEEK, br=True, horizon=HORIZON_DAY, n_lstm_neurons=128):\n",
        "        \"\"\"\n",
        "        Initializes a LSTMModel class using below parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        window: Window size.\n",
        "        horizon: Horizon size.\n",
        "        n_lstm_neurons: Number of neurons to be used in LSTM layer.\n",
        "        \"\"\"\n",
        "        # TODO: Adjust this function to take into consideration if more features are used for the model, if more features are\n",
        "        # used, then the window + 1 will need to change.\n",
        "        if br == True:\n",
        "            self.inputs_layer = layers.Input(shape=(window + 1))\n",
        "        else:\n",
        "            self.inputs_layer = layers.Input(shape=(window))\n",
        "        self.lambda_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))\n",
        "        self.lstm_layer = layers.LSTM(n_lstm_neurons, activation=\"relu\")\n",
        "        self.outputs_layer = layers.Dense(horizon)\n",
        "\n",
        "    def get_model(self, model_name=\"LSTM_model\"):\n",
        "        \"\"\"\n",
        "        Returns a LSTM model using the Functional API, utilizing the\n",
        "        inputs_layer and outputs_layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name: Model's name.\n",
        "        \"\"\"\n",
        "        inputs = self.inputs_layer\n",
        "        x = self.lambda_layer(inputs)\n",
        "        x = self.lstm_layer(x)\n",
        "        outputs = self.outputs_layer(x)\n",
        "\n",
        "        lstm_model = tf.keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
        "\n",
        "        return lstm_model\n",
        "\n",
        "class DenseModel():\n",
        "    def __init__(self, n_neurons=128, horizon=HORIZON_DAY):\n",
        "        \"\"\"\n",
        "        Initializes a DenseModel class using below parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_neurons: Number of neurons in dense layers.\n",
        "        horizon: Horizon size.\n",
        "        \"\"\"\n",
        "        self.n_neurons = n_neurons\n",
        "        self.horizon = horizon\n",
        "\n",
        "    def get_model(self, model_name=\"dense_model\"):\n",
        "        \"\"\"\n",
        "        Returns the dense model, using the Sequential API.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name: Model's name\n",
        "        \"\"\"\n",
        "        dense_model = tf.keras.Sequential([\n",
        "            layers.Dense(self.n_neurons, activation=\"relu\"),\n",
        "            layers.Dense(self.horizon, activation=\"relu\")\n",
        "        ], name=model_name)\n",
        "\n",
        "        return dense_model\n",
        "\n",
        "class WaveNet():\n",
        "    def __init__(self):\n",
        "        self.model = keras.models.Sequential()\n",
        "        self.model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "\n",
        "        for rate in (1, 2, 4, 8) * 2:\n",
        "            # Define a spacing between the values in a kernel\n",
        "            # 3x3 will be similar to 5x5 with a spacing of 2\n",
        "            self.model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=rate))\n",
        "\n",
        "        self.model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "# Generate an ensemble of models using various loss functions\n",
        "def get_ensemble_models(models, train_data, test_data, horizon=HORIZON_DAY, num_iter=10, num_epochs=100,\n",
        "                        loss_funcs=[\"mae\", \"mse\", \"mape\"]):\n",
        "    \"\"\"\n",
        "    Returns a list of num_iter models each trained on MAE, MSE and MAPE loss\n",
        "    functions by default.\n",
        "\n",
        "    For instance, if num_iter = 10, a list of 30 trained models will be returned.\n",
        "    10 * len(loss_funcs) * 3 = 90\n",
        "    \"\"\"\n",
        "    # Make empty list for trained ensemble models\n",
        "    ensemble_models = []\n",
        "\n",
        "    # Create num_iter number of models per loss function for NBeats, LSTM and Dense models\n",
        "    for i in range(num_iter):\n",
        "        # Build and fit ensemble of models\n",
        "        for model in models:\n",
        "            for loss_func in loss_funcs:\n",
        "                print(f\"Optimizing model by reducing: {loss_func} for epochs: {num_epochs}, num_iter: {i}, model: {model.name}\")\n",
        "\n",
        "                # Compile model with current loss function\n",
        "                model.compile(loss=loss_func, optimizer=tf.keras.optimizers.Adam(), metrics=[\"mae\", \"mse\"])\n",
        "\n",
        "                # Fit the model\n",
        "                model.fit(train_data, epochs=num_epochs, verbose=2, validation_data=test_data,\n",
        "                          callbacks=[#create_model_checkpoint(model_name=f\"{model.name}_{i}_{loss_func}\"),\n",
        "                                     tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),\n",
        "                                     tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)])\n",
        "\n",
        "                # Append fitted model to list of ensemble models\n",
        "                ensemble_models.append(model)\n",
        "\n",
        "    return ensemble_models\n",
        "\n",
        "def make_ensemble_preds(ensemble_models, input_data):\n",
        "    \"\"\"\n",
        "    Returns predictions of ensemble models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ensemble_models: Trained ensemble of models.\n",
        "    input_data: Data to be predicted with.\n",
        "    \"\"\"\n",
        "    ensemble_preds = []\n",
        "\n",
        "    for model in ensemble_models:\n",
        "        preds = model.predict(input_data) # Make predictions with current ensemble model\n",
        "        ensemble_preds.append(preds)\n",
        "\n",
        "    return tf.constant(tf.squeeze(ensemble_preds))\n",
        "\n",
        "def make_preds(model, input_data):\n",
        "    \"\"\"\n",
        "    Uses model to make predictions on input_data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : trained model\n",
        "    input_data : windowed input data (same kind of data model was trained on)\n",
        "\n",
        "    Returns model predictions on input_data.\n",
        "    \"\"\"\n",
        "    forecast = model.predict(input_data)\n",
        "    return tf.squeeze(forecast) # return 1D array of predictions\n",
        "\n",
        "def get_ensemble_models_summary(models):\n",
        "    \"\"\"\n",
        "    Generates model summaries of ensemble models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models: Ensemble of models.\n",
        "    \"\"\"\n",
        "    for model in models:\n",
        "        print(model.summary())\n",
        "\n",
        "def auto_regressive_model(window_size=WINDOW_SIZE_WEEK, lr=0.1):\n",
        "    model = nn.Linear(window_size, 1)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    return model, criterion, optimizer\n",
        "\n",
        "class RNNModel_torch(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n",
        "        super(RNNModel_torch, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_rnnlayers = n_rnnlayers\n",
        "\n",
        "        self.rnn = nn.RNN(\n",
        "                input_size=self.n_inputs,\n",
        "                hidden_size=self.n_hidden,\n",
        "                num_layers=self.n_rnnlayers,\n",
        "                nonlinearity=\"relu\",\n",
        "                batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(self.n_hidden, self.n_outputs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_state = torch.zeros(self.n_rnnlayers, inputs.size(0), self.n_hidden).to(device)\n",
        "\n",
        "        out, time_step_val = self.rnn(inputs, hidden_state)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "class LSTMModel_torch(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n",
        "        super(LSTMModel_torch, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_rnnlayers = n_rnnlayers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.n_inputs,\n",
        "                          hidden_size=self.n_hidden,\n",
        "                          num_layers=self.n_rnnlayers,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(self.n_hidden, self.n_outputs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_state = torch.zeros(self.n_rnnlayers, inputs.size(0), self.n_hidden).to(device)\n",
        "        cell_state = torch.zeros(self.n_rnnlayers, inputs.size(0), self.n_hidden).to(device)\n",
        "\n",
        "        out, time_step_val = self.lstm(inputs, (hidden_state, cell_state))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "def BGD_training(model, criterion, optimizer, train_windows, train_labels, test_windows, test_labels, epochs=200):\n",
        "    train_losses = np.zeros(epochs)\n",
        "    test_losses = np.zeros(epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(train_windows)\n",
        "        loss = criterion(outputs, train_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses[epoch] = loss.item()\n",
        "\n",
        "        test_outputs = model(test_windows)\n",
        "        test_loss = criterion(test_outputs, test_labels)\n",
        "\n",
        "        test_losses[epoch] = test_loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch + 1} / {epochs}, test loss: {test_loss.item()}\")\n",
        "\n",
        "    return train_losses, test_losses"
      ],
      "metadata": {
        "id": "XaLxiuFaQAS_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "t-7WOrGDQkuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_scaled_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Implement MASE (assuming to seasonality of data). MASE measures the\n",
        "    forecast error compared to the error of a naive forecast. When he have a\n",
        "    MASE = 1, that means the model is exactly as good as just picking the last\n",
        "    observation. An MASE = 0.5, means that our model has doubled the prediction\n",
        "    accuracy.\n",
        "    \"\"\"\n",
        "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
        "\n",
        "    # Find MAE of naive forecast (no seasonality)\n",
        "    mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # the seasonality is 1 day (hence the shifting of 1 day)\n",
        "\n",
        "    return mae / mae_naive_no_season\n",
        "\n",
        "# Updated evaluate_preds for large horizons (greater than 1 day)\n",
        "def evaluate_preds(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate mae, mse, rmse, mape, mase metrics for predictions. Updated\n",
        "    for large horizons as well (greater than 1 day)\n",
        "    \"\"\"\n",
        "    # Make sure float32 (for metric calculations)\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Calculate various metrics\n",
        "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # MSE gives emphasis on outliers, as they're squared\n",
        "    rmse = tf.sqrt(mse)\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "    # Account for different sized horizons, if the horizon is greater than 1 day, reduce it to a single number using the mean\n",
        "    if mae.ndim > 0:\n",
        "        mae = tf.reduce_mean(mae)\n",
        "        mse = tf.reduce_mean(mse)\n",
        "        rmse = tf.reduce_mean(rmse)\n",
        "        mape = tf.reduce_mean(mape)\n",
        "        mase = tf.reduce_mean(mase)\n",
        "\n",
        "    return {\"mae\": mae.numpy(),\n",
        "            \"mse\": mse.numpy(),\n",
        "            \"rmse\": rmse.numpy(),\n",
        "            \"mape\": mape.numpy(),\n",
        "            \"mase\": mase.numpy()}\n",
        "\n",
        "# Create a function to take in model predictions and truth values and return evaluation metrics\n",
        "def evaluate_preds_1_day_horizon(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate mae, mse, rmse, mape, mase metrics for predictions.\n",
        "    \"\"\"\n",
        "    # Make sure float32 (for metric calculations)\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_true = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Calculate various metrics\n",
        "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
        "    rmse = tf.sqrt(mse)\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "    return {\"mae\": mae.numpy(),\n",
        "            \"mse\": mse.numpy(),\n",
        "            \"rmse\": rmse.numpy(),\n",
        "            \"mape\": mape.numpy(),\n",
        "            \"mase\": mase.numpy()}\n",
        "\n",
        "# Comparing the performance of models using different metrics\n",
        "def compare_model_metrics(model_results: list, model_names: list, metrics=['mae', 'mse', 'rmse', 'mape', 'mase']):\n",
        "    \"\"\"\n",
        "    Plots comparison of metrics for models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models : List of dictionaries containing model metrics.\n",
        "    i.e. model_results=[model_1_results, model_2_results]. Where\n",
        "    model_1_results and model_2_results are dictionaries of metrics.\n",
        "\n",
        "    model_names : List of model names.\n",
        "\n",
        "    metrics : Metrics for comparison. Default is ['mae', 'mse', 'rmse', 'mape',\n",
        "    'mase'].\n",
        "    \"\"\"\n",
        "    for metric in metrics:\n",
        "        pd.DataFrame({model_name: model_result[metric] for (model_name, model_result) in zip(model_names, model_results)},\n",
        "                     index=[metric]).plot(figsize=(10, 7), kind='bar')"
      ],
      "metadata": {
        "id": "9-jV8LS3QmEL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "-XtYEMSATx1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_upper_lower_confidence(preds):\n",
        "    \"\"\"\n",
        "    Returns the upper and lower bounds of the 95% confidence level using the\n",
        "    following logic:\n",
        "\n",
        "    95% confidence upper, lower bounds = mean of preds +/- (standard deviation\n",
        "    of preds * 1.96)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    preds: Predictions of ensemble of models.\n",
        "    \"\"\"\n",
        "    # 1. Take the predictions of multiple randomly initialized deep learning neural networks using the preds parameter\n",
        "    # 2. Measure the standard deviation of the predictions\n",
        "    std = tf.math.reduce_std(preds, axis=0)\n",
        "\n",
        "    # 3. Multiply the standard deviation by 1.96\n",
        "    interval = 1.96 * std\n",
        "\n",
        "    # 4. Get the prediction interval's upper and lower bounds\n",
        "    preds_mean = tf.reduce_mean(preds, axis=0)\n",
        "    lower, upper = preds_mean - interval, preds_mean + interval\n",
        "\n",
        "    return lower, upper\n",
        "\n",
        "# Function to make predictions into future (can include retraining the model with the predicted data appended, everytime the model makes a prediction)\n",
        "# 1. Create function to make predictions into the future\n",
        "def make_future_forecast(models, values, into_future, window_size=WINDOW_SIZE_WEEK) -> list:\n",
        "    \"\"\"\n",
        "    Makes future forecasts into_future steps after values ends.\n",
        "\n",
        "    Returns future forecasts as list of floats.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    values: BTC price labels after windowing is in effect.\n",
        "    models: List of ensemble models to predict forecast using.\n",
        "    into_future: How many days into future to make forecast for.\n",
        "    window_size: Window size.\n",
        "    \"\"\"\n",
        "    # 2. Make an empty list for future forecasts/prepare data to forecast on\n",
        "    future_forecast = []\n",
        "    last_window = values[-window_size:] # Only want preds from the last window (this will get updated after every prediction)\n",
        "\n",
        "    # 3. Make into_future number of predictions, altering the data which gets predicted on each time\n",
        "    for day in range(into_future):\n",
        "        # Predict on last window then append it again and again (model will start to make forecasts on its own forecasts)\n",
        "        ensemble_preds = make_ensemble_preds(ensemble_models=models, input_data=tf.expand_dims(last_window, axis=0))\n",
        "        future_pred = np.median(ensemble_preds, axis=0)\n",
        "        print(f\"Predicting on: \\n {last_window} -> Prediction: {tf.squeeze(future_pred).numpy()}\\n\")\n",
        "\n",
        "        # Append predictions on future_forecast\n",
        "        future_forecast.append(tf.squeeze(future_pred).numpy())\n",
        "\n",
        "        # Update last window with the new pred and get window_size of most recent preds (model was trained on window_size windows)\n",
        "        last_window = np.append(last_window, future_pred)[-window_size:]\n",
        "\n",
        "    return future_forecast"
      ],
      "metadata": {
        "id": "bfBtqHJjT0nk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "TiLytFChQz76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETL"
      ],
      "metadata": {
        "id": "X5_tJYfaRHJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate"
      ],
      "metadata": {
        "id": "bTri92AwRIbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_btc_price_closing = get_historical_btc_data()\n",
        "\n",
        "# Take the full dataset's timesteps and closing prices\n",
        "dataset_timesteps = df_btc_price_closing.index.to_numpy()\n",
        "dataset_prices = df_btc_price_closing['Price'].to_numpy()\n",
        "\n",
        "# Testing out the windowing function for multiple records\n",
        "dataset_full_windows, dataset_full_labels = make_windows(dataset_prices, window_size=WINDOW_SIZE_WEEK, horizon=HORIZON_DAY)\n",
        "print(dataset_full_windows.shape, dataset_full_labels.shape)\n",
        "\n",
        "print(pd.DataFrame(dataset_prices))\n",
        "print(pd.DataFrame(dataset_full_windows))\n",
        "print(pd.DataFrame(dataset_full_labels))\n",
        "\n",
        "# Testing out the train/test split function\n",
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(dataset_full_windows, dataset_full_labels)\n",
        "print(train_windows.shape, test_windows.shape, train_labels.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT-IYiQKRGnI",
        "outputId": "a9ea0d2c-d7eb-4041-a097-c9111bfecc7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of BTC prices: \n",
            "                  Open        High         Low       Close   Adj Close  \\\n",
            "Date                                                                     \n",
            "2014-09-17  465.864014  468.174011  452.421997  457.334015  457.334015   \n",
            "2014-09-18  456.859985  456.859985  413.104004  424.440002  424.440002   \n",
            "2014-09-19  424.102997  427.834991  384.532013  394.795990  394.795990   \n",
            "2014-09-20  394.673004  423.295990  389.882996  408.903992  408.903992   \n",
            "2014-09-21  408.084991  412.425995  393.181000  398.821014  398.821014   \n",
            "\n",
            "              Volume  \n",
            "Date                  \n",
            "2014-09-17  21056800  \n",
            "2014-09-18  34483200  \n",
            "2014-09-19  37919700  \n",
            "2014-09-20  36863600  \n",
            "2014-09-21  26580100  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 3549 entries, 2014-09-17 to 2024-06-04\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Open       3549 non-null   float64\n",
            " 1   High       3549 non-null   float64\n",
            " 2   Low        3549 non-null   float64\n",
            " 3   Close      3549 non-null   float64\n",
            " 4   Adj Close  3549 non-null   float64\n",
            " 5   Volume     3549 non-null   int64  \n",
            "dtypes: float64(5), int64(1)\n",
            "memory usage: 194.1 KB\n",
            "BTC prices info: \n",
            "None\n",
            "                 Price\n",
            "Date                  \n",
            "2014-09-17  457.334015\n",
            "2014-09-18  424.440002\n",
            "2014-09-19  394.795990\n",
            "2014-09-20  408.903992\n",
            "2014-09-21  398.821014\n",
            "(3542, 7) (3542, 1)\n",
            "                 0\n",
            "0       457.334015\n",
            "1       424.440002\n",
            "2       394.795990\n",
            "3       408.903992\n",
            "4       398.821014\n",
            "...            ...\n",
            "3544  67491.414062\n",
            "3545  67706.937500\n",
            "3546  67751.601562\n",
            "3547  68804.781250\n",
            "3548  70582.593750\n",
            "\n",
            "[3549 rows x 1 columns]\n",
            "                 0             1             2             3             4  \\\n",
            "0       457.334015    424.440002    394.795990    408.903992    398.821014   \n",
            "1       424.440002    394.795990    408.903992    398.821014    402.152008   \n",
            "2       394.795990    408.903992    398.821014    402.152008    435.790985   \n",
            "3       408.903992    398.821014    402.152008    435.790985    423.204987   \n",
            "4       398.821014    402.152008    435.790985    423.204987    411.574005   \n",
            "...            ...           ...           ...           ...           ...   \n",
            "3537  68526.101562  69265.945312  68518.093750  69394.554688  68296.218750   \n",
            "3538  69265.945312  68518.093750  69394.554688  68296.218750  67578.093750   \n",
            "3539  68518.093750  69394.554688  68296.218750  67578.093750  68364.992188   \n",
            "3540  69394.554688  68296.218750  67578.093750  68364.992188  67491.414062   \n",
            "3541  68296.218750  67578.093750  68364.992188  67491.414062  67706.937500   \n",
            "\n",
            "                 5             6  \n",
            "0       402.152008    435.790985  \n",
            "1       435.790985    423.204987  \n",
            "2       423.204987    411.574005  \n",
            "3       411.574005    404.424988  \n",
            "4       404.424988    399.519989  \n",
            "...            ...           ...  \n",
            "3537  67578.093750  68364.992188  \n",
            "3538  68364.992188  67491.414062  \n",
            "3539  67491.414062  67706.937500  \n",
            "3540  67706.937500  67751.601562  \n",
            "3541  67751.601562  68804.781250  \n",
            "\n",
            "[3542 rows x 7 columns]\n",
            "                 0\n",
            "0       423.204987\n",
            "1       411.574005\n",
            "2       404.424988\n",
            "3       399.519989\n",
            "4       377.181000\n",
            "...            ...\n",
            "3537  67491.414062\n",
            "3538  67706.937500\n",
            "3539  67751.601562\n",
            "3540  68804.781250\n",
            "3541  70582.593750\n",
            "\n",
            "[3542 rows x 1 columns]\n",
            "(2833, 7) (709, 7) (2833, 1) (709, 1)\n",
            "(2833, 7) (709, 7) (2833, 1) (709, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate"
      ],
      "metadata": {
        "id": "Yt7ZbDnmRpWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_btc_price_closing = get_historical_btc_data()\n",
        "\n",
        "# Including block reward to the full BTC closing price dataset\n",
        "df_btc_price_block_reward = include_block_reward(df_btc_price=df_btc_price_closing)\n",
        "\n",
        "# Obtain the full windows and labels with block reward included\n",
        "dataset_full_windows_br, dataset_full_labels_br = make_windows_labels_multivariate(df_btc_price_block_reward)\n",
        "\n",
        "# Obtain the train/test sets of the full windows and labels with block reward included\n",
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=dataset_full_windows_br, labels=dataset_full_labels_br)\n",
        "\n",
        "# Obtain the train and test datasets\n",
        "train_dataset, test_dataset = gen_train_test_datasets(X_train=train_windows, y_train=train_labels,\n",
        "                                                      X_test=test_windows, y_test=test_labels)\n",
        "print(f\"Train dataset: {train_dataset}\", f\"Test dataset: {test_dataset}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D104IgRRq0T",
        "outputId": "278b586a-b473-48b8-b4a8-ce30f602df08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of BTC prices: \n",
            "                  Open        High         Low       Close   Adj Close  \\\n",
            "Date                                                                     \n",
            "2014-09-17  465.864014  468.174011  452.421997  457.334015  457.334015   \n",
            "2014-09-18  456.859985  456.859985  413.104004  424.440002  424.440002   \n",
            "2014-09-19  424.102997  427.834991  384.532013  394.795990  394.795990   \n",
            "2014-09-20  394.673004  423.295990  389.882996  408.903992  408.903992   \n",
            "2014-09-21  408.084991  412.425995  393.181000  398.821014  398.821014   \n",
            "\n",
            "              Volume  \n",
            "Date                  \n",
            "2014-09-17  21056800  \n",
            "2014-09-18  34483200  \n",
            "2014-09-19  37919700  \n",
            "2014-09-20  36863600  \n",
            "2014-09-21  26580100  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 3549 entries, 2014-09-17 to 2024-06-04\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Open       3549 non-null   float64\n",
            " 1   High       3549 non-null   float64\n",
            " 2   Low        3549 non-null   float64\n",
            " 3   Close      3549 non-null   float64\n",
            " 4   Adj Close  3549 non-null   float64\n",
            " 5   Volume     3549 non-null   int64  \n",
            "dtypes: float64(5), int64(1)\n",
            "memory usage: 194.1 KB\n",
            "BTC prices info: \n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Price\n",
            "Date                  \n",
            "2014-09-17  457.334015\n",
            "2014-09-18  424.440002\n",
            "2014-09-19  394.795990\n",
            "2014-09-20  408.903992\n",
            "2014-09-21  398.821014\n",
            "Days from block reward 1: -658\n",
            " Days from block reward 2: 661\n",
            " Days from block reward 3: 2063\n",
            " Days from block reward 4: 1485\n",
            "\n",
            "Head of dataframe with block reward included: \n",
            "                  Price block_reward\n",
            "Date                               \n",
            "2014-09-17  457.334015           25\n",
            "2014-09-18  424.440002           25\n",
            "2014-09-19  394.795990           25\n",
            "2014-09-20  408.903992           25\n",
            "2014-09-21  398.821014           25\n",
            "Tail of dataframe with block reward included: \n",
            "                    Price block_reward\n",
            "Date                                 \n",
            "2024-05-31  67491.414062         6.25\n",
            "2024-06-01  67706.937500         6.25\n",
            "2024-06-02  67751.601562         6.25\n",
            "2024-06-03  68804.781250         6.25\n",
            "2024-06-04  70582.593750         6.25\n",
            "Head of windowed dataframe: \n",
            "                  Price block_reward      Price1      Price2      Price3  \\\n",
            "Date                                                                      \n",
            "2014-09-17  457.334015           25         NaN         NaN         NaN   \n",
            "2014-09-18  424.440002           25  457.334015         NaN         NaN   \n",
            "2014-09-19  394.795990           25  424.440002  457.334015         NaN   \n",
            "2014-09-20  408.903992           25  394.795990  424.440002  457.334015   \n",
            "2014-09-21  398.821014           25  408.903992  394.795990  424.440002   \n",
            "2014-09-22  402.152008           25  398.821014  408.903992  394.795990   \n",
            "2014-09-23  435.790985           25  402.152008  398.821014  408.903992   \n",
            "2014-09-24  423.204987           25  435.790985  402.152008  398.821014   \n",
            "2014-09-25  411.574005           25  423.204987  435.790985  402.152008   \n",
            "2014-09-26  404.424988           25  411.574005  423.204987  435.790985   \n",
            "\n",
            "                Price4      Price5      Price6      Price7  \n",
            "Date                                                        \n",
            "2014-09-17         NaN         NaN         NaN         NaN  \n",
            "2014-09-18         NaN         NaN         NaN         NaN  \n",
            "2014-09-19         NaN         NaN         NaN         NaN  \n",
            "2014-09-20         NaN         NaN         NaN         NaN  \n",
            "2014-09-21  457.334015         NaN         NaN         NaN  \n",
            "2014-09-22  424.440002  457.334015         NaN         NaN  \n",
            "2014-09-23  394.795990  424.440002  457.334015         NaN  \n",
            "2014-09-24  408.903992  394.795990  424.440002  457.334015  \n",
            "2014-09-25  398.821014  408.903992  394.795990  424.440002  \n",
            "2014-09-26  402.152008  398.821014  408.903992  394.795990  \n",
            "Head of X: \n",
            "             block_reward      Price1      Price2      Price3      Price4  \\\n",
            "Date                                                                       \n",
            "2014-09-24          25.0  435.790985  402.152008  398.821014  408.903992   \n",
            "2014-09-25          25.0  423.204987  435.790985  402.152008  398.821014   \n",
            "2014-09-26          25.0  411.574005  423.204987  435.790985  402.152008   \n",
            "2014-09-27          25.0  404.424988  411.574005  423.204987  435.790985   \n",
            "2014-09-28          25.0  399.519989  404.424988  411.574005  423.204987   \n",
            "\n",
            "                Price5      Price6      Price7  \n",
            "Date                                            \n",
            "2014-09-24  394.795990  424.440002  457.334015  \n",
            "2014-09-25  408.903992  394.795990  424.440002  \n",
            "2014-09-26  398.821014  408.903992  394.795990  \n",
            "2014-09-27  402.152008  398.821014  408.903992  \n",
            "2014-09-28  435.790985  402.152008  398.821014  \n",
            "Head of y: \n",
            " Date\n",
            "2014-09-24    423.204987\n",
            "2014-09-25    411.574005\n",
            "2014-09-26    404.424988\n",
            "2014-09-27    399.519989\n",
            "2014-09-28    377.181000\n",
            "Name: Price, dtype: float32\n",
            "Shape of X: (3542, 8) \n",
            " Shape of y: (3542,)\n",
            "(2833, 8) (709, 8) (2833,) (709,)\n",
            "Train dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))> Test dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Prediction"
      ],
      "metadata": {
        "id": "8gEJqIS4Sc9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate current day pred"
      ],
      "metadata": {
        "id": "cnNx3Ma1S3fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_btc_price_closing = get_historical_btc_data()\n",
        "\n",
        "# Including block reward to the full BTC closing price dataset\n",
        "df_btc_price_block_reward = include_block_reward(df_btc_price=df_btc_price_closing)\n",
        "\n",
        "# Plotting the BTC price and block reward over time\n",
        "# plot_block_reward_price_data(df_btc_price_block_reward=df_btc_price_block_reward)\n",
        "\n",
        "# Obtain the full windows and labels with block reward included\n",
        "dataset_full_windows_br, dataset_full_labels_br = make_windows_labels_multivariate(df_btc_price_block_reward)\n",
        "\n",
        "# Obtain the train/test sets of the full windows and labels with block reward included\n",
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=dataset_full_windows_br, labels=dataset_full_labels_br)\n",
        "\n",
        "# Obtain the train and test datasets\n",
        "train_dataset, test_dataset = gen_train_test_datasets(X_train=train_windows, y_train=train_labels,\n",
        "                                                      X_test=test_windows, y_test=test_labels)\n",
        "print(f\"Train dataset: {train_dataset}\", f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "# Create AR model\n",
        "AR_model = get_AR_model(len(train_windows), WINDOW_SIZE_WEEK + 1)\n",
        "\n",
        "# Create NBEATS model\n",
        "NBEATS_model = get_NBEATS_model()\n",
        "\n",
        "# Create LNRNN model\n",
        "LNRNN_model = get_LNRNN_model()\n",
        "\n",
        "# Create LSTM model\n",
        "LSTM_model_obj = LSTMModel()\n",
        "LSTM_model = LSTM_model_obj.get_model()\n",
        "\n",
        "# Create Dense model\n",
        "dense_model_obj = DenseModel()\n",
        "dense_model = dense_model_obj.get_model()\n",
        "\n",
        "train_models = [NBEATS_model, LSTM_model, dense_model]\n",
        "# train_models = [NBEATS_model, LNRNN_model, LSTM_model, dense_model]\n",
        "# train_models = [LNRNN_model]\n",
        "\n",
        "# Obtain list of trained ensemble models\n",
        "ensemble_models = get_ensemble_models(models=train_models, train_data=train_dataset, test_data=test_dataset,\n",
        "                                      num_iter=ENSEMBLE_NUM_ITER_TEST, num_epochs=ENSEMBLE_NUM_EPOCHS_TEST)\n",
        "\n",
        "# Generate model summaries\n",
        "# get_ensemble_models_summary(models=train_models)\n",
        "\n",
        "# Plot the ensemble models\n",
        "# plot_model(AR_model)\n",
        "\n",
        "# plot_model(NBEATS_model)\n",
        "\n",
        "# plot_model(LNRNN_model)\n",
        "\n",
        "# plot_model(LSTM_model)\n",
        "\n",
        "# plot_model(dense_model)\n",
        "\n",
        "# Generate ensemble predictions\n",
        "ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models, input_data=test_dataset)\n",
        "\n",
        "# Evaluate ensemble model predictions\n",
        "ensemble_results = evaluate_preds(y_true=test_labels, y_pred=np.median(ensemble_preds, axis=0))\n",
        "# print(ensemble_results)\n",
        "\n",
        "# Obtain the upper and lower bounds of the 95% confidence levels\n",
        "# lower, upper = get_upper_lower_confidence(preds=ensemble_preds)\n",
        "\n",
        "# Get the median values of the ensemble preds\n",
        "ensemble_median = np.median(ensemble_preds, axis=0)\n",
        "\n",
        "# Plot the confidence interval\n",
        "# plot_confidence_interval(test_windows, test_labels, ensemble_median, lower=lower, upper=upper, offset=300)\n",
        "\n",
        "# Make forecasts into future of the price of bitcoin\n",
        "future_forecast = make_future_forecast(models=ensemble_models, values=dataset_full_labels_br,\n",
        "                                       into_future=INTO_FUTURE_1_DAY, window_size=WINDOW_SIZE_WEEK + 1)\n",
        "\n",
        "# plot_future_forecast(df_btc_price=df_btc_price_closing, future_forecast=future_forecast)\n",
        "print(f\"Future forecast: {future_forecast}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmfip2PLS6NM",
        "outputId": "3dd0259e-3d2f-4a7a-d818-55602a7d2acf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of BTC prices: \n",
            "                  Open        High         Low       Close   Adj Close  \\\n",
            "Date                                                                     \n",
            "2014-09-17  465.864014  468.174011  452.421997  457.334015  457.334015   \n",
            "2014-09-18  456.859985  456.859985  413.104004  424.440002  424.440002   \n",
            "2014-09-19  424.102997  427.834991  384.532013  394.795990  394.795990   \n",
            "2014-09-20  394.673004  423.295990  389.882996  408.903992  408.903992   \n",
            "2014-09-21  408.084991  412.425995  393.181000  398.821014  398.821014   \n",
            "\n",
            "              Volume  \n",
            "Date                  \n",
            "2014-09-17  21056800  \n",
            "2014-09-18  34483200  \n",
            "2014-09-19  37919700  \n",
            "2014-09-20  36863600  \n",
            "2014-09-21  26580100  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 3549 entries, 2014-09-17 to 2024-06-04\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Open       3549 non-null   float64\n",
            " 1   High       3549 non-null   float64\n",
            " 2   Low        3549 non-null   float64\n",
            " 3   Close      3549 non-null   float64\n",
            " 4   Adj Close  3549 non-null   float64\n",
            " 5   Volume     3549 non-null   int64  \n",
            "dtypes: float64(5), int64(1)\n",
            "memory usage: 194.1 KB\n",
            "BTC prices info: \n",
            "None\n",
            "                 Price\n",
            "Date                  \n",
            "2014-09-17  457.334015\n",
            "2014-09-18  424.440002\n",
            "2014-09-19  394.795990\n",
            "2014-09-20  408.903992\n",
            "2014-09-21  398.821014\n",
            "Days from block reward 1: -658\n",
            " Days from block reward 2: 661\n",
            " Days from block reward 3: 2063\n",
            " Days from block reward 4: 1485\n",
            "\n",
            "Head of dataframe with block reward included: \n",
            "                  Price block_reward\n",
            "Date                               \n",
            "2014-09-17  457.334015           25\n",
            "2014-09-18  424.440002           25\n",
            "2014-09-19  394.795990           25\n",
            "2014-09-20  408.903992           25\n",
            "2014-09-21  398.821014           25\n",
            "Tail of dataframe with block reward included: \n",
            "                    Price block_reward\n",
            "Date                                 \n",
            "2024-05-31  67491.414062         6.25\n",
            "2024-06-01  67706.937500         6.25\n",
            "2024-06-02  67751.601562         6.25\n",
            "2024-06-03  68804.781250         6.25\n",
            "2024-06-04  70582.593750         6.25"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Head of windowed dataframe: \n",
            "                  Price block_reward      Price1      Price2      Price3  \\\n",
            "Date                                                                      \n",
            "2014-09-17  457.334015           25         NaN         NaN         NaN   \n",
            "2014-09-18  424.440002           25  457.334015         NaN         NaN   \n",
            "2014-09-19  394.795990           25  424.440002  457.334015         NaN   \n",
            "2014-09-20  408.903992           25  394.795990  424.440002  457.334015   \n",
            "2014-09-21  398.821014           25  408.903992  394.795990  424.440002   \n",
            "2014-09-22  402.152008           25  398.821014  408.903992  394.795990   \n",
            "2014-09-23  435.790985           25  402.152008  398.821014  408.903992   \n",
            "2014-09-24  423.204987           25  435.790985  402.152008  398.821014   \n",
            "2014-09-25  411.574005           25  423.204987  435.790985  402.152008   \n",
            "2014-09-26  404.424988           25  411.574005  423.204987  435.790985   \n",
            "\n",
            "                Price4      Price5      Price6      Price7  \n",
            "Date                                                        \n",
            "2014-09-17         NaN         NaN         NaN         NaN  \n",
            "2014-09-18         NaN         NaN         NaN         NaN  \n",
            "2014-09-19         NaN         NaN         NaN         NaN  \n",
            "2014-09-20         NaN         NaN         NaN         NaN  \n",
            "2014-09-21  457.334015         NaN         NaN         NaN  \n",
            "2014-09-22  424.440002  457.334015         NaN         NaN  \n",
            "2014-09-23  394.795990  424.440002  457.334015         NaN  \n",
            "2014-09-24  408.903992  394.795990  424.440002  457.334015  \n",
            "2014-09-25  398.821014  408.903992  394.795990  424.440002  \n",
            "2014-09-26  402.152008  398.821014  408.903992  394.795990  \n",
            "Head of X: \n",
            "             block_reward      Price1      Price2      Price3      Price4  \\\n",
            "Date                                                                       \n",
            "2014-09-24          25.0  435.790985  402.152008  398.821014  408.903992   \n",
            "2014-09-25          25.0  423.204987  435.790985  402.152008  398.821014   \n",
            "2014-09-26          25.0  411.574005  423.204987  435.790985  402.152008   \n",
            "2014-09-27          25.0  404.424988  411.574005  423.204987  435.790985   \n",
            "2014-09-28          25.0  399.519989  404.424988  411.574005  423.204987   \n",
            "\n",
            "                Price5      Price6      Price7  \n",
            "Date                                            \n",
            "2014-09-24  394.795990  424.440002  457.334015  \n",
            "2014-09-25  408.903992  394.795990  424.440002  \n",
            "2014-09-26  398.821014  408.903992  394.795990  \n",
            "2014-09-27  402.152008  398.821014  408.903992  \n",
            "2014-09-28  435.790985  402.152008  398.821014  \n",
            "Head of y: \n",
            " Date\n",
            "2014-09-24    423.204987\n",
            "2014-09-25    411.574005\n",
            "2014-09-26    404.424988\n",
            "2014-09-27    399.519989\n",
            "2014-09-28    377.181000\n",
            "Name: Price, dtype: float32\n",
            "Shape of X: (3542, 8) \n",
            " Shape of y: (3542,)\n",
            "(2833, 8) (709, 8) (2833,) (709,)\n",
            "Train dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))> Test dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing model by reducing: mae for epochs: 1, num_iter: 0, model: NBEATS_model\n",
            "3/3 - 46s - loss: 87351.2734 - mae: 87351.2734 - mse: 21536237568.0000 - val_loss: 1753.1476 - val_mae: 1753.1476 - val_mse: 5880621.0000 - lr: 0.0010 - 46s/epoch - 15s/step\n",
            "Optimizing model by reducing: mse for epochs: 1, num_iter: 0, model: NBEATS_model\n",
            "3/3 - 43s - loss: 6412093440.0000 - mae: 46684.5625 - mse: 6412093440.0000 - val_loss: 432320160.0000 - val_mae: 18626.6621 - val_mse: 432320160.0000 - lr: 0.0010 - 43s/epoch - 14s/step\n",
            "Optimizing model by reducing: mape for epochs: 1, num_iter: 0, model: NBEATS_model\n",
            "3/3 - 45s - loss: 126.6349 - mae: 9284.0078 - mse: 195750528.0000 - val_loss: 72.5001 - val_mae: 23647.2422 - val_mse: 690143552.0000 - lr: 0.0010 - 45s/epoch - 15s/step\n",
            "Optimizing model by reducing: mae for epochs: 1, num_iter: 0, model: LSTM_model\n",
            "3/3 - 3s - loss: 11990.2178 - mae: 11990.2178 - mse: 396130944.0000 - val_loss: 30785.6836 - val_mae: 30785.6836 - val_mse: 1160938496.0000 - lr: 0.0010 - 3s/epoch - 887ms/step\n",
            "Optimizing model by reducing: mse for epochs: 1, num_iter: 0, model: LSTM_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f473b3c3910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7f4739df7490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 3s - loss: 340592480.0000 - mae: 11109.5684 - mse: 340592480.0000 - val_loss: 1016501696.0000 - val_mae: 28799.4727 - val_mse: 1016501696.0000 - lr: 0.0010 - 3s/epoch - 947ms/step\n",
            "Optimizing model by reducing: mape for epochs: 1, num_iter: 0, model: LSTM_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f4739b66ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7f4738ad2cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 2s - loss: 84.1327 - mae: 10134.0420 - mse: 281156224.0000 - val_loss: 76.8601 - val_mae: 25376.0137 - val_mse: 790620224.0000 - lr: 0.0010 - 2s/epoch - 597ms/step\n",
            "Optimizing model by reducing: mae for epochs: 1, num_iter: 0, model: dense_model\n",
            "3/3 - 1s - loss: 5606.3994 - mae: 5606.3994 - mse: 83807976.0000 - val_loss: 12786.1807 - val_mae: 12786.1807 - val_mse: 201744976.0000 - lr: 0.0010 - 908ms/epoch - 303ms/step\n",
            "Optimizing model by reducing: mse for epochs: 1, num_iter: 0, model: dense_model\n",
            "3/3 - 1s - loss: 32113788.0000 - mae: 3485.0310 - mse: 32113788.0000 - val_loss: 68723160.0000 - val_mae: 7398.1699 - val_mse: 68723160.0000 - lr: 0.0010 - 827ms/epoch - 276ms/step\n",
            "Optimizing model by reducing: mape for epochs: 1, num_iter: 0, model: dense_model\n",
            "3/3 - 1s - loss: 15.9899 - mae: 1309.4910 - mse: 4577603.0000 - val_loss: 2.9316 - val_mae: 1009.2288 - val_mse: 2318228.7500 - lr: 0.0010 - 860ms/epoch - 287ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 8ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f473837beb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f473837bb50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicting on: \n",
            " Date\n",
            "2024-05-28    68296.218750\n",
            "2024-05-29    67578.093750\n",
            "2024-05-30    68364.992188\n",
            "2024-05-31    67491.414062\n",
            "2024-06-01    67706.937500\n",
            "2024-06-02    67751.601562\n",
            "2024-06-03    68804.781250\n",
            "2024-06-04    70582.593750\n",
            "Name: Price, dtype: float32 -> Prediction: 68159.1328125\n",
            "\n",
            "Future forecast: [68159.13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate full pipeline"
      ],
      "metadata": {
        "id": "12MW5gnKV7MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_btc_price_closing = get_historical_btc_data()\n",
        "\n",
        "# Including block reward to the full BTC closing price dataset\n",
        "df_btc_price_block_reward = include_block_reward(df_btc_price=df_btc_price_closing)\n",
        "\n",
        "# Plotting the BTC price and block reward over time\n",
        "# plot_block_reward_price_data(df_btc_price_block_reward=df_btc_price_block_reward)\n",
        "\n",
        "# Obtain the full windows and labels with block reward included\n",
        "dataset_full_windows_br, dataset_full_labels_br = make_windows_labels_multivariate(df_btc_price_block_reward)\n",
        "\n",
        "# Obtain the train/test sets of the full windows and labels with block reward included\n",
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=dataset_full_windows_br, labels=dataset_full_labels_br)\n",
        "\n",
        "# Obtain the train and test datasets\n",
        "train_dataset, test_dataset = gen_train_test_datasets(X_train=train_windows, y_train=train_labels,\n",
        "                                                      X_test=test_windows, y_test=test_labels)\n",
        "print(f\"Train dataset: {train_dataset}\", f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "# Create AR model\n",
        "AR_model = get_AR_model(len(train_windows), WINDOW_SIZE_WEEK + 1)\n",
        "\n",
        "# Create NBEATS model\n",
        "NBEATS_model = get_NBEATS_model()\n",
        "\n",
        "# Create LNRNN model\n",
        "LNRNN_model = get_LNRNN_model()\n",
        "\n",
        "# Create LSTM model\n",
        "LSTM_model_obj = LSTMModel()\n",
        "LSTM_model = LSTM_model_obj.get_model()\n",
        "\n",
        "# Create Dense model\n",
        "dense_model_obj = DenseModel()\n",
        "dense_model = dense_model_obj.get_model()\n",
        "\n",
        "train_models = [NBEATS_model, LSTM_model, dense_model]\n",
        "# train_models = [NBEATS_model, LNRNN_model, LSTM_model, dense_model]\n",
        "# train_models = [LNRNN_model]\n",
        "\n",
        "# Obtain list of trained ensemble models\n",
        "ensemble_models = get_ensemble_models(models=train_models, train_data=train_dataset, test_data=test_dataset,\n",
        "                                      num_iter=ENSEMBLE_NUM_ITER, num_epochs=ENSEMBLE_NUM_EPOCHS)\n",
        "\n",
        "# Generate model summaries\n",
        "get_ensemble_models_summary(models=train_models)\n",
        "\n",
        "# Plot the ensemble models\n",
        "# plot_model(AR_model)\n",
        "\n",
        "# plot_model(NBEATS_model)\n",
        "\n",
        "# plot_model(LNRNN_model)\n",
        "\n",
        "# plot_model(LSTM_model)\n",
        "\n",
        "# plot_model(dense_model)\n",
        "\n",
        "# Generate ensemble predictions\n",
        "ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models, input_data=test_dataset)\n",
        "\n",
        "# Evaluate ensemble model predictions\n",
        "ensemble_results = evaluate_preds(y_true=test_labels, y_pred=np.median(ensemble_preds, axis=0))\n",
        "print(ensemble_results)\n",
        "\n",
        "# Obtain the upper and lower bounds of the 95% confidence levels\n",
        "lower, upper = get_upper_lower_confidence(preds=ensemble_preds)\n",
        "\n",
        "# Get the median values of the ensemble preds\n",
        "ensemble_median = np.median(ensemble_preds, axis=0)\n",
        "\n",
        "# Plot the confidence interval\n",
        "# plot_confidence_interval(test_windows, test_labels, ensemble_median, lower=lower, upper=upper, offset=300)\n",
        "\n",
        "# Make forecasts into future of the price of bitcoin\n",
        "future_forecast = make_future_forecast(models=ensemble_models, values=dataset_full_labels_br,\n",
        "                                       into_future=INTO_FUTURE_2_WEEK, window_size=WINDOW_SIZE_WEEK + 1)\n",
        "\n",
        "# plot_future_forecast(df_btc_price=df_btc_price_closing, future_forecast=future_forecast)\n",
        "print(f\"Future forecast: {future_forecast}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLwmzN-fV5Cs",
        "outputId": "1b12c8ea-84e3-48af-cce2-f335da7d5bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Head of BTC prices: \n",
            "                  Open        High         Low       Close   Adj Close  \\\n",
            "Date                                                                     \n",
            "2014-09-17  465.864014  468.174011  452.421997  457.334015  457.334015   \n",
            "2014-09-18  456.859985  456.859985  413.104004  424.440002  424.440002   \n",
            "2014-09-19  424.102997  427.834991  384.532013  394.795990  394.795990   \n",
            "2014-09-20  394.673004  423.295990  389.882996  408.903992  408.903992   \n",
            "2014-09-21  408.084991  412.425995  393.181000  398.821014  398.821014   \n",
            "\n",
            "              Volume  \n",
            "Date                  \n",
            "2014-09-17  21056800  \n",
            "2014-09-18  34483200  \n",
            "2014-09-19  37919700  \n",
            "2014-09-20  36863600  \n",
            "2014-09-21  26580100  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 3549 entries, 2014-09-17 to 2024-06-04\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Open       3549 non-null   float64\n",
            " 1   High       3549 non-null   float64\n",
            " 2   Low        3549 non-null   float64\n",
            " 3   Close      3549 non-null   float64\n",
            " 4   Adj Close  3549 non-null   float64\n",
            " 5   Volume     3549 non-null   int64  \n",
            "dtypes: float64(5), int64(1)\n",
            "memory usage: 194.1 KB\n",
            "BTC prices info: \n",
            "None\n",
            "                 Price\n",
            "Date                  \n",
            "2014-09-17  457.334015\n",
            "2014-09-18  424.440002\n",
            "2014-09-19  394.795990\n",
            "2014-09-20  408.903992\n",
            "2014-09-21  398.821014\n",
            "Days from block reward 1: -658\n",
            " Days from block reward 2: 661\n",
            " Days from block reward 3: 2063\n",
            " Days from block reward 4: 1485\n",
            "\n",
            "Head of dataframe with block reward included: \n",
            "                  Price block_reward\n",
            "Date                               \n",
            "2014-09-17  457.334015           25\n",
            "2014-09-18  424.440002           25\n",
            "2014-09-19  394.795990           25\n",
            "2014-09-20  408.903992           25\n",
            "2014-09-21  398.821014           25\n",
            "Tail of dataframe with block reward included: \n",
            "                    Price block_reward\n",
            "Date                                 \n",
            "2024-05-31  67491.414062         6.25\n",
            "2024-06-01  67706.937500         6.25\n",
            "2024-06-02  67751.601562         6.25\n",
            "2024-06-03  68804.781250         6.25\n",
            "2024-06-04  70564.289062         6.25\n",
            "Head of windowed dataframe: \n",
            "                  Price block_reward      Price1      Price2      Price3  \\\n",
            "Date                                                                      \n",
            "2014-09-17  457.334015           25         NaN         NaN         NaN   \n",
            "2014-09-18  424.440002           25  457.334015         NaN         NaN   \n",
            "2014-09-19  394.795990           25  424.440002  457.334015         NaN   \n",
            "2014-09-20  408.903992           25  394.795990  424.440002  457.334015   \n",
            "2014-09-21  398.821014           25  408.903992  394.795990  424.440002   \n",
            "2014-09-22  402.152008           25  398.821014  408.903992  394.795990   \n",
            "2014-09-23  435.790985           25  402.152008  398.821014  408.903992   \n",
            "2014-09-24  423.204987           25  435.790985  402.152008  398.821014   \n",
            "2014-09-25  411.574005           25  423.204987  435.790985  402.152008   \n",
            "2014-09-26  404.424988           25  411.574005  423.204987  435.790985   \n",
            "\n",
            "                Price4      Price5      Price6      Price7  \n",
            "Date                                                        \n",
            "2014-09-17         NaN         NaN         NaN         NaN  \n",
            "2014-09-18         NaN         NaN         NaN         NaN  \n",
            "2014-09-19         NaN         NaN         NaN         NaN  \n",
            "2014-09-20         NaN         NaN         NaN         NaN  \n",
            "2014-09-21  457.334015         NaN         NaN         NaN  \n",
            "2014-09-22  424.440002  457.334015         NaN         NaN  \n",
            "2014-09-23  394.795990  424.440002  457.334015         NaN  \n",
            "2014-09-24  408.903992  394.795990  424.440002  457.334015  \n",
            "2014-09-25  398.821014  408.903992  394.795990  424.440002  \n",
            "2014-09-26  402.152008  398.821014  408.903992  394.795990  \n",
            "Head of X: \n",
            "             block_reward      Price1      Price2      Price3      Price4  \\\n",
            "Date                                                                       \n",
            "2014-09-24          25.0  435.790985  402.152008  398.821014  408.903992   \n",
            "2014-09-25          25.0  423.204987  435.790985  402.152008  398.821014   \n",
            "2014-09-26          25.0  411.574005  423.204987  435.790985  402.152008   \n",
            "2014-09-27          25.0  404.424988  411.574005  423.204987  435.790985   \n",
            "2014-09-28          25.0  399.519989  404.424988  411.574005  423.204987   \n",
            "\n",
            "                Price5      Price6      Price7  \n",
            "Date                                            \n",
            "2014-09-24  394.795990  424.440002  457.334015  \n",
            "2014-09-25  408.903992  394.795990  424.440002  \n",
            "2014-09-26  398.821014  408.903992  394.795990  \n",
            "2014-09-27  402.152008  398.821014  408.903992  \n",
            "2014-09-28  435.790985  402.152008  398.821014  \n",
            "Head of y: \n",
            " Date\n",
            "2014-09-24    423.204987\n",
            "2014-09-25    411.574005\n",
            "2014-09-26    404.424988\n",
            "2014-09-27    399.519989\n",
            "2014-09-28    377.181000\n",
            "Name: Price, dtype: float32\n",
            "Shape of X: (3542, 8) \n",
            " Shape of y: (3542,)\n",
            "(2833, 8) (709, 8) (2833,) (709,)\n",
            "Train dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))> Test dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3/3 - 0s - loss: 2.5417 - mae: 353.0638 - mse: 651228.1875 - val_loss: 1.7926 - val_mae: 616.6508 - val_mse: 1020710.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 2.5544 - mae: 353.2340 - mse: 649174.1875 - val_loss: 1.7824 - val_mae: 613.7260 - val_mse: 1018570.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 2.5423 - mae: 352.7848 - mse: 650532.3750 - val_loss: 1.7494 - val_mae: 605.6154 - val_mse: 1023215.3750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 2.5628 - mae: 354.4389 - mse: 654066.2500 - val_loss: 1.7962 - val_mae: 622.4111 - val_mse: 1067867.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 2.5851 - mae: 356.7438 - mse: 663637.6250 - val_loss: 1.7586 - val_mae: 609.7024 - val_mse: 1036285.1250 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 2.5492 - mae: 354.6795 - mse: 658011.7500 - val_loss: 1.7865 - val_mae: 614.9273 - val_mse: 1019922.6250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 2.5660 - mae: 355.2119 - mse: 650323.7500 - val_loss: 1.8058 - val_mae: 620.3102 - val_mse: 1023807.3750 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 2.5556 - mae: 353.9872 - mse: 650862.9375 - val_loss: 1.7517 - val_mae: 606.9833 - val_mse: 1029470.4375 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 2.5626 - mae: 354.5963 - mse: 655298.5000 - val_loss: 1.7872 - val_mae: 619.4599 - val_mse: 1061440.7500 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 2.5771 - mae: 356.1989 - mse: 662531.4375 - val_loss: 1.7552 - val_mae: 608.5302 - val_mse: 1033132.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 2.5439 - mae: 353.8274 - mse: 655771.5000 - val_loss: 1.7699 - val_mae: 610.4462 - val_mse: 1017303.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 2.5431 - mae: 352.7489 - mse: 648785.1250 - val_loss: 1.7713 - val_mae: 610.8559 - val_mse: 1017276.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 2.5386 - mae: 352.5865 - mse: 649922.2500 - val_loss: 1.7536 - val_mae: 607.8748 - val_mse: 1030599.8750 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 2.5603 - mae: 354.4891 - mse: 655430.5000 - val_loss: 1.7879 - val_mae: 619.6105 - val_mse: 1058887.6250 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 2.5601 - mae: 355.4014 - mse: 661121.3750 - val_loss: 1.7535 - val_mae: 606.5956 - val_mse: 1021139.4375 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 2.5454 - mae: 353.2935 - mse: 650734.5000 - val_loss: 1.7960 - val_mae: 617.6395 - val_mse: 1021517.4375 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 2.5600 - mae: 353.7466 - mse: 649480.6875 - val_loss: 1.7878 - val_mae: 615.2498 - val_mse: 1020153.4375 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 2.5469 - mae: 353.0752 - mse: 649647.3125 - val_loss: 1.7500 - val_mae: 605.4857 - val_mse: 1022086.5000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 2.5487 - mae: 353.0942 - mse: 651010.7500 - val_loss: 1.7821 - val_mae: 617.9732 - val_mse: 1059697.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 2.5869 - mae: 356.4692 - mse: 661613.8750 - val_loss: 1.7697 - val_mae: 613.8478 - val_mse: 1049721.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 2.5589 - mae: 354.6575 - mse: 658737.6250 - val_loss: 1.7503 - val_mae: 605.4882 - val_mse: 1019636.6250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 2.5377 - mae: 352.6844 - mse: 650737.4375 - val_loss: 1.7730 - val_mae: 611.3005 - val_mse: 1017885.8750 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 2.5404 - mae: 352.3442 - mse: 649242.7500 - val_loss: 1.7596 - val_mae: 607.6806 - val_mse: 1017393.3125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 2.5370 - mae: 352.4529 - mse: 650206.6250 - val_loss: 1.7567 - val_mae: 609.1498 - val_mse: 1034400.6875 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 2.5562 - mae: 354.0550 - mse: 654984.6250 - val_loss: 1.7721 - val_mae: 614.4868 - val_mse: 1046985.9375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 2.5498 - mae: 354.0267 - mse: 656424.6875 - val_loss: 1.7534 - val_mae: 606.6884 - val_mse: 1021042.1250 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 2.5388 - mae: 352.7745 - mse: 650505.5625 - val_loss: 1.7807 - val_mae: 613.4967 - val_mse: 1019018.7500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 2.5469 - mae: 352.7787 - mse: 649422.8750 - val_loss: 1.7688 - val_mae: 610.0949 - val_mse: 1017293.0625 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 2.5375 - mae: 352.3898 - mse: 650116.8750 - val_loss: 1.7505 - val_mae: 606.4692 - val_mse: 1026250.8125 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 2.5534 - mae: 353.6351 - mse: 652750.2500 - val_loss: 1.7775 - val_mae: 616.4081 - val_mse: 1053670.2500 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 2.5695 - mae: 355.2576 - mse: 659091.0000 - val_loss: 1.7553 - val_mae: 608.7545 - val_mse: 1033446.6250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 140: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 2.5443 - mae: 354.1435 - mse: 656590.8125 - val_loss: 1.7723 - val_mae: 611.2173 - val_mse: 1018364.3750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 2.5393 - mae: 352.5404 - mse: 650708.5625 - val_loss: 1.7523 - val_mae: 606.3537 - val_mse: 1021009.0625 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 2.5358 - mae: 352.2691 - mse: 648519.8750 - val_loss: 1.7522 - val_mae: 607.1537 - val_mse: 1026554.3125 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 2.5369 - mae: 352.2967 - mse: 648957.6250 - val_loss: 1.7529 - val_mae: 606.4057 - val_mse: 1020559.1875 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 2.5339 - mae: 352.5023 - mse: 651182.3750 - val_loss: 1.7610 - val_mae: 608.1592 - val_mse: 1018043.5000 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 2.5349 - mae: 352.5071 - mse: 651410.8125 - val_loss: 1.7560 - val_mae: 606.9310 - val_mse: 1018704.0000 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 2.5329 - mae: 352.1423 - mse: 649694.7500 - val_loss: 1.7513 - val_mae: 606.1676 - val_mse: 1022193.1250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 2.5332 - mae: 352.1486 - mse: 649552.6250 - val_loss: 1.7517 - val_mae: 606.1028 - val_mse: 1020996.1250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 2.5322 - mae: 352.2818 - mse: 650671.3125 - val_loss: 1.7551 - val_mae: 606.6627 - val_mse: 1018628.9375 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.3689 - mse: 651317.2500 - val_loss: 1.7554 - val_mae: 606.7019 - val_mse: 1018424.4375 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.2286 - mse: 650757.2500 - val_loss: 1.7526 - val_mae: 606.1713 - val_mse: 1019614.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 2.5312 - mae: 352.1422 - mse: 650411.6250 - val_loss: 1.7522 - val_mae: 606.0932 - val_mse: 1019898.7500 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 2.5311 - mae: 352.1824 - mse: 650730.8125 - val_loss: 1.7534 - val_mae: 606.2998 - val_mse: 1019063.4375 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 2.5309 - mae: 352.2664 - mse: 651254.8750 - val_loss: 1.7548 - val_mae: 606.5427 - val_mse: 1018463.6250 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 2.5308 - mae: 352.2698 - mse: 651313.0625 - val_loss: 1.7541 - val_mae: 606.4105 - val_mse: 1018697.8125 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 2.5308 - mae: 352.2255 - mse: 651028.8125 - val_loss: 1.7533 - val_mae: 606.2602 - val_mse: 1019037.2500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 2.5309 - mae: 352.2738 - mse: 651257.7500 - val_loss: 1.7549 - val_mae: 606.5613 - val_mse: 1018370.1875 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 2.5308 - mae: 352.2909 - mse: 651359.3125 - val_loss: 1.7544 - val_mae: 606.4515 - val_mse: 1018537.5000 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 2.5306 - mae: 352.2247 - mse: 650997.0625 - val_loss: 1.7535 - val_mae: 606.2791 - val_mse: 1018905.1250 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 2.5306 - mae: 352.2272 - mse: 651004.7500 - val_loss: 1.7539 - val_mae: 606.3559 - val_mse: 1018708.5000 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 2.5306 - mae: 352.2356 - mse: 651064.1875 - val_loss: 1.7539 - val_mae: 606.3497 - val_mse: 1018721.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 2.5306 - mae: 352.2150 - mse: 650967.6875 - val_loss: 1.7535 - val_mae: 606.2810 - val_mse: 1018892.8125 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 2.5306 - mae: 352.2215 - mse: 650996.1250 - val_loss: 1.7538 - val_mae: 606.3342 - val_mse: 1018765.6250 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 2.5306 - mae: 352.2457 - mse: 651143.6250 - val_loss: 1.7542 - val_mae: 606.4187 - val_mse: 1018588.2500 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 2.5305 - mae: 352.2430 - mse: 651141.5000 - val_loss: 1.7538 - val_mae: 606.3453 - val_mse: 1018738.6250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 2.5305 - mae: 352.2340 - mse: 651089.1250 - val_loss: 1.7538 - val_mae: 606.3517 - val_mse: 1018719.8750 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 2.5305 - mae: 352.2519 - mse: 651190.5625 - val_loss: 1.7542 - val_mae: 606.4301 - val_mse: 1018556.3750 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 2.5305 - mae: 352.2581 - mse: 651225.8750 - val_loss: 1.7540 - val_mae: 606.3942 - val_mse: 1018629.4375 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2386 - mse: 651114.6250 - val_loss: 1.7538 - val_mae: 606.3461 - val_mse: 1018737.3750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2384 - mse: 651111.5625 - val_loss: 1.7539 - val_mae: 606.3770 - val_mse: 1018680.3125 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2438 - mse: 651149.1250 - val_loss: 1.7541 - val_mae: 606.4135 - val_mse: 1018616.3125 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2453 - mse: 651162.1250 - val_loss: 1.7540 - val_mae: 606.3895 - val_mse: 1018673.6250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2270 - mse: 651066.4375 - val_loss: 1.7537 - val_mae: 606.3267 - val_mse: 1018815.8125 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2415 - mse: 651138.3750 - val_loss: 1.7541 - val_mae: 606.4160 - val_mse: 1018624.6250 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2424 - mse: 651144.5625 - val_loss: 1.7539 - val_mae: 606.3744 - val_mse: 1018708.5000 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2430 - mse: 651143.2500 - val_loss: 1.7541 - val_mae: 606.4199 - val_mse: 1018622.8125 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 2.5304 - mae: 352.2488 - mse: 651169.8125 - val_loss: 1.7542 - val_mae: 606.4267 - val_mse: 1018615.6875 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2363 - mse: 651104.3125 - val_loss: 1.7539 - val_mae: 606.3685 - val_mse: 1018736.3750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2505 - mse: 651182.3125 - val_loss: 1.7545 - val_mae: 606.4844 - val_mse: 1018513.3125 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2496 - mse: 651183.4375 - val_loss: 1.7540 - val_mae: 606.3902 - val_mse: 1018702.0000 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2359 - mse: 651111.0625 - val_loss: 1.7540 - val_mae: 606.4034 - val_mse: 1018684.5625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2580 - mse: 651223.9375 - val_loss: 1.7544 - val_mae: 606.4833 - val_mse: 1018536.6875 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2402 - mse: 651137.0625 - val_loss: 1.7539 - val_mae: 606.3803 - val_mse: 1018745.6875 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2367 - mse: 651123.1875 - val_loss: 1.7541 - val_mae: 606.4182 - val_mse: 1018670.0625 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2559 - mse: 651232.5625 - val_loss: 1.7543 - val_mae: 606.4581 - val_mse: 1018597.6250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2283 - mse: 651089.1250 - val_loss: 1.7537 - val_mae: 606.3412 - val_mse: 1018847.0625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2486 - mse: 651197.6875 - val_loss: 1.7546 - val_mae: 606.5194 - val_mse: 1018494.1250 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2618 - mse: 651273.3750 - val_loss: 1.7541 - val_mae: 606.4257 - val_mse: 1018673.8125 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2327 - mse: 651118.8750 - val_loss: 1.7540 - val_mae: 606.4073 - val_mse: 1018717.6875 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2593 - mse: 651258.0625 - val_loss: 1.7544 - val_mae: 606.4788 - val_mse: 1018583.7500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2301 - mse: 651107.2500 - val_loss: 1.7537 - val_mae: 606.3567 - val_mse: 1018838.6875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2462 - mse: 651185.5000 - val_loss: 1.7546 - val_mae: 606.5248 - val_mse: 1018510.4375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2694 - mse: 651310.3750 - val_loss: 1.7543 - val_mae: 606.4698 - val_mse: 1018613.6250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2227 - mse: 651072.6875 - val_loss: 1.7536 - val_mae: 606.3369 - val_mse: 1018895.0000 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2635 - mse: 651283.5000 - val_loss: 1.7548 - val_mae: 606.5602 - val_mse: 1018463.4375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2470 - mse: 651202.5625 - val_loss: 1.7540 - val_mae: 606.4116 - val_mse: 1018744.3125 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2383 - mse: 651153.2500 - val_loss: 1.7542 - val_mae: 606.4628 - val_mse: 1018649.8125 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2582 - mse: 651261.6250 - val_loss: 1.7543 - val_mae: 606.4823 - val_mse: 1018620.3750 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2377 - mse: 651161.1250 - val_loss: 1.7541 - val_mae: 606.4312 - val_mse: 1018731.3125 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2468 - mse: 651217.3125 - val_loss: 1.7544 - val_mae: 606.4979 - val_mse: 1018600.8750 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2754 - mse: 651358.3125 - val_loss: 1.7546 - val_mae: 606.5278 - val_mse: 1018548.8125 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2219 - mse: 651077.4375 - val_loss: 1.7536 - val_mae: 606.3386 - val_mse: 1018949.6250 - lr: 1.0000e-04 - 46ms/epoch - 15ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2431 - mse: 651179.8750 - val_loss: 1.7548 - val_mae: 606.5762 - val_mse: 1018481.5625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2970 - mse: 651466.8750 - val_loss: 1.7551 - val_mae: 606.6423 - val_mse: 1018399.7500 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2195 - mse: 651074.6875 - val_loss: 1.7533 - val_mae: 606.3027 - val_mse: 1019082.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 2.5303 - mae: 352.2625 - mse: 651286.2500 - val_loss: 1.7552 - val_mae: 606.6649 - val_mse: 1018391.1250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2765 - mse: 651360.1875 - val_loss: 1.7544 - val_mae: 606.5110 - val_mse: 1018647.1875 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2144 - mse: 651034.3125 - val_loss: 1.7537 - val_mae: 606.3773 - val_mse: 1018949.8750 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2490 - mse: 651210.8750 - val_loss: 1.7549 - val_mae: 606.6076 - val_mse: 1018494.6250 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2809 - mse: 651390.3125 - val_loss: 1.7547 - val_mae: 606.5855 - val_mse: 1018533.5625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2214 - mse: 651096.8125 - val_loss: 1.7537 - val_mae: 606.3837 - val_mse: 1018949.6250 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 2.5302 - mae: 352.2683 - mse: 651330.0625 - val_loss: 1.7550 - val_mae: 606.6324 - val_mse: 1018468.8125 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2633 - mse: 651304.7500 - val_loss: 1.7543 - val_mae: 606.5014 - val_mse: 1018710.1250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2296 - mse: 651125.4375 - val_loss: 1.7543 - val_mae: 606.4955 - val_mse: 1018725.1250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2586 - mse: 651279.5625 - val_loss: 1.7545 - val_mae: 606.5487 - val_mse: 1018623.3750 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2247 - mse: 651113.0625 - val_loss: 1.7539 - val_mae: 606.4347 - val_mse: 1018850.1875 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2691 - mse: 651345.5625 - val_loss: 1.7550 - val_mae: 606.6346 - val_mse: 1018478.9375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2469 - mse: 651236.5000 - val_loss: 1.7541 - val_mae: 606.4650 - val_mse: 1018784.2500 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2274 - mse: 651127.6250 - val_loss: 1.7542 - val_mae: 606.4840 - val_mse: 1018745.5000 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2728 - mse: 651362.8750 - val_loss: 1.7550 - val_mae: 606.6519 - val_mse: 1018456.1250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2523 - mse: 651272.5000 - val_loss: 1.7541 - val_mae: 606.4799 - val_mse: 1018763.0000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 2.5301 - mae: 352.2516 - mse: 651257.6875 - val_loss: 1.7545 - val_mae: 606.5567 - val_mse: 1018626.6250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2318 - mse: 651151.1250 - val_loss: 1.7541 - val_mae: 606.4783 - val_mse: 1018794.9375 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2536 - mse: 651266.3125 - val_loss: 1.7547 - val_mae: 606.5818 - val_mse: 1018606.1875 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 224/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2289 - mse: 651140.4375 - val_loss: 1.7540 - val_mae: 606.4605 - val_mse: 1018848.0625 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2432 - mse: 651227.2500 - val_loss: 1.7547 - val_mae: 606.5834 - val_mse: 1018605.0625 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2457 - mse: 651248.1250 - val_loss: 1.7542 - val_mae: 606.5055 - val_mse: 1018751.1250 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2578 - mse: 651291.1250 - val_loss: 1.7547 - val_mae: 606.5864 - val_mse: 1018597.3750 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 228/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2264 - mse: 651117.6875 - val_loss: 1.7540 - val_mae: 606.4711 - val_mse: 1018830.6250 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 229/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2558 - mse: 651266.8750 - val_loss: 1.7548 - val_mae: 606.6138 - val_mse: 1018570.9375 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 230/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2375 - mse: 651188.8125 - val_loss: 1.7542 - val_mae: 606.4969 - val_mse: 1018804.8125 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 231/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2238 - mse: 651121.1875 - val_loss: 1.7543 - val_mae: 606.5169 - val_mse: 1018770.9375 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 232/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2837 - mse: 651424.6875 - val_loss: 1.7556 - val_mae: 606.7971 - val_mse: 1018362.8750 - lr: 1.0000e-04 - 57ms/epoch - 19ms/step\n",
            "Epoch 233/300\n",
            "3/3 - 0s - loss: 2.5298 - mae: 352.2438 - mse: 651228.3125 - val_loss: 1.7540 - val_mae: 606.4741 - val_mse: 1018863.3750 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 234/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2144 - mse: 651071.4375 - val_loss: 1.7542 - val_mae: 606.5128 - val_mse: 1018789.1875 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 235/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 352.2701 - mse: 651371.0000 - val_loss: 1.7551 - val_mae: 606.6985 - val_mse: 1018479.5625 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 236/300\n",
            "3/3 - 0s - loss: 2.5298 - mae: 352.2445 - mse: 651252.3125 - val_loss: 1.7543 - val_mae: 606.5265 - val_mse: 1018779.6250 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 237/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2399 - mse: 651219.6250 - val_loss: 1.7544 - val_mae: 606.5598 - val_mse: 1018719.6875 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 238/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2390 - mse: 651215.1875 - val_loss: 1.7544 - val_mae: 606.5430 - val_mse: 1018752.0000 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 239/300\n",
            "3/3 - 0s - loss: 2.5299 - mae: 352.2430 - mse: 651231.6250 - val_loss: 1.7546 - val_mae: 606.5959 - val_mse: 1018651.6250 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 240: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 2.5298 - mae: 352.2363 - mse: 651200.8125 - val_loss: 1.7543 - val_mae: 606.5416 - val_mse: 1018756.6250 - lr: 1.0000e-04 - 74ms/epoch - 25ms/step\n",
            "Optimizing model by reducing: mae for epochs: 300, num_iter: 2, model: NBEATS_model\n",
            "Epoch 1/300\n",
            "3/3 - 45s - loss: 3434.6975 - mae: 3434.6975 - mse: 30505782.0000 - val_loss: 2185.2446 - val_mae: 2185.2446 - val_mse: 8176489.0000 - lr: 0.0010 - 45s/epoch - 15s/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 1538.4814 - mae: 1538.4814 - mse: 6854801.5000 - val_loss: 1601.7384 - val_mae: 1601.7384 - val_mse: 6094733.0000 - lr: 0.0010 - 433ms/epoch - 144ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 1014.4077 - mae: 1014.4077 - mse: 4667745.0000 - val_loss: 1672.5652 - val_mae: 1672.5652 - val_mse: 6477529.5000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 1015.0972 - mae: 1015.0972 - mse: 4646331.5000 - val_loss: 1646.6425 - val_mae: 1646.6425 - val_mse: 6336775.5000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 980.3555 - mae: 980.3555 - mse: 4299276.5000 - val_loss: 1820.4310 - val_mae: 1820.4310 - val_mse: 7217314.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 1135.4065 - mae: 1135.4065 - mse: 5178173.0000 - val_loss: 1712.6555 - val_mae: 1712.6555 - val_mse: 6749899.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 949.9083 - mae: 949.9082 - mse: 4117970.0000 - val_loss: 1517.8770 - val_mae: 1517.8770 - val_mse: 4741132.5000 - lr: 0.0010 - 372ms/epoch - 124ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 850.0787 - mae: 850.0787 - mse: 3417835.7500 - val_loss: 1126.1689 - val_mae: 1126.1689 - val_mse: 2911470.0000 - lr: 0.0010 - 328ms/epoch - 109ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 771.8966 - mae: 771.8966 - mse: 3047416.5000 - val_loss: 1509.7920 - val_mae: 1509.7920 - val_mse: 5045062.5000 - lr: 0.0010 - 247ms/epoch - 82ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 959.8697 - mae: 959.8697 - mse: 4138208.0000 - val_loss: 1478.4386 - val_mae: 1478.4386 - val_mse: 5103313.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 1038.9430 - mae: 1038.9430 - mse: 4680558.5000 - val_loss: 1756.6791 - val_mae: 1756.6791 - val_mse: 6977740.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 1002.3055 - mae: 1002.3055 - mse: 4511367.5000 - val_loss: 1618.8656 - val_mae: 1618.8656 - val_mse: 5886620.5000 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 902.7089 - mae: 902.7089 - mse: 3705257.7500 - val_loss: 1457.7603 - val_mae: 1457.7603 - val_mse: 3929668.2500 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 1053.4294 - mae: 1053.4294 - mse: 4421857.0000 - val_loss: 1252.0449 - val_mae: 1252.0449 - val_mse: 3559268.7500 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 876.0483 - mae: 876.0483 - mse: 3352190.0000 - val_loss: 2171.9163 - val_mae: 2171.9163 - val_mse: 8195915.5000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 895.9499 - mae: 895.9500 - mse: 3095697.2500 - val_loss: 1420.6769 - val_mae: 1420.6769 - val_mse: 4693442.5000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 930.0512 - mae: 930.0512 - mse: 3972827.2500 - val_loss: 1316.8219 - val_mae: 1316.8219 - val_mse: 3853505.2500 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 786.6375 - mae: 786.6375 - mse: 2857712.0000 - val_loss: 3070.9604 - val_mae: 3070.9604 - val_mse: 12961991.0000 - lr: 0.0010 - 214ms/epoch - 71ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 1139.4869 - mae: 1139.4869 - mse: 3578235.5000 - val_loss: 2543.5305 - val_mae: 2543.5305 - val_mse: 10341849.0000 - lr: 0.0010 - 215ms/epoch - 72ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 1362.4999 - mae: 1362.4999 - mse: 5814118.5000 - val_loss: 1628.2802 - val_mae: 1628.2802 - val_mse: 6257376.5000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 1008.8304 - mae: 1008.8304 - mse: 4624653.0000 - val_loss: 1677.7346 - val_mae: 1677.7346 - val_mse: 6490175.5000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 1017.9785 - mae: 1017.9785 - mse: 4683685.5000 - val_loss: 1673.4132 - val_mae: 1673.4132 - val_mse: 6421478.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 1002.9337 - mae: 1002.9337 - mse: 4510159.0000 - val_loss: 1583.7900 - val_mae: 1583.7900 - val_mse: 5554308.5000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 908.9614 - mae: 908.9612 - mse: 3755793.0000 - val_loss: 1604.3025 - val_mae: 1604.3025 - val_mse: 5710785.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 1000.4028 - mae: 1000.4028 - mse: 4527636.0000 - val_loss: 1685.6940 - val_mae: 1685.6940 - val_mse: 6622384.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 993.9616 - mae: 993.9616 - mse: 4382225.0000 - val_loss: 1717.7096 - val_mae: 1717.7096 - val_mse: 6792314.0000 - lr: 0.0010 - 215ms/epoch - 72ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 980.5090 - mae: 980.5090 - mse: 4250626.0000 - val_loss: 1550.6774 - val_mae: 1550.6774 - val_mse: 5590565.0000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 1041.9996 - mae: 1041.9996 - mse: 4279481.0000 - val_loss: 2577.1196 - val_mae: 2577.1196 - val_mse: 11340872.0000 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 991.6728 - mae: 991.6728 - mse: 4019709.2500 - val_loss: 1681.8174 - val_mae: 1681.8174 - val_mse: 6428803.5000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 1100.8073 - mae: 1100.8073 - mse: 5552214.5000 - val_loss: 1704.9202 - val_mae: 1704.9202 - val_mse: 6521495.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 1023.3971 - mae: 1023.3971 - mse: 4725929.0000 - val_loss: 1679.9614 - val_mae: 1679.9614 - val_mse: 6515716.5000 - lr: 0.0010 - 217ms/epoch - 72ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 1014.1978 - mae: 1014.1978 - mse: 4640985.0000 - val_loss: 1669.7687 - val_mae: 1669.7687 - val_mse: 6485328.0000 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 994.2172 - mae: 994.2172 - mse: 4439800.0000 - val_loss: 1752.2834 - val_mae: 1752.2834 - val_mse: 6900697.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 1005.9513 - mae: 1005.9513 - mse: 4320781.0000 - val_loss: 1550.3640 - val_mae: 1550.3640 - val_mse: 5662704.5000 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 1027.8772 - mae: 1027.8772 - mse: 4809757.0000 - val_loss: 1636.3215 - val_mae: 1636.3215 - val_mse: 6013333.5000 - lr: 0.0010 - 213ms/epoch - 71ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 992.1641 - mae: 992.1641 - mse: 4457253.5000 - val_loss: 1576.3781 - val_mae: 1576.3781 - val_mse: 5748219.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 910.9003 - mae: 910.9002 - mse: 3809322.2500 - val_loss: 1375.8783 - val_mae: 1375.8783 - val_mse: 3764667.7500 - lr: 0.0010 - 316ms/epoch - 105ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 816.2303 - mae: 816.2303 - mse: 2801886.0000 - val_loss: 1018.9023 - val_mae: 1018.9023 - val_mse: 2120717.7500 - lr: 0.0010 - 405ms/epoch - 135ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 1700.2395 - mae: 1700.2394 - mse: 10778803.0000 - val_loss: 2117.5657 - val_mae: 2117.5657 - val_mse: 7531092.0000 - lr: 0.0010 - 298ms/epoch - 99ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 1138.5967 - mae: 1138.5967 - mse: 5369228.5000 - val_loss: 1738.9341 - val_mae: 1738.9341 - val_mse: 7049350.5000 - lr: 0.0010 - 287ms/epoch - 96ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 988.1772 - mae: 988.1772 - mse: 4313332.5000 - val_loss: 1915.4684 - val_mae: 1915.4684 - val_mse: 6061146.5000 - lr: 0.0010 - 314ms/epoch - 105ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 1240.9681 - mae: 1240.9681 - mse: 7464934.0000 - val_loss: 1711.2281 - val_mae: 1711.2281 - val_mse: 6846129.5000 - lr: 0.0010 - 277ms/epoch - 92ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 928.9657 - mae: 928.9658 - mse: 3751959.7500 - val_loss: 1665.8318 - val_mae: 1665.8318 - val_mse: 6259447.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 1030.5310 - mae: 1030.5310 - mse: 4825108.0000 - val_loss: 1679.0397 - val_mae: 1679.0397 - val_mse: 6229169.5000 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 1018.8286 - mae: 1018.8286 - mse: 4707061.0000 - val_loss: 1662.3444 - val_mae: 1662.3444 - val_mse: 6240240.0000 - lr: 0.0010 - 217ms/epoch - 72ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 1010.0982 - mae: 1010.0982 - mse: 4618970.5000 - val_loss: 1618.0245 - val_mae: 1618.0245 - val_mse: 5709380.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 981.0974 - mae: 981.0974 - mse: 4429672.0000 - val_loss: 2927.3472 - val_mae: 2927.3472 - val_mse: 13619971.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 1403.0753 - mae: 1403.0753 - mse: 7205991.5000 - val_loss: 1656.9012 - val_mae: 1656.9012 - val_mse: 6122600.5000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 1013.3079 - mae: 1013.3079 - mse: 4645042.0000 - val_loss: 1665.0682 - val_mae: 1665.0682 - val_mse: 6391241.0000 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 999.7455 - mae: 999.7455 - mse: 4526983.0000 - val_loss: 1601.3469 - val_mae: 1601.3469 - val_mse: 5929032.0000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 948.3497 - mae: 948.3497 - mse: 4095364.5000 - val_loss: 1389.3193 - val_mae: 1389.3193 - val_mse: 4531539.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 755.1390 - mae: 755.1390 - mse: 2683584.7500 - val_loss: 1360.9465 - val_mae: 1360.9465 - val_mse: 3478090.0000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 885.3325 - mae: 885.3325 - mse: 3270179.5000 - val_loss: 1980.0938 - val_mae: 1980.0938 - val_mse: 7313048.5000 - lr: 0.0010 - 215ms/epoch - 72ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 950.8888 - mae: 950.8888 - mse: 3840287.0000 - val_loss: 1637.7576 - val_mae: 1637.7576 - val_mse: 6302161.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 989.4123 - mae: 989.4123 - mse: 4443111.0000 - val_loss: 1592.6797 - val_mae: 1592.6797 - val_mse: 5910663.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 940.9999 - mae: 940.9999 - mse: 3946742.5000 - val_loss: 1467.9362 - val_mae: 1467.9362 - val_mse: 5354427.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 868.5740 - mae: 868.5740 - mse: 3249177.0000 - val_loss: 1574.9531 - val_mae: 1574.9531 - val_mse: 5667447.0000 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 932.6567 - mae: 932.6567 - mse: 3952344.2500 - val_loss: 1487.1451 - val_mae: 1487.1451 - val_mse: 4585363.5000 - lr: 0.0010 - 216ms/epoch - 72ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 895.2977 - mae: 895.2977 - mse: 3717068.2500 - val_loss: 1229.3065 - val_mae: 1229.3065 - val_mse: 3489219.2500 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 851.2189 - mae: 851.2189 - mse: 3257549.0000 - val_loss: 1895.6346 - val_mae: 1895.6346 - val_mse: 8011116.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 1192.3947 - mae: 1192.3947 - mse: 5977961.5000 - val_loss: 1704.7817 - val_mae: 1704.7817 - val_mse: 6752953.5000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 1006.9854 - mae: 1006.9854 - mse: 4642405.0000 - val_loss: 1777.1680 - val_mae: 1777.1680 - val_mse: 7338901.0000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 1055.3594 - mae: 1055.3594 - mse: 5029556.0000 - val_loss: 1801.5658 - val_mae: 1801.5658 - val_mse: 7589443.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 1032.4543 - mae: 1032.4543 - mse: 4786305.5000 - val_loss: 1707.0659 - val_mae: 1707.0659 - val_mse: 6763171.0000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 1019.7254 - mae: 1019.7254 - mse: 4680014.5000 - val_loss: 1690.7266 - val_mae: 1690.7266 - val_mse: 6658973.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 998.7827 - mae: 998.7827 - mse: 4487967.5000 - val_loss: 1709.9270 - val_mae: 1709.9270 - val_mse: 6719661.5000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 956.3757 - mae: 956.3757 - mse: 4056599.0000 - val_loss: 1253.5980 - val_mae: 1253.5980 - val_mse: 3168033.2500 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 755.0329 - mae: 755.0329 - mse: 2426788.2500 - val_loss: 17044.5293 - val_mae: 17044.5293 - val_mse: 360405344.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 2458.3660 - mae: 2458.3660 - mse: 13214740.0000 - val_loss: 1680.4630 - val_mae: 1680.4630 - val_mse: 6488314.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 1023.6431 - mae: 1023.6431 - mse: 4731835.0000 - val_loss: 1682.4175 - val_mae: 1682.4175 - val_mse: 6519307.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 1016.7216 - mae: 1016.7216 - mse: 4676902.5000 - val_loss: 1664.9818 - val_mae: 1664.9818 - val_mse: 6361084.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 1006.1983 - mae: 1006.1983 - mse: 4580114.5000 - val_loss: 1631.2327 - val_mae: 1631.2327 - val_mse: 5976608.0000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 984.0076 - mae: 984.0077 - mse: 4398997.5000 - val_loss: 1614.6011 - val_mae: 1614.6011 - val_mse: 5300653.5000 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 901.8328 - mae: 901.8329 - mse: 3707942.7500 - val_loss: 1327.6815 - val_mae: 1327.6815 - val_mse: 3827801.5000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 753.8098 - mae: 753.8098 - mse: 2867168.5000 - val_loss: 6954.8726 - val_mae: 6954.8726 - val_mse: 68941368.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 1589.0542 - mae: 1589.0542 - mse: 6189769.0000 - val_loss: 1637.4967 - val_mae: 1637.4967 - val_mse: 6247385.0000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 987.6506 - mae: 987.6506 - mse: 4320342.5000 - val_loss: 1781.0809 - val_mae: 1781.0809 - val_mse: 6917211.5000 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 942.8973 - mae: 942.8974 - mse: 3967055.5000 - val_loss: 1558.8247 - val_mae: 1558.8247 - val_mse: 6085931.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 895.2264 - mae: 895.2264 - mse: 3607738.7500 - val_loss: 8882.3291 - val_mae: 8882.3291 - val_mse: 101887032.0000 - lr: 0.0010 - 214ms/epoch - 71ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 1826.2360 - mae: 1826.2358 - mse: 7209948.0000 - val_loss: 1664.7599 - val_mae: 1664.7599 - val_mse: 6421767.0000 - lr: 0.0010 - 240ms/epoch - 80ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 1022.2896 - mae: 1022.2896 - mse: 4729343.5000 - val_loss: 1684.4377 - val_mae: 1684.4377 - val_mse: 6554068.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 1018.9286 - mae: 1018.9286 - mse: 4694732.5000 - val_loss: 1674.5684 - val_mae: 1674.5684 - val_mse: 6458861.0000 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 1013.1751 - mae: 1013.1751 - mse: 4639887.5000 - val_loss: 1654.5334 - val_mae: 1654.5334 - val_mse: 6312700.0000 - lr: 0.0010 - 277ms/epoch - 92ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 994.8808 - mae: 994.8807 - mse: 4481118.0000 - val_loss: 1575.4310 - val_mae: 1575.4310 - val_mse: 5682952.5000 - lr: 0.0010 - 259ms/epoch - 86ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 909.7247 - mae: 909.7247 - mse: 3725874.5000 - val_loss: 3742.8579 - val_mae: 3742.8579 - val_mse: 22961692.0000 - lr: 0.0010 - 293ms/epoch - 98ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 1496.8444 - mae: 1496.8444 - mse: 7426474.5000 - val_loss: 1714.8695 - val_mae: 1714.8695 - val_mse: 6816380.0000 - lr: 0.0010 - 255ms/epoch - 85ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 1021.5012 - mae: 1021.5012 - mse: 4710890.5000 - val_loss: 1669.9449 - val_mae: 1669.9449 - val_mse: 6414671.0000 - lr: 0.0010 - 271ms/epoch - 90ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 1002.3087 - mae: 1002.3087 - mse: 4561468.5000 - val_loss: 1603.6805 - val_mae: 1603.6805 - val_mse: 5904604.5000 - lr: 0.0010 - 290ms/epoch - 97ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 958.7488 - mae: 958.7488 - mse: 4186377.7500 - val_loss: 1655.6357 - val_mae: 1655.6357 - val_mse: 5428526.0000 - lr: 0.0010 - 306ms/epoch - 102ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 1033.6826 - mae: 1033.6826 - mse: 4689995.0000 - val_loss: 1551.1143 - val_mae: 1551.1143 - val_mse: 5091228.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 918.0068 - mae: 918.0067 - mse: 3869543.0000 - val_loss: 1280.2474 - val_mae: 1280.2474 - val_mse: 3279964.5000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 739.8781 - mae: 739.8781 - mse: 2816791.7500 - val_loss: 1498.1888 - val_mae: 1498.1888 - val_mse: 4709651.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 1009.5098 - mae: 1009.5098 - mse: 4634507.0000 - val_loss: 1701.3726 - val_mae: 1701.3726 - val_mse: 6737399.5000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 1003.3565 - mae: 1003.3565 - mse: 4489923.0000 - val_loss: 1707.2866 - val_mae: 1707.2866 - val_mse: 6758003.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 976.5139 - mae: 976.5139 - mse: 4247337.5000 - val_loss: 1812.5583 - val_mae: 1812.5583 - val_mse: 6990308.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 934.2587 - mae: 934.2587 - mse: 3590311.7500 - val_loss: 1836.2639 - val_mae: 1836.2639 - val_mse: 6695471.0000 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 1169.2839 - mae: 1169.2839 - mse: 5950294.0000 - val_loss: 1673.6488 - val_mae: 1673.6488 - val_mse: 6268253.5000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 1008.7325 - mae: 1008.7325 - mse: 4616279.0000 - val_loss: 1621.8365 - val_mae: 1621.8365 - val_mse: 5641899.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 948.8298 - mae: 948.8298 - mse: 4111218.0000 - val_loss: 3873.5017 - val_mae: 3873.5017 - val_mse: 19784094.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 1119.4896 - mae: 1119.4896 - mse: 4869187.5000 - val_loss: 1673.9979 - val_mae: 1673.9979 - val_mse: 6422651.5000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 1014.8133 - mae: 1014.8133 - mse: 4657221.5000 - val_loss: 1612.9120 - val_mae: 1612.9120 - val_mse: 5827071.5000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 993.6219 - mae: 993.6219 - mse: 4507886.0000 - val_loss: 1616.8810 - val_mae: 1616.8810 - val_mse: 5894133.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 982.0279 - mae: 982.0280 - mse: 4356996.0000 - val_loss: 1595.2983 - val_mae: 1595.2983 - val_mse: 5771032.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 1004.2700 - mae: 1004.2700 - mse: 4544813.0000 - val_loss: 3264.5283 - val_mae: 3264.5283 - val_mse: 17738272.0000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 1254.9594 - mae: 1254.9594 - mse: 5355735.5000 - val_loss: 1706.1731 - val_mae: 1706.1731 - val_mse: 6466165.0000 - lr: 0.0010 - 217ms/epoch - 72ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 1039.1906 - mae: 1039.1906 - mse: 4892035.0000 - val_loss: 1676.1428 - val_mae: 1676.1428 - val_mse: 6348696.0000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 1022.0592 - mae: 1022.0592 - mse: 4736762.5000 - val_loss: 1668.7896 - val_mae: 1668.7896 - val_mse: 6283848.0000 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 1013.8397 - mae: 1013.8397 - mse: 4656259.0000 - val_loss: 1639.5884 - val_mae: 1639.5884 - val_mse: 6038582.0000 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 986.7419 - mae: 986.7419 - mse: 4411995.5000 - val_loss: 1569.9016 - val_mae: 1569.9016 - val_mse: 5275026.0000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 951.5013 - mae: 951.5013 - mse: 4115666.2500 - val_loss: 3209.0510 - val_mae: 3209.0510 - val_mse: 15508224.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 1281.5159 - mae: 1281.5159 - mse: 5432541.5000 - val_loss: 1647.3307 - val_mae: 1647.3307 - val_mse: 6117784.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 1010.2826 - mae: 1010.2826 - mse: 4626336.0000 - val_loss: 1653.0187 - val_mae: 1653.0187 - val_mse: 6210340.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 1002.6823 - mae: 1002.6823 - mse: 4550016.0000 - val_loss: 1615.0944 - val_mae: 1615.0944 - val_mse: 5891838.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 961.0068 - mae: 961.0069 - mse: 4204990.0000 - val_loss: 1446.5920 - val_mae: 1446.5920 - val_mse: 4418673.5000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 867.5960 - mae: 867.5960 - mse: 3496430.0000 - val_loss: 1658.2679 - val_mae: 1658.2679 - val_mse: 6192399.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 1028.2594 - mae: 1028.2594 - mse: 4793626.5000 - val_loss: 1681.1890 - val_mae: 1681.1890 - val_mse: 6408323.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 1024.0206 - mae: 1024.0206 - mse: 4752220.0000 - val_loss: 1681.2356 - val_mae: 1681.2356 - val_mse: 6429132.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 1024.4686 - mae: 1024.4686 - mse: 4754121.0000 - val_loss: 1678.7191 - val_mae: 1678.7191 - val_mse: 6441051.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 1019.1388 - mae: 1019.1388 - mse: 4697540.5000 - val_loss: 1680.6451 - val_mae: 1680.6451 - val_mse: 6513494.5000 - lr: 0.0010 - 213ms/epoch - 71ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 1015.9413 - mae: 1015.9413 - mse: 4664699.5000 - val_loss: 1677.4207 - val_mae: 1677.4207 - val_mse: 6433036.5000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 1020.5453 - mae: 1020.5453 - mse: 4712004.0000 - val_loss: 1680.6188 - val_mae: 1680.6188 - val_mse: 6511719.5000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 1017.6149 - mae: 1017.6149 - mse: 4675983.5000 - val_loss: 1679.8076 - val_mae: 1679.8076 - val_mse: 6536152.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 1030.0215 - mae: 1030.0215 - mse: 4718212.0000 - val_loss: 1725.3630 - val_mae: 1725.3630 - val_mse: 6937008.0000 - lr: 0.0010 - 256ms/epoch - 85ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 1067.5625 - mae: 1067.5625 - mse: 5018934.0000 - val_loss: 1686.5143 - val_mae: 1686.5143 - val_mse: 6348827.5000 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 1050.2867 - mae: 1050.2867 - mse: 4972902.5000 - val_loss: 1721.8156 - val_mae: 1721.8156 - val_mse: 6908706.5000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 1023.3834 - mae: 1023.3834 - mse: 4696024.0000 - val_loss: 1689.3940 - val_mae: 1689.3940 - val_mse: 6617020.0000 - lr: 0.0010 - 243ms/epoch - 81ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 1023.9645 - mae: 1023.9645 - mse: 4760635.5000 - val_loss: 1676.5261 - val_mae: 1676.5261 - val_mse: 6376842.5000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 1020.3652 - mae: 1020.3652 - mse: 4710678.0000 - val_loss: 1688.4709 - val_mae: 1688.4709 - val_mse: 6604468.0000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 1019.3734 - mae: 1019.3734 - mse: 4684896.0000 - val_loss: 1705.4166 - val_mae: 1705.4166 - val_mse: 6780401.0000 - lr: 0.0010 - 221ms/epoch - 74ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 1015.6958 - mae: 1015.6958 - mse: 4649996.5000 - val_loss: 1695.1852 - val_mae: 1695.1852 - val_mse: 6694385.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 1009.8507 - mae: 1009.8507 - mse: 4598996.0000 - val_loss: 1677.1914 - val_mae: 1677.1914 - val_mse: 6562434.5000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 996.3996 - mae: 996.3996 - mse: 4481433.0000 - val_loss: 1733.2266 - val_mae: 1733.2266 - val_mse: 6887284.0000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 1012.7258 - mae: 1012.7258 - mse: 4508905.0000 - val_loss: 1701.0575 - val_mae: 1701.0575 - val_mse: 6707338.0000 - lr: 0.0010 - 284ms/epoch - 95ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 988.2254 - mae: 988.2254 - mse: 4409964.0000 - val_loss: 1606.9824 - val_mae: 1606.9824 - val_mse: 6098761.5000 - lr: 0.0010 - 244ms/epoch - 81ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 956.0118 - mae: 956.0118 - mse: 4113785.7500 - val_loss: 1700.3417 - val_mae: 1700.3417 - val_mse: 6159430.0000 - lr: 0.0010 - 244ms/epoch - 81ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 994.0928 - mae: 994.0928 - mse: 3925606.0000 - val_loss: 1568.3254 - val_mae: 1568.3254 - val_mse: 5813125.0000 - lr: 0.0010 - 240ms/epoch - 80ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 999.0010 - mae: 999.0010 - mse: 4530840.0000 - val_loss: 1625.8533 - val_mae: 1625.8533 - val_mse: 5703978.5000 - lr: 0.0010 - 262ms/epoch - 87ms/step\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 138: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 1005.7874 - mae: 1005.7874 - mse: 4545061.0000 - val_loss: 1605.7611 - val_mae: 1605.7611 - val_mse: 5688821.5000 - lr: 0.0010 - 278ms/epoch - 93ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 988.2504 - mae: 988.2504 - mse: 4456239.5000 - val_loss: 1606.5933 - val_mae: 1606.5933 - val_mse: 5754182.5000 - lr: 1.0000e-04 - 297ms/epoch - 99ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 981.1340 - mae: 981.1340 - mse: 4398094.0000 - val_loss: 1574.2023 - val_mae: 1574.2023 - val_mse: 5554179.0000 - lr: 1.0000e-04 - 289ms/epoch - 96ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 956.6393 - mae: 956.6393 - mse: 4210176.5000 - val_loss: 1480.8956 - val_mae: 1480.8956 - val_mse: 5063932.5000 - lr: 1.0000e-04 - 266ms/epoch - 89ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 923.8004 - mae: 923.8004 - mse: 3970271.0000 - val_loss: 1386.5979 - val_mae: 1386.5979 - val_mse: 4479224.5000 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 876.1950 - mae: 876.1950 - mse: 3603503.7500 - val_loss: 1316.8644 - val_mae: 1316.8644 - val_mse: 3772377.7500 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 804.9348 - mae: 804.9348 - mse: 3103124.5000 - val_loss: 1113.3342 - val_mae: 1113.3342 - val_mse: 2757325.7500 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 715.8802 - mae: 715.8802 - mse: 2505114.0000 - val_loss: 1496.0908 - val_mae: 1496.0908 - val_mse: 3721610.2500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 937.7672 - mae: 937.7672 - mse: 3526518.0000 - val_loss: 1844.8209 - val_mae: 1844.8209 - val_mse: 5451353.5000 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 1593.1865 - mae: 1593.1865 - mse: 9750449.0000 - val_loss: 2872.2168 - val_mae: 2872.2168 - val_mse: 14534415.0000 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 996.0681 - mae: 996.0681 - mse: 3956785.2500 - val_loss: 1439.5621 - val_mae: 1439.5621 - val_mse: 4157957.0000 - lr: 1.0000e-04 - 210ms/epoch - 70ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 812.8436 - mae: 812.8436 - mse: 3260421.7500 - val_loss: 1235.5778 - val_mae: 1235.5778 - val_mse: 3389310.7500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 830.8523 - mae: 830.8523 - mse: 3360360.7500 - val_loss: 1342.4479 - val_mae: 1342.4479 - val_mse: 3560529.5000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 812.2423 - mae: 812.2423 - mse: 3228744.0000 - val_loss: 1287.8511 - val_mae: 1287.8511 - val_mse: 3274596.5000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 769.1783 - mae: 769.1783 - mse: 3030468.5000 - val_loss: 1145.6198 - val_mae: 1145.6198 - val_mse: 2863980.5000 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 731.8476 - mae: 731.8476 - mse: 2894819.5000 - val_loss: 1058.3203 - val_mae: 1058.3203 - val_mse: 2612574.5000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 709.5841 - mae: 709.5841 - mse: 2818034.5000 - val_loss: 1037.9177 - val_mae: 1037.9177 - val_mse: 2517238.5000 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 707.5920 - mae: 707.5920 - mse: 2797691.2500 - val_loss: 1032.3878 - val_mae: 1032.3878 - val_mse: 2505191.5000 - lr: 1.0000e-04 - 209ms/epoch - 70ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 707.1577 - mae: 707.1577 - mse: 2778537.0000 - val_loss: 1023.1933 - val_mae: 1023.1933 - val_mse: 2485728.5000 - lr: 1.0000e-04 - 217ms/epoch - 72ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 696.7427 - mae: 696.7427 - mse: 2731168.0000 - val_loss: 986.9490 - val_mae: 986.9490 - val_mse: 2356565.7500 - lr: 1.0000e-04 - 340ms/epoch - 113ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 677.8020 - mae: 677.8020 - mse: 2659012.0000 - val_loss: 943.2236 - val_mae: 943.2236 - val_mse: 2220582.0000 - lr: 1.0000e-04 - 350ms/epoch - 117ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 664.3514 - mae: 664.3514 - mse: 2597616.2500 - val_loss: 914.7349 - val_mae: 914.7349 - val_mse: 2134345.0000 - lr: 1.0000e-04 - 335ms/epoch - 112ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 655.8611 - mae: 655.8611 - mse: 2545215.7500 - val_loss: 902.9869 - val_mae: 902.9869 - val_mse: 2057568.3750 - lr: 1.0000e-04 - 344ms/epoch - 115ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 645.0529 - mae: 645.0529 - mse: 2474364.0000 - val_loss: 875.4212 - val_mae: 875.4212 - val_mse: 1958678.6250 - lr: 1.0000e-04 - 345ms/epoch - 115ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 629.3497 - mae: 629.3497 - mse: 2374158.5000 - val_loss: 839.2877 - val_mae: 839.2877 - val_mse: 1816620.6250 - lr: 1.0000e-04 - 346ms/epoch - 115ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 608.4888 - mae: 608.4888 - mse: 2229736.5000 - val_loss: 804.7747 - val_mae: 804.7747 - val_mse: 1645899.2500 - lr: 1.0000e-04 - 338ms/epoch - 113ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 581.6251 - mae: 581.6251 - mse: 2027852.1250 - val_loss: 779.3621 - val_mae: 779.3621 - val_mse: 1474772.0000 - lr: 1.0000e-04 - 338ms/epoch - 113ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 547.4304 - mae: 547.4304 - mse: 1759775.6250 - val_loss: 755.0142 - val_mae: 755.0142 - val_mse: 1319689.3750 - lr: 1.0000e-04 - 348ms/epoch - 116ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 507.0652 - mae: 507.0652 - mse: 1454002.5000 - val_loss: 720.8490 - val_mae: 720.8490 - val_mse: 1200936.2500 - lr: 1.0000e-04 - 343ms/epoch - 114ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 462.0135 - mae: 462.0134 - mse: 1173490.7500 - val_loss: 721.4175 - val_mae: 721.4175 - val_mse: 1265810.1250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 452.1052 - mae: 452.1052 - mse: 1051809.3750 - val_loss: 769.8856 - val_mae: 769.8856 - val_mse: 1386847.8750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 452.7253 - mae: 452.7253 - mse: 1023948.2500 - val_loss: 728.6277 - val_mae: 728.6277 - val_mse: 1291916.1250 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 450.5532 - mae: 450.5532 - mse: 1003738.3125 - val_loss: 668.5181 - val_mae: 668.5181 - val_mse: 1201883.0000 - lr: 1.0000e-04 - 342ms/epoch - 114ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 428.0809 - mae: 428.0809 - mse: 961837.0625 - val_loss: 653.9737 - val_mae: 653.9737 - val_mse: 1095183.1250 - lr: 1.0000e-04 - 343ms/epoch - 114ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 412.3556 - mae: 412.3556 - mse: 874139.8125 - val_loss: 670.2477 - val_mae: 670.2477 - val_mse: 1158664.1250 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 398.8633 - mae: 398.8633 - mse: 829097.6250 - val_loss: 660.5153 - val_mae: 660.5153 - val_mse: 1109408.3750 - lr: 1.0000e-04 - 269ms/epoch - 90ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 385.8253 - mae: 385.8253 - mse: 772298.6250 - val_loss: 632.8652 - val_mae: 632.8652 - val_mse: 1079727.0000 - lr: 1.0000e-04 - 422ms/epoch - 141ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 383.4269 - mae: 383.4269 - mse: 764729.0000 - val_loss: 637.5022 - val_mae: 637.5022 - val_mse: 1076866.7500 - lr: 1.0000e-04 - 254ms/epoch - 85ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 385.0819 - mae: 385.0819 - mse: 759244.8750 - val_loss: 632.8726 - val_mae: 632.8726 - val_mse: 1078853.0000 - lr: 1.0000e-04 - 309ms/epoch - 103ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 392.6727 - mae: 392.6727 - mse: 767482.8750 - val_loss: 642.0085 - val_mae: 642.0085 - val_mse: 1072235.0000 - lr: 1.0000e-04 - 273ms/epoch - 91ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 436.9446 - mae: 436.9446 - mse: 917995.3125 - val_loss: 634.3416 - val_mae: 634.3416 - val_mse: 1058052.0000 - lr: 1.0000e-04 - 283ms/epoch - 94ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 440.6487 - mae: 440.6487 - mse: 907586.5000 - val_loss: 707.9821 - val_mae: 707.9821 - val_mse: 1286524.1250 - lr: 1.0000e-04 - 294ms/epoch - 98ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 393.3045 - mae: 393.3045 - mse: 791874.1250 - val_loss: 673.5507 - val_mae: 673.5507 - val_mse: 1109551.8750 - lr: 1.0000e-04 - 262ms/epoch - 87ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 383.8405 - mae: 383.8405 - mse: 736410.3750 - val_loss: 642.5696 - val_mae: 642.5696 - val_mse: 1107737.0000 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 432.3764 - mae: 432.3764 - mse: 881169.0000 - val_loss: 644.2023 - val_mae: 644.2023 - val_mse: 1111616.3750 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 430.8648 - mae: 430.8648 - mse: 905361.8750 - val_loss: 676.4358 - val_mae: 676.4358 - val_mse: 1115864.6250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 395.0810 - mae: 395.0810 - mse: 776761.5625 - val_loss: 699.5076 - val_mae: 699.5076 - val_mse: 1281801.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 386.0176 - mae: 386.0176 - mse: 733948.5000 - val_loss: 625.8800 - val_mae: 625.8800 - val_mse: 1067138.7500 - lr: 1.0000e-04 - 345ms/epoch - 115ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 410.6865 - mae: 410.6865 - mse: 827197.3750 - val_loss: 669.9869 - val_mae: 669.9869 - val_mse: 1130989.1250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 395.5322 - mae: 395.5322 - mse: 774762.0625 - val_loss: 636.8732 - val_mae: 636.8732 - val_mse: 1127673.3750 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 371.9762 - mae: 371.9762 - mse: 714417.5000 - val_loss: 647.9295 - val_mae: 647.9295 - val_mse: 1081272.0000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 368.8214 - mae: 368.8214 - mse: 698897.6250 - val_loss: 619.0179 - val_mae: 619.0179 - val_mse: 1077385.1250 - lr: 1.0000e-04 - 323ms/epoch - 108ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 372.7000 - mae: 372.7000 - mse: 702188.3750 - val_loss: 638.7549 - val_mae: 638.7549 - val_mse: 1094821.8750 - lr: 1.0000e-04 - 224ms/epoch - 75ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 376.5588 - mae: 376.5588 - mse: 718843.5000 - val_loss: 638.5039 - val_mae: 638.5039 - val_mse: 1079357.6250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 361.2526 - mae: 361.2526 - mse: 671416.8125 - val_loss: 636.1588 - val_mae: 636.1588 - val_mse: 1112172.0000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 355.3098 - mae: 355.3098 - mse: 657817.1875 - val_loss: 620.3344 - val_mae: 620.3344 - val_mse: 1055188.5000 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 355.6558 - mae: 355.6558 - mse: 656435.0625 - val_loss: 621.9145 - val_mae: 621.9145 - val_mse: 1065916.8750 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 357.2286 - mae: 357.2286 - mse: 658705.0625 - val_loss: 624.5865 - val_mae: 624.5865 - val_mse: 1068846.7500 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 360.0661 - mae: 360.0661 - mse: 667711.2500 - val_loss: 625.9601 - val_mae: 625.9601 - val_mse: 1060004.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 353.8672 - mae: 353.8672 - mse: 650265.6250 - val_loss: 623.2547 - val_mae: 623.2547 - val_mse: 1069020.6250 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 355.0458 - mae: 355.0458 - mse: 652716.5625 - val_loss: 624.4009 - val_mae: 624.4009 - val_mse: 1059979.1250 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 350.5721 - mae: 350.5721 - mse: 641760.1875 - val_loss: 623.2496 - val_mae: 623.2496 - val_mse: 1073259.2500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 350.5306 - mae: 350.5306 - mse: 640612.0625 - val_loss: 620.1470 - val_mae: 620.1470 - val_mse: 1060663.6250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 348.3533 - mae: 348.3533 - mse: 635674.0000 - val_loss: 625.0016 - val_mae: 625.0016 - val_mse: 1080197.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 347.8229 - mae: 347.8229 - mse: 632098.8125 - val_loss: 622.9594 - val_mae: 622.9594 - val_mse: 1069886.6250 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 347.5255 - mae: 347.5255 - mse: 632276.7500 - val_loss: 624.7205 - val_mae: 624.7205 - val_mse: 1083068.3750 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 350.5726 - mae: 350.5726 - mse: 636990.5625 - val_loss: 624.1079 - val_mae: 624.1079 - val_mse: 1064036.5000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 368.3729 - mae: 368.3729 - mse: 684073.6875 - val_loss: 627.9792 - val_mae: 627.9792 - val_mse: 1068515.2500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 368.9154 - mae: 368.9154 - mse: 682575.3125 - val_loss: 625.8796 - val_mae: 625.8796 - val_mse: 1099696.2500 - lr: 1.0000e-04 - 213ms/epoch - 71ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 362.4366 - mae: 362.4366 - mse: 671981.1875 - val_loss: 634.9196 - val_mae: 634.9196 - val_mse: 1067505.6250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 346.1678 - mae: 346.1678 - mse: 628696.2500 - val_loss: 628.4945 - val_mae: 628.4945 - val_mse: 1090961.0000 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 346.5846 - mae: 346.5846 - mse: 623276.0000 - val_loss: 628.7208 - val_mae: 628.7208 - val_mse: 1076256.3750 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 344.4928 - mae: 344.4928 - mse: 621740.3125 - val_loss: 628.7603 - val_mae: 628.7603 - val_mse: 1093496.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 350.0853 - mae: 350.0853 - mse: 633407.0000 - val_loss: 622.1779 - val_mae: 622.1779 - val_mse: 1065631.3750 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 363.2241 - mae: 363.2241 - mse: 668714.5000 - val_loss: 631.9036 - val_mae: 631.9036 - val_mse: 1071262.3750 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 362.9022 - mae: 362.9022 - mse: 664283.9375 - val_loss: 622.0704 - val_mae: 622.0704 - val_mse: 1079728.0000 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 366.5476 - mae: 366.5476 - mse: 682308.2500 - val_loss: 637.6665 - val_mae: 637.6665 - val_mse: 1067342.1250 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 347.6064 - mae: 347.6064 - mse: 630480.8125 - val_loss: 627.0294 - val_mae: 627.0294 - val_mse: 1091968.6250 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 344.4445 - mae: 344.4445 - mse: 617219.2500 - val_loss: 632.0115 - val_mae: 632.0115 - val_mse: 1078913.1250 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 343.1569 - mae: 343.1569 - mse: 613777.5000 - val_loss: 625.9545 - val_mae: 625.9545 - val_mse: 1076809.0000 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 343.4824 - mae: 343.4824 - mse: 620344.4375 - val_loss: 620.0867 - val_mae: 620.0867 - val_mse: 1069905.3750 - lr: 1.0000e-04 - 215ms/epoch - 72ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 341.1249 - mae: 341.1249 - mse: 611702.4375 - val_loss: 628.8489 - val_mae: 628.8489 - val_mse: 1083522.0000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 344.2937 - mae: 344.2937 - mse: 616495.8750 - val_loss: 623.6334 - val_mae: 623.6334 - val_mse: 1065585.8750 - lr: 1.0000e-04 - 277ms/epoch - 92ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 357.7863 - mae: 357.7863 - mse: 656738.1875 - val_loss: 622.5051 - val_mae: 622.5051 - val_mse: 1061457.1250 - lr: 1.0000e-04 - 259ms/epoch - 86ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 349.7674 - mae: 349.7674 - mse: 634379.4375 - val_loss: 624.6786 - val_mae: 624.6786 - val_mse: 1086293.6250 - lr: 1.0000e-04 - 254ms/epoch - 85ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 347.6263 - mae: 347.6263 - mse: 626514.3750 - val_loss: 633.9634 - val_mae: 633.9634 - val_mse: 1078208.8750 - lr: 1.0000e-04 - 261ms/epoch - 87ms/step\n",
            "Epoch 224/300\n",
            "3/3 - 0s - loss: 348.2980 - mae: 348.2980 - mse: 624949.3125 - val_loss: 626.5982 - val_mae: 626.5982 - val_mse: 1072362.0000 - lr: 1.0000e-04 - 288ms/epoch - 96ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 354.7054 - mae: 354.7054 - mse: 647725.2500 - val_loss: 623.3770 - val_mae: 623.3770 - val_mse: 1060103.0000 - lr: 1.0000e-04 - 310ms/epoch - 103ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 346.6250 - mae: 346.6250 - mse: 626097.8125 - val_loss: 627.9504 - val_mae: 627.9504 - val_mse: 1089045.2500 - lr: 1.0000e-04 - 306ms/epoch - 102ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 344.9334 - mae: 344.9334 - mse: 617472.0625 - val_loss: 637.9111 - val_mae: 637.9111 - val_mse: 1084658.2500 - lr: 1.0000e-04 - 278ms/epoch - 93ms/step\n",
            "Epoch 228/300\n",
            "3/3 - 0s - loss: 338.1459 - mae: 338.1459 - mse: 602958.5000 - val_loss: 626.1025 - val_mae: 626.1025 - val_mse: 1083858.8750 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 229/300\n",
            "3/3 - 0s - loss: 343.9486 - mae: 343.9486 - mse: 617925.0625 - val_loss: 620.7480 - val_mae: 620.7480 - val_mse: 1068268.1250 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 230/300\n",
            "3/3 - 0s - loss: 342.4853 - mae: 342.4853 - mse: 612638.5000 - val_loss: 633.0426 - val_mae: 633.0426 - val_mse: 1085103.6250 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 231/300\n",
            "3/3 - 0s - loss: 352.1169 - mae: 352.1169 - mse: 632102.0625 - val_loss: 631.0191 - val_mae: 631.0191 - val_mse: 1077903.1250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 232/300\n",
            "3/3 - 0s - loss: 366.7143 - mae: 366.7143 - mse: 677875.6250 - val_loss: 629.1056 - val_mae: 629.1056 - val_mse: 1067738.3750 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 233/300\n",
            "3/3 - 0s - loss: 359.7148 - mae: 359.7148 - mse: 659905.0625 - val_loss: 640.9439 - val_mae: 640.9439 - val_mse: 1141473.2500 - lr: 1.0000e-04 - 220ms/epoch - 73ms/step\n",
            "Epoch 234/300\n",
            "3/3 - 0s - loss: 342.5158 - mae: 342.5158 - mse: 612829.7500 - val_loss: 630.6648 - val_mae: 630.6648 - val_mse: 1068891.3750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 235/300\n",
            "3/3 - 0s - loss: 340.4141 - mae: 340.4141 - mse: 604037.5000 - val_loss: 632.5771 - val_mae: 632.5771 - val_mse: 1083127.6250 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 236/300\n",
            "3/3 - 0s - loss: 336.8135 - mae: 336.8135 - mse: 600671.8125 - val_loss: 625.4042 - val_mae: 625.4042 - val_mse: 1075122.2500 - lr: 1.0000e-04 - 224ms/epoch - 75ms/step\n",
            "Epoch 237/300\n",
            "3/3 - 0s - loss: 336.4771 - mae: 336.4771 - mse: 599093.0000 - val_loss: 627.7654 - val_mae: 627.7654 - val_mse: 1079478.8750 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 238/300\n",
            "3/3 - 0s - loss: 338.7628 - mae: 338.7628 - mse: 601912.7500 - val_loss: 627.0671 - val_mae: 627.0671 - val_mse: 1074838.3750 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 239/300\n",
            "3/3 - 0s - loss: 341.2596 - mae: 341.2596 - mse: 610863.8125 - val_loss: 623.4687 - val_mae: 623.4687 - val_mse: 1074344.7500 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 240/300\n",
            "3/3 - 0s - loss: 343.8322 - mae: 343.8322 - mse: 616676.0000 - val_loss: 626.3409 - val_mae: 626.3409 - val_mse: 1073172.6250 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 241/300\n",
            "3/3 - 0s - loss: 354.2277 - mae: 354.2277 - mse: 641843.5000 - val_loss: 634.0344 - val_mae: 634.0344 - val_mse: 1079766.1250 - lr: 1.0000e-04 - 192ms/epoch - 64ms/step\n",
            "Epoch 242/300\n",
            "3/3 - 0s - loss: 361.4854 - mae: 361.4854 - mse: 655824.1250 - val_loss: 627.5613 - val_mae: 627.5613 - val_mse: 1086633.0000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 243/300\n",
            "3/3 - 0s - loss: 363.4005 - mae: 363.4005 - mse: 670673.0625 - val_loss: 636.2412 - val_mae: 636.2412 - val_mse: 1071035.0000 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 244/300\n",
            "3/3 - 0s - loss: 346.0549 - mae: 346.0549 - mse: 625014.3750 - val_loss: 635.8342 - val_mae: 635.8342 - val_mse: 1116184.5000 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 245/300\n",
            "3/3 - 0s - loss: 335.6775 - mae: 335.6775 - mse: 594406.8750 - val_loss: 635.8500 - val_mae: 635.8500 - val_mse: 1083037.3750 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 246/300\n",
            "3/3 - 0s - loss: 336.6513 - mae: 336.6513 - mse: 594330.5000 - val_loss: 628.6673 - val_mae: 628.6673 - val_mse: 1077179.2500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 247/300\n",
            "3/3 - 0s - loss: 337.3059 - mae: 337.3059 - mse: 601676.3750 - val_loss: 622.5168 - val_mae: 622.5168 - val_mse: 1073857.2500 - lr: 1.0000e-04 - 210ms/epoch - 70ms/step\n",
            "Epoch 248/300\n",
            "3/3 - 0s - loss: 334.6019 - mae: 334.6019 - mse: 592258.6875 - val_loss: 633.9052 - val_mae: 633.9052 - val_mse: 1088904.2500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 249/300\n",
            "3/3 - 0s - loss: 342.5621 - mae: 342.5621 - mse: 605894.1875 - val_loss: 632.2357 - val_mae: 632.2357 - val_mse: 1080477.3750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 250/300\n",
            "3/3 - 0s - loss: 345.5372 - mae: 345.5372 - mse: 620629.8750 - val_loss: 622.3160 - val_mae: 622.3160 - val_mse: 1066228.7500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 251/300\n",
            "3/3 - 0s - loss: 346.1730 - mae: 346.1730 - mse: 623788.7500 - val_loss: 626.5471 - val_mae: 626.5471 - val_mse: 1088777.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 252/300\n",
            "3/3 - 0s - loss: 343.6929 - mae: 343.6929 - mse: 614206.3750 - val_loss: 634.7972 - val_mae: 634.7972 - val_mse: 1080692.1250 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 253/300\n",
            "3/3 - 0s - loss: 346.4908 - mae: 346.4908 - mse: 616984.5625 - val_loss: 630.7510 - val_mae: 630.7510 - val_mse: 1080985.5000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 254/300\n",
            "3/3 - 0s - loss: 351.6231 - mae: 351.6231 - mse: 635538.3125 - val_loss: 623.4702 - val_mae: 623.4702 - val_mse: 1062993.7500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 255/300\n",
            "3/3 - 0s - loss: 353.6046 - mae: 353.6046 - mse: 641889.4375 - val_loss: 632.3453 - val_mae: 632.3453 - val_mse: 1108147.1250 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 256/300\n",
            "3/3 - 0s - loss: 343.3062 - mae: 343.3062 - mse: 614636.7500 - val_loss: 639.4385 - val_mae: 639.4385 - val_mse: 1080809.0000 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 257/300\n",
            "3/3 - 0s - loss: 341.5300 - mae: 341.5300 - mse: 604583.6250 - val_loss: 633.1746 - val_mae: 633.1746 - val_mse: 1092769.6250 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 258/300\n",
            "3/3 - 0s - loss: 334.9942 - mae: 334.9942 - mse: 592565.0000 - val_loss: 629.7575 - val_mae: 629.7575 - val_mse: 1084425.2500 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 259/300\n",
            "3/3 - 0s - loss: 336.2702 - mae: 336.2702 - mse: 597332.7500 - val_loss: 634.8558 - val_mae: 634.8558 - val_mse: 1095228.2500 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 260/300\n",
            "3/3 - 0s - loss: 334.1999 - mae: 334.1999 - mse: 587806.1875 - val_loss: 633.2438 - val_mae: 633.2438 - val_mse: 1083822.2500 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 261/300\n",
            "3/3 - 0s - loss: 333.8232 - mae: 333.8232 - mse: 590104.5625 - val_loss: 626.6068 - val_mae: 626.6068 - val_mse: 1078932.0000 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 262/300\n",
            "3/3 - 0s - loss: 337.4996 - mae: 337.4996 - mse: 598503.3125 - val_loss: 627.2654 - val_mae: 627.2654 - val_mse: 1074565.1250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 263/300\n",
            "3/3 - 0s - loss: 337.0038 - mae: 337.0038 - mse: 599762.3125 - val_loss: 633.2322 - val_mae: 633.2322 - val_mse: 1081134.0000 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 264/300\n",
            "3/3 - 0s - loss: 349.1123 - mae: 349.1123 - mse: 623018.8750 - val_loss: 633.4413 - val_mae: 633.4413 - val_mse: 1085146.7500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 265/300\n",
            "3/3 - 0s - loss: 360.5952 - mae: 360.5952 - mse: 652029.1250 - val_loss: 629.7950 - val_mae: 629.7950 - val_mse: 1101576.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 266/300\n",
            "3/3 - 0s - loss: 361.7176 - mae: 361.7176 - mse: 665405.5625 - val_loss: 643.9330 - val_mae: 643.9330 - val_mse: 1081395.1250 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 267/300\n",
            "3/3 - 0s - loss: 336.6905 - mae: 336.6905 - mse: 600526.1875 - val_loss: 642.3372 - val_mae: 642.3372 - val_mse: 1129418.3750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 268/300\n",
            "3/3 - 0s - loss: 342.6472 - mae: 342.6472 - mse: 601094.9375 - val_loss: 643.7269 - val_mae: 643.7269 - val_mse: 1098914.5000 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 269/300\n",
            "3/3 - 0s - loss: 350.7868 - mae: 350.7868 - mse: 625976.5000 - val_loss: 626.8658 - val_mae: 626.8658 - val_mse: 1076470.1250 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 270/300\n",
            "3/3 - 0s - loss: 358.1208 - mae: 358.1208 - mse: 652106.0625 - val_loss: 647.1506 - val_mae: 647.1506 - val_mse: 1156377.7500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 271/300\n",
            "3/3 - 0s - loss: 340.9953 - mae: 340.9953 - mse: 606948.3750 - val_loss: 635.8329 - val_mae: 635.8329 - val_mse: 1070842.6250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 272/300\n",
            "3/3 - 0s - loss: 331.5533 - mae: 331.5533 - mse: 580595.5000 - val_loss: 640.7685 - val_mae: 640.7685 - val_mse: 1104949.1250 - lr: 1.0000e-04 - 268ms/epoch - 89ms/step\n",
            "Epoch 273/300\n",
            "3/3 - 0s - loss: 335.4206 - mae: 335.4206 - mse: 590337.1875 - val_loss: 628.2342 - val_mae: 628.2342 - val_mse: 1073229.3750 - lr: 1.0000e-04 - 250ms/epoch - 83ms/step\n",
            "Epoch 274/300\n",
            "3/3 - 0s - loss: 353.4289 - mae: 353.4289 - mse: 639690.3125 - val_loss: 628.5278 - val_mae: 628.5278 - val_mse: 1071653.7500 - lr: 1.0000e-04 - 248ms/epoch - 83ms/step\n",
            "Epoch 275/300\n",
            "3/3 - 0s - loss: 345.4462 - mae: 345.4462 - mse: 618662.3125 - val_loss: 642.5676 - val_mae: 642.5676 - val_mse: 1129305.8750 - lr: 1.0000e-04 - 233ms/epoch - 78ms/step\n",
            "Epoch 276/300\n",
            "3/3 - 0s - loss: 332.7276 - mae: 332.7276 - mse: 582430.2500 - val_loss: 643.4839 - val_mae: 643.4839 - val_mse: 1088401.0000 - lr: 1.0000e-04 - 314ms/epoch - 105ms/step\n",
            "Epoch 277/300\n",
            "3/3 - 0s - loss: 330.1389 - mae: 330.1389 - mse: 577866.5625 - val_loss: 629.3173 - val_mae: 629.3173 - val_mse: 1084280.6250 - lr: 1.0000e-04 - 289ms/epoch - 96ms/step\n",
            "Epoch 278/300\n",
            "3/3 - 0s - loss: 334.7707 - mae: 334.7707 - mse: 592845.6875 - val_loss: 625.6240 - val_mae: 625.6240 - val_mse: 1079846.8750 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 279/300\n",
            "3/3 - 0s - loss: 336.3720 - mae: 336.3720 - mse: 592971.3125 - val_loss: 634.7623 - val_mae: 634.7623 - val_mse: 1085346.0000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 280/300\n",
            "3/3 - 0s - loss: 342.1890 - mae: 342.1890 - mse: 603592.7500 - val_loss: 633.1605 - val_mae: 633.1605 - val_mse: 1083931.8750 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 281/300\n",
            "3/3 - 0s - loss: 353.2155 - mae: 353.2155 - mse: 637020.3750 - val_loss: 629.4244 - val_mae: 629.4244 - val_mse: 1066820.7500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 282/300\n",
            "3/3 - 0s - loss: 344.2896 - mae: 344.2896 - mse: 618632.0625 - val_loss: 641.5449 - val_mae: 641.5449 - val_mse: 1128545.2500 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 283/300\n",
            "3/3 - 0s - loss: 331.3837 - mae: 331.3837 - mse: 580147.3125 - val_loss: 644.2555 - val_mae: 644.2555 - val_mse: 1086054.1250 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 284/300\n",
            "3/3 - 0s - loss: 330.3134 - mae: 330.3134 - mse: 575591.1250 - val_loss: 632.6697 - val_mae: 632.6697 - val_mse: 1085413.5000 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 285/300\n",
            "3/3 - 0s - loss: 336.6790 - mae: 336.6790 - mse: 595277.3125 - val_loss: 628.1155 - val_mae: 628.1155 - val_mse: 1079327.7500 - lr: 1.0000e-04 - 191ms/epoch - 64ms/step\n",
            "Epoch 286/300\n",
            "3/3 - 0s - loss: 340.7778 - mae: 340.7778 - mse: 602275.2500 - val_loss: 632.4543 - val_mae: 632.4543 - val_mse: 1076380.0000 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 287/300\n",
            "3/3 - 0s - loss: 341.7319 - mae: 341.7319 - mse: 604049.9375 - val_loss: 632.9739 - val_mae: 632.9739 - val_mse: 1089524.0000 - lr: 1.0000e-04 - 210ms/epoch - 70ms/step\n",
            "Epoch 288/300\n",
            "3/3 - 0s - loss: 345.5143 - mae: 345.5143 - mse: 615552.2500 - val_loss: 631.1658 - val_mae: 631.1658 - val_mse: 1068031.1250 - lr: 1.0000e-04 - 191ms/epoch - 64ms/step\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 289: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 337.0798 - mae: 337.0798 - mse: 600088.5000 - val_loss: 636.1364 - val_mae: 636.1364 - val_mse: 1107100.1250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 290/300\n",
            "3/3 - 0s - loss: 326.3394 - mae: 326.3394 - mse: 570423.6875 - val_loss: 632.6284 - val_mae: 632.6284 - val_mse: 1089603.3750 - lr: 1.0000e-05 - 199ms/epoch - 66ms/step\n",
            "Epoch 291/300\n",
            "3/3 - 0s - loss: 325.6821 - mae: 325.6821 - mse: 568352.5000 - val_loss: 632.7817 - val_mae: 632.7817 - val_mse: 1080933.2500 - lr: 1.0000e-05 - 201ms/epoch - 67ms/step\n",
            "Epoch 292/300\n",
            "3/3 - 0s - loss: 325.8755 - mae: 325.8755 - mse: 567956.1250 - val_loss: 633.8220 - val_mae: 633.8220 - val_mse: 1079956.6250 - lr: 1.0000e-05 - 195ms/epoch - 65ms/step\n",
            "Epoch 293/300\n",
            "3/3 - 0s - loss: 325.8042 - mae: 325.8042 - mse: 567721.0000 - val_loss: 634.4071 - val_mae: 634.4071 - val_mse: 1081294.1250 - lr: 1.0000e-05 - 199ms/epoch - 66ms/step\n",
            "Epoch 294/300\n",
            "3/3 - 0s - loss: 325.5404 - mae: 325.5404 - mse: 567212.0625 - val_loss: 634.6711 - val_mae: 634.6711 - val_mse: 1084079.7500 - lr: 1.0000e-05 - 193ms/epoch - 64ms/step\n",
            "Epoch 295/300\n",
            "3/3 - 0s - loss: 325.3680 - mae: 325.3680 - mse: 567130.5000 - val_loss: 634.3104 - val_mae: 634.3104 - val_mse: 1086348.5000 - lr: 1.0000e-05 - 193ms/epoch - 64ms/step\n",
            "Epoch 296/300\n",
            "3/3 - 0s - loss: 325.2161 - mae: 325.2161 - mse: 567424.0625 - val_loss: 633.4761 - val_mae: 633.4761 - val_mse: 1086980.5000 - lr: 1.0000e-05 - 203ms/epoch - 68ms/step\n",
            "Epoch 297/300\n",
            "3/3 - 0s - loss: 325.0891 - mae: 325.0891 - mse: 567621.0000 - val_loss: 632.4338 - val_mae: 632.4338 - val_mse: 1085767.0000 - lr: 1.0000e-05 - 207ms/epoch - 69ms/step\n",
            "Epoch 298/300\n",
            "3/3 - 0s - loss: 325.0333 - mae: 325.0333 - mse: 567687.1875 - val_loss: 631.7915 - val_mae: 631.7915 - val_mse: 1084310.1250 - lr: 1.0000e-05 - 200ms/epoch - 67ms/step\n",
            "Epoch 299/300\n",
            "3/3 - 0s - loss: 324.9766 - mae: 324.9766 - mse: 567604.4375 - val_loss: 631.6854 - val_mae: 631.6854 - val_mse: 1083601.7500 - lr: 1.0000e-05 - 202ms/epoch - 67ms/step\n",
            "Epoch 300/300\n",
            "3/3 - 0s - loss: 324.8742 - mae: 324.8742 - mse: 567381.8750 - val_loss: 631.9611 - val_mae: 631.9611 - val_mse: 1083581.8750 - lr: 1.0000e-05 - 205ms/epoch - 68ms/step\n",
            "Optimizing model by reducing: mse for epochs: 300, num_iter: 2, model: NBEATS_model\n",
            "Epoch 1/300\n",
            "3/3 - 44s - loss: 16996566.0000 - mae: 2513.5452 - mse: 16996566.0000 - val_loss: 23174004.0000 - val_mae: 3796.2097 - val_mse: 23174004.0000 - lr: 0.0010 - 44s/epoch - 15s/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 6319424.5000 - mae: 1359.3207 - mse: 6319424.5000 - val_loss: 6390991.5000 - val_mae: 1662.5918 - val_mse: 6390991.5000 - lr: 0.0010 - 457ms/epoch - 152ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 4386374.5000 - mae: 992.9603 - mse: 4386374.5000 - val_loss: 5977380.0000 - val_mae: 1685.4052 - val_mse: 5977380.0000 - lr: 0.0010 - 367ms/epoch - 122ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 5683689.0000 - mae: 1129.9578 - mse: 5683689.0000 - val_loss: 5960737.0000 - val_mae: 1630.6541 - val_mse: 5960737.0000 - lr: 0.0010 - 342ms/epoch - 114ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 4500583.5000 - mae: 1019.5540 - mse: 4500583.5000 - val_loss: 7046996.0000 - val_mae: 1774.6218 - val_mse: 7046996.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 4311915.0000 - mae: 977.8450 - mse: 4311915.0000 - val_loss: 6281073.5000 - val_mae: 1642.4148 - val_mse: 6281073.5000 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 3617942.7500 - mae: 910.6004 - mse: 3617942.7500 - val_loss: 4197515.5000 - val_mae: 1326.3030 - val_mse: 4197515.5000 - lr: 0.0010 - 381ms/epoch - 127ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 2166428.7500 - mae: 698.7510 - mse: 2166428.7500 - val_loss: 13093175.0000 - val_mae: 3114.4509 - val_mse: 13093175.0000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 13447074.0000 - mae: 2176.0325 - mse: 13447074.0000 - val_loss: 2803815.0000 - val_mae: 1148.4468 - val_mse: 2803815.0000 - lr: 0.0010 - 334ms/epoch - 111ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 3367605.0000 - mae: 833.2230 - mse: 3367605.0000 - val_loss: 3518125.0000 - val_mae: 1354.6049 - val_mse: 3518125.0000 - lr: 0.0010 - 234ms/epoch - 78ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 7611624.5000 - mae: 1415.5022 - mse: 7611624.5000 - val_loss: 5503828.0000 - val_mae: 1571.0493 - val_mse: 5503828.0000 - lr: 0.0010 - 259ms/epoch - 86ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 4584900.0000 - mae: 1006.1518 - mse: 4584900.0000 - val_loss: 5883115.5000 - val_mae: 1601.0068 - val_mse: 5883115.5000 - lr: 0.0010 - 260ms/epoch - 87ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 4260559.0000 - mae: 964.9378 - mse: 4260559.0000 - val_loss: 5254901.5000 - val_mae: 1513.9795 - val_mse: 5254901.5000 - lr: 0.0010 - 252ms/epoch - 84ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 4339819.5000 - mae: 983.7973 - mse: 4339819.5000 - val_loss: 5544445.0000 - val_mae: 1772.7920 - val_mse: 5544445.0000 - lr: 0.0010 - 273ms/epoch - 91ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 6709178.0000 - mae: 1250.4718 - mse: 6709178.0000 - val_loss: 6054127.5000 - val_mae: 1659.0529 - val_mse: 6054127.5000 - lr: 0.0010 - 279ms/epoch - 93ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 4644640.5000 - mae: 1015.9608 - mse: 4644640.5000 - val_loss: 6411402.5000 - val_mae: 1662.6361 - val_mse: 6411402.5000 - lr: 0.0010 - 292ms/epoch - 97ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 4393896.0000 - mae: 984.7141 - mse: 4393896.0000 - val_loss: 6148564.0000 - val_mae: 1615.5090 - val_mse: 6148564.0000 - lr: 0.0010 - 294ms/epoch - 98ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 4167830.2500 - mae: 966.1253 - mse: 4167830.2500 - val_loss: 6412094.5000 - val_mae: 1707.9719 - val_mse: 6412094.5000 - lr: 0.0010 - 220ms/epoch - 73ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 3502371.2500 - mae: 911.2928 - mse: 3502371.2500 - val_loss: 5774924.0000 - val_mae: 1584.5848 - val_mse: 5774924.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 4688639.0000 - mae: 1015.3044 - mse: 4688639.0000 - val_loss: 6038622.5000 - val_mae: 1638.7777 - val_mse: 6038622.5000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 4435932.0000 - mae: 987.6764 - mse: 4435932.0000 - val_loss: 5393635.0000 - val_mae: 1564.5135 - val_mse: 5393635.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 3897186.5000 - mae: 921.5912 - mse: 3897186.5000 - val_loss: 4726534.0000 - val_mae: 1459.0675 - val_mse: 4726534.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 3255588.5000 - mae: 862.4818 - mse: 3255588.5000 - val_loss: 2210163.5000 - val_mae: 1010.5237 - val_mse: 2210163.5000 - lr: 0.0010 - 362ms/epoch - 121ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 4398990.5000 - mae: 1084.4180 - mse: 4398990.5000 - val_loss: 292574432.0000 - val_mae: 15304.2314 - val_mse: 292574432.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 26124690.0000 - mae: 3456.1401 - mse: 26124690.0000 - val_loss: 5964170.5000 - val_mae: 1617.6134 - val_mse: 5964170.5000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 4546293.0000 - mae: 997.2114 - mse: 4546293.0000 - val_loss: 6023837.5000 - val_mae: 1634.3394 - val_mse: 6023837.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 4461455.0000 - mae: 990.8598 - mse: 4461455.0000 - val_loss: 5754765.0000 - val_mae: 1618.4620 - val_mse: 5754765.0000 - lr: 0.0010 - 189ms/epoch - 63ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 4535112.0000 - mae: 999.0525 - mse: 4535112.0000 - val_loss: 5713869.5000 - val_mae: 1633.6210 - val_mse: 5713869.5000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 4289597.0000 - mae: 972.2704 - mse: 4289597.0000 - val_loss: 5537193.5000 - val_mae: 1578.8429 - val_mse: 5537193.5000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 4303902.0000 - mae: 971.8820 - mse: 4303902.0000 - val_loss: 4978195.5000 - val_mae: 1507.0176 - val_mse: 4978195.5000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 3702265.0000 - mae: 895.2094 - mse: 3702265.0000 - val_loss: 3475658.2500 - val_mae: 1310.8547 - val_mse: 3475658.2500 - lr: 0.0010 - 192ms/epoch - 64ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 2794284.2500 - mae: 722.7076 - mse: 2794284.2500 - val_loss: 2513625.7500 - val_mae: 1044.9344 - val_mse: 2513625.7500 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 2914702.7500 - mae: 766.7653 - mse: 2914702.5000 - val_loss: 2788848.7500 - val_mae: 1135.9630 - val_mse: 2788848.7500 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 2265117.7500 - mae: 657.8246 - mse: 2265117.7500 - val_loss: 6098341.5000 - val_mae: 1777.8431 - val_mse: 6098341.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 6137909.0000 - mae: 1225.4370 - mse: 6137909.0000 - val_loss: 8048352.5000 - val_mae: 1929.2485 - val_mse: 8048352.5000 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 4341075.0000 - mae: 985.0712 - mse: 4341075.0000 - val_loss: 5704837.0000 - val_mae: 1589.4589 - val_mse: 5704837.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 4503159.0000 - mae: 994.7510 - mse: 4503159.0000 - val_loss: 5793164.5000 - val_mae: 1623.4771 - val_mse: 5793164.5000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 4417077.5000 - mae: 985.6924 - mse: 4417077.5000 - val_loss: 5528079.5000 - val_mae: 1593.2170 - val_mse: 5528079.5000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 4246776.5000 - mae: 961.8964 - mse: 4246776.5000 - val_loss: 5019333.0000 - val_mae: 1509.8953 - val_mse: 5019333.0000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 3865325.0000 - mae: 907.1863 - mse: 3865325.0000 - val_loss: 4033637.0000 - val_mae: 1357.4196 - val_mse: 4033637.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 3367064.5000 - mae: 833.2819 - mse: 3367064.5000 - val_loss: 3651405.0000 - val_mae: 1311.1167 - val_mse: 3651405.0000 - lr: 0.0010 - 246ms/epoch - 82ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 4400086.5000 - mae: 978.2100 - mse: 4400086.5000 - val_loss: 4230398.0000 - val_mae: 1416.6643 - val_mse: 4230398.0000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 3428419.5000 - mae: 829.0063 - mse: 3428419.5000 - val_loss: 4863535.5000 - val_mae: 1505.2516 - val_mse: 4863535.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 4652191.0000 - mae: 1008.5391 - mse: 4652191.0000 - val_loss: 6498262.0000 - val_mae: 1681.7098 - val_mse: 6498262.0000 - lr: 0.0010 - 192ms/epoch - 64ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 4705429.5000 - mae: 1020.0889 - mse: 4705429.5000 - val_loss: 6499457.5000 - val_mae: 1681.5531 - val_mse: 6499457.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 4694006.0000 - mae: 1018.8530 - mse: 4694006.0000 - val_loss: 6464624.0000 - val_mae: 1676.7708 - val_mse: 6464624.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 4652188.0000 - mae: 1014.6305 - mse: 4652188.0000 - val_loss: 6225259.0000 - val_mae: 1654.1820 - val_mse: 6225259.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 4391329.0000 - mae: 983.5964 - mse: 4391329.0000 - val_loss: 5533996.0000 - val_mae: 1564.9231 - val_mse: 5533996.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 4309003.5000 - mae: 968.2460 - mse: 4309003.5000 - val_loss: 6009928.0000 - val_mae: 1743.9009 - val_mse: 6009928.0000 - lr: 0.0010 - 191ms/epoch - 64ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 5743937.5000 - mae: 1144.9573 - mse: 5743937.5000 - val_loss: 5541526.0000 - val_mae: 1562.8672 - val_mse: 5541526.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 4685958.0000 - mae: 1016.6230 - mse: 4685958.0000 - val_loss: 6479443.0000 - val_mae: 1677.3481 - val_mse: 6479443.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 4631649.0000 - mae: 1012.4639 - mse: 4631649.0000 - val_loss: 6449157.0000 - val_mae: 1667.8575 - val_mse: 6449157.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 4508055.0000 - mae: 999.7126 - mse: 4508055.0000 - val_loss: 6238591.5000 - val_mae: 1630.4233 - val_mse: 6238591.5000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 4247874.5000 - mae: 968.8922 - mse: 4247874.5000 - val_loss: 7988207.5000 - val_mae: 1990.0259 - val_mse: 7988207.5000 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 8037755.0000 - mae: 1438.0511 - mse: 8037755.0000 - val_loss: 6358536.5000 - val_mae: 1674.6077 - val_mse: 6358536.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 4715757.0000 - mae: 1019.2480 - mse: 4715757.0000 - val_loss: 6402726.0000 - val_mae: 1671.6088 - val_mse: 6402726.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 4645754.5000 - mae: 1013.2974 - mse: 4645754.5000 - val_loss: 6142192.0000 - val_mae: 1652.2921 - val_mse: 6142192.0000 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 4589363.5000 - mae: 1005.4698 - mse: 4589363.5000 - val_loss: 5806679.5000 - val_mae: 1649.3862 - val_mse: 5806679.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 4454425.5000 - mae: 990.4461 - mse: 4454425.5000 - val_loss: 5411105.0000 - val_mae: 1589.8723 - val_mse: 5411105.0000 - lr: 0.0010 - 190ms/epoch - 63ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 4264136.0000 - mae: 966.3207 - mse: 4264136.0000 - val_loss: 4893061.0000 - val_mae: 1604.9929 - val_mse: 4893061.0000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 5672379.5000 - mae: 1171.9633 - mse: 5672379.5000 - val_loss: 5209986.5000 - val_mae: 1530.6091 - val_mse: 5209986.5000 - lr: 0.0010 - 215ms/epoch - 72ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 4702594.0000 - mae: 1017.0892 - mse: 4702594.0000 - val_loss: 6432680.5000 - val_mae: 1674.1823 - val_mse: 6432680.5000 - lr: 0.0010 - 225ms/epoch - 75ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 4690053.5000 - mae: 1017.1115 - mse: 4690053.5000 - val_loss: 6314858.5000 - val_mae: 1658.5238 - val_mse: 6314858.5000 - lr: 0.0010 - 267ms/epoch - 89ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 4593974.5000 - mae: 1006.7093 - mse: 4593974.5000 - val_loss: 6014106.0000 - val_mae: 1625.3663 - val_mse: 6014106.0000 - lr: 0.0010 - 267ms/epoch - 89ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 4336147.0000 - mae: 975.5773 - mse: 4336147.0000 - val_loss: 4990790.5000 - val_mae: 1493.5310 - val_mse: 4990790.5000 - lr: 0.0010 - 261ms/epoch - 87ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 3576476.2500 - mae: 870.9060 - mse: 3576476.2500 - val_loss: 3620531.7500 - val_mae: 1292.0515 - val_mse: 3620531.7500 - lr: 0.0010 - 277ms/epoch - 92ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 3692166.7500 - mae: 887.5842 - mse: 3692166.7500 - val_loss: 6897612.5000 - val_mae: 1658.1362 - val_mse: 6897612.5000 - lr: 0.0010 - 279ms/epoch - 93ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 5237018.5000 - mae: 1156.0044 - mse: 5237018.5000 - val_loss: 6219538.5000 - val_mae: 1660.4702 - val_mse: 6219538.5000 - lr: 0.0010 - 289ms/epoch - 96ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 4727399.0000 - mae: 1021.1120 - mse: 4727399.0000 - val_loss: 6505336.0000 - val_mae: 1683.6592 - val_mse: 6505336.0000 - lr: 0.0010 - 274ms/epoch - 91ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 4723188.0000 - mae: 1021.8660 - mse: 4723188.0000 - val_loss: 6512193.5000 - val_mae: 1681.9874 - val_mse: 6512193.5000 - lr: 0.0010 - 227ms/epoch - 76ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 4678549.5000 - mae: 1017.6455 - mse: 4678549.5000 - val_loss: 6569121.0000 - val_mae: 1683.5417 - val_mse: 6569121.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 4609708.5000 - mae: 1011.2695 - mse: 4609708.5000 - val_loss: 6773548.5000 - val_mae: 1705.3524 - val_mse: 6773548.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 4533652.5000 - mae: 1007.4005 - mse: 4533652.5000 - val_loss: 7108574.0000 - val_mae: 1763.3743 - val_mse: 7108574.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 4442493.0000 - mae: 1004.3165 - mse: 4442493.0000 - val_loss: 7251207.5000 - val_mae: 1800.6439 - val_mse: 7251207.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 4278996.5000 - mae: 985.3880 - mse: 4278996.5000 - val_loss: 7178105.0000 - val_mae: 1813.9177 - val_mse: 7178105.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 3925223.2500 - mae: 962.2055 - mse: 3925223.2500 - val_loss: 6139634.0000 - val_mae: 1606.4003 - val_mse: 6139634.0000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 4543423.0000 - mae: 999.4578 - mse: 4543423.0000 - val_loss: 6425150.0000 - val_mae: 1663.6527 - val_mse: 6425150.0000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 4461677.0000 - mae: 996.6538 - mse: 4461677.0000 - val_loss: 6136986.0000 - val_mae: 1609.3473 - val_mse: 6136986.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 3071624.5000 - mae: 822.6841 - mse: 3071624.5000 - val_loss: 11197651.0000 - val_mae: 2615.4102 - val_mse: 11197651.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 7153134.0000 - mae: 1597.1956 - mse: 7153134.0000 - val_loss: 6405415.5000 - val_mae: 1670.2491 - val_mse: 6405415.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 4693328.5000 - mae: 1018.4077 - mse: 4693328.5000 - val_loss: 6350383.0000 - val_mae: 1667.1333 - val_mse: 6350383.0000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 4666148.0000 - mae: 1015.0200 - mse: 4666148.0000 - val_loss: 6187024.5000 - val_mae: 1655.3770 - val_mse: 6187024.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 4577113.5000 - mae: 1004.5430 - mse: 4577113.5000 - val_loss: 5743534.0000 - val_mae: 1620.8401 - val_mse: 5743534.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 4373896.0000 - mae: 981.7380 - mse: 4373896.0000 - val_loss: 5201551.0000 - val_mae: 1556.6842 - val_mse: 5201551.0000 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 4365460.0000 - mae: 977.6493 - mse: 4365460.0000 - val_loss: 4449260.0000 - val_mae: 1522.1116 - val_mse: 4449260.0000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 6238251.5000 - mae: 1265.5081 - mse: 6238252.0000 - val_loss: 6473424.5000 - val_mae: 1679.2612 - val_mse: 6473424.5000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 4712854.5000 - mae: 1020.1018 - mse: 4712854.5000 - val_loss: 6299803.5000 - val_mae: 1667.6714 - val_mse: 6299803.5000 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 4683969.0000 - mae: 1016.2271 - mse: 4683969.0000 - val_loss: 6191853.0000 - val_mae: 1660.9314 - val_mse: 6191853.0000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 4627378.5000 - mae: 1010.5398 - mse: 4627378.5000 - val_loss: 6756246.0000 - val_mae: 1704.7610 - val_mse: 6756246.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 4815250.5000 - mae: 1033.9027 - mse: 4815250.5000 - val_loss: 6548551.0000 - val_mae: 1683.2394 - val_mse: 6548551.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 4705905.0000 - mae: 1021.6328 - mse: 4705905.0000 - val_loss: 6066312.0000 - val_mae: 1712.3907 - val_mse: 6066312.0000 - lr: 0.0010 - 192ms/epoch - 64ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 4641356.5000 - mae: 1012.6241 - mse: 4641356.5000 - val_loss: 6005503.0000 - val_mae: 1651.4960 - val_mse: 6005503.0000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 4613209.5000 - mae: 1007.1589 - mse: 4613209.5000 - val_loss: 6728200.0000 - val_mae: 1700.4381 - val_mse: 6728200.0000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 5137110.0000 - mae: 1079.3258 - mse: 5137110.5000 - val_loss: 8101111.5000 - val_mae: 2158.5256 - val_mse: 8101111.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 5607973.0000 - mae: 1138.0718 - mse: 5607973.0000 - val_loss: 5823404.0000 - val_mae: 1612.8167 - val_mse: 5823404.0000 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 4690884.0000 - mae: 1011.8848 - mse: 4690884.0000 - val_loss: 6201919.0000 - val_mae: 1660.4357 - val_mse: 6201919.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 4640724.5000 - mae: 1012.4716 - mse: 4640724.5000 - val_loss: 6290131.5000 - val_mae: 1652.7960 - val_mse: 6290131.5000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 4493988.5000 - mae: 997.0485 - mse: 4493988.5000 - val_loss: 5757677.0000 - val_mae: 1649.7721 - val_mse: 5757677.0000 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 5401692.5000 - mae: 1102.5792 - mse: 5401692.5000 - val_loss: 7138070.5000 - val_mae: 1761.9471 - val_mse: 7138070.5000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 4762186.5000 - mae: 1028.6868 - mse: 4762186.5000 - val_loss: 6686713.5000 - val_mae: 1697.7076 - val_mse: 6686713.5000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 4699363.5000 - mae: 1020.0032 - mse: 4699363.5000 - val_loss: 6554110.0000 - val_mae: 1683.6080 - val_mse: 6554110.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 4673996.0000 - mae: 1016.9958 - mse: 4673996.0000 - val_loss: 6549536.5000 - val_mae: 1681.2532 - val_mse: 6549536.5000 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 4612953.5000 - mae: 1011.2708 - mse: 4612953.5000 - val_loss: 6602394.5000 - val_mae: 1684.5023 - val_mse: 6602394.5000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 4427540.0000 - mae: 990.7239 - mse: 4427540.0000 - val_loss: 6301283.5000 - val_mae: 1641.8521 - val_mse: 6301283.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 3646933.5000 - mae: 912.9570 - mse: 3646933.5000 - val_loss: 6729094.5000 - val_mae: 1705.5488 - val_mse: 6729094.5000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 4827290.5000 - mae: 1039.5994 - mse: 4827290.5000 - val_loss: 6618374.0000 - val_mae: 1684.1897 - val_mse: 6618374.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 4541748.0000 - mae: 1003.5687 - mse: 4541748.0000 - val_loss: 36757776.0000 - val_mae: 5199.2847 - val_mse: 36757776.0000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 6081683.0000 - mae: 1544.8998 - mse: 6081683.0000 - val_loss: 6608034.5000 - val_mae: 1691.4139 - val_mse: 6608034.5000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 4702061.0000 - mae: 1019.6940 - mse: 4702061.0000 - val_loss: 6509043.0000 - val_mae: 1679.2135 - val_mse: 6509043.0000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 4702692.5000 - mae: 1018.5378 - mse: 4702692.5000 - val_loss: 6371073.0000 - val_mae: 1657.1504 - val_mse: 6371073.0000 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 4648058.5000 - mae: 1014.4020 - mse: 4648058.5000 - val_loss: 6176566.0000 - val_mae: 1629.3226 - val_mse: 6176566.0000 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 4436495.0000 - mae: 982.5988 - mse: 4436495.0000 - val_loss: 5227450.5000 - val_mae: 1486.4750 - val_mse: 5227450.5000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 3765650.7500 - mae: 920.8713 - mse: 3765650.7500 - val_loss: 15265353.0000 - val_mae: 3290.7683 - val_mse: 15265353.0000 - lr: 0.0010 - 260ms/epoch - 87ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 7175397.0000 - mae: 1705.7748 - mse: 7175397.0000 - val_loss: 5753270.5000 - val_mae: 1511.6167 - val_mse: 5753270.5000 - lr: 0.0010 - 245ms/epoch - 82ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 4417013.5000 - mae: 992.9407 - mse: 4417013.5000 - val_loss: 6952194.5000 - val_mae: 1747.7708 - val_mse: 6952194.5000 - lr: 0.0010 - 249ms/epoch - 83ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 4005619.0000 - mae: 955.2031 - mse: 4005619.0000 - val_loss: 5868269.0000 - val_mae: 1624.9587 - val_mse: 5868269.0000 - lr: 0.0010 - 266ms/epoch - 89ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 3043532.5000 - mae: 808.5359 - mse: 3043532.2500 - val_loss: 6380544.5000 - val_mae: 1654.8114 - val_mse: 6380544.5000 - lr: 0.0010 - 283ms/epoch - 94ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 4647312.5000 - mae: 1012.7926 - mse: 4647312.5000 - val_loss: 6444202.5000 - val_mae: 1670.2797 - val_mse: 6444202.5000 - lr: 0.0010 - 289ms/epoch - 96ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 4598470.5000 - mae: 1009.3469 - mse: 4598470.5000 - val_loss: 6333997.5000 - val_mae: 1655.6798 - val_mse: 6333997.5000 - lr: 0.0010 - 282ms/epoch - 94ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 4340687.0000 - mae: 981.9931 - mse: 4340687.0000 - val_loss: 6061245.0000 - val_mae: 1604.0420 - val_mse: 6061245.0000 - lr: 0.0010 - 285ms/epoch - 95ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 4022156.2500 - mae: 976.3048 - mse: 4022156.2500 - val_loss: 5433808.5000 - val_mae: 1586.5471 - val_mse: 5433808.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 4977444.5000 - mae: 1049.7598 - mse: 4977444.5000 - val_loss: 5852194.5000 - val_mae: 1671.5392 - val_mse: 5852194.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 123: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 4625575.5000 - mae: 1008.9824 - mse: 4625575.5000 - val_loss: 6219633.5000 - val_mae: 1655.3761 - val_mse: 6219633.5000 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 4613602.5000 - mae: 1009.0362 - mse: 4613602.5000 - val_loss: 6185831.0000 - val_mae: 1651.7555 - val_mse: 6185831.0000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 4585509.5000 - mae: 1005.7554 - mse: 4585509.5000 - val_loss: 6088439.0000 - val_mae: 1642.6241 - val_mse: 6088439.0000 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 4542629.5000 - mae: 1000.4624 - mse: 4542629.5000 - val_loss: 5965300.0000 - val_mae: 1631.7739 - val_mse: 5965300.0000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 4495694.0000 - mae: 994.6588 - mse: 4495694.0000 - val_loss: 5826832.5000 - val_mae: 1620.0374 - val_mse: 5826832.5000 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 4440573.5000 - mae: 987.6918 - mse: 4440573.5000 - val_loss: 5673883.0000 - val_mae: 1607.8311 - val_mse: 5673883.0000 - lr: 1.0000e-04 - 213ms/epoch - 71ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 4381086.0000 - mae: 980.0339 - mse: 4381086.0000 - val_loss: 5525624.5000 - val_mae: 1599.8088 - val_mse: 5525624.5000 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 4320719.5000 - mae: 972.2112 - mse: 4320719.5000 - val_loss: 5389376.0000 - val_mae: 1594.8257 - val_mse: 5389376.0000 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 4259750.5000 - mae: 964.6166 - mse: 4259750.5000 - val_loss: 5271029.0000 - val_mae: 1597.1147 - val_mse: 5271029.0000 - lr: 1.0000e-04 - 210ms/epoch - 70ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 4200416.5000 - mae: 957.7312 - mse: 4200416.5000 - val_loss: 5174861.5000 - val_mae: 1600.6715 - val_mse: 5174861.5000 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 4139905.7500 - mae: 951.1221 - mse: 4139905.7500 - val_loss: 5083013.0000 - val_mae: 1601.1116 - val_mse: 5083013.0000 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 4070950.5000 - mae: 942.2958 - mse: 4070950.5000 - val_loss: 4963625.5000 - val_mae: 1590.4069 - val_mse: 4963625.5000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 3989077.0000 - mae: 930.4858 - mse: 3989077.0000 - val_loss: 4796941.5000 - val_mae: 1563.5208 - val_mse: 4796941.5000 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 3894019.2500 - mae: 915.6112 - mse: 3894019.2500 - val_loss: 4586784.5000 - val_mae: 1523.3231 - val_mse: 4586784.5000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 3787610.0000 - mae: 897.7662 - mse: 3787610.0000 - val_loss: 4356191.0000 - val_mae: 1478.2762 - val_mse: 4356191.0000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 3670603.2500 - mae: 877.5307 - mse: 3670603.2500 - val_loss: 4111316.7500 - val_mae: 1433.1791 - val_mse: 4111316.7500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 3540961.2500 - mae: 854.1160 - mse: 3540961.2500 - val_loss: 3854138.7500 - val_mae: 1382.0977 - val_mse: 3854138.7500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 3405779.5000 - mae: 827.7462 - mse: 3405779.5000 - val_loss: 3586013.2500 - val_mae: 1317.5536 - val_mse: 3586013.2500 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 3272043.5000 - mae: 799.6292 - mse: 3272044.0000 - val_loss: 3328803.0000 - val_mae: 1246.9893 - val_mse: 3328803.0000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 3150577.0000 - mae: 773.0699 - mse: 3150577.0000 - val_loss: 3111887.0000 - val_mae: 1185.8073 - val_mse: 3111887.0000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 3049572.5000 - mae: 750.0214 - mse: 3049572.5000 - val_loss: 2949873.7500 - val_mae: 1135.8466 - val_mse: 2949873.7500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 2968928.7500 - mae: 732.9577 - mse: 2968928.5000 - val_loss: 2844169.5000 - val_mae: 1104.0332 - val_mse: 2844169.5000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 2905805.7500 - mae: 721.7229 - mse: 2905805.7500 - val_loss: 2777125.2500 - val_mae: 1087.4871 - val_mse: 2777125.2500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 2853889.5000 - mae: 714.7756 - mse: 2853889.2500 - val_loss: 2732775.7500 - val_mae: 1080.2001 - val_mse: 2732775.7500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 2804157.2500 - mae: 709.4824 - mse: 2804157.2500 - val_loss: 2697325.5000 - val_mae: 1075.2327 - val_mse: 2697325.5000 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 2745553.7500 - mae: 703.5625 - mse: 2745553.7500 - val_loss: 2659741.5000 - val_mae: 1068.3013 - val_mse: 2659741.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 2668966.2500 - mae: 696.0020 - mse: 2668966.0000 - val_loss: 2617708.5000 - val_mae: 1059.0609 - val_mse: 2617708.5000 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 2568354.5000 - mae: 686.1578 - mse: 2568354.5000 - val_loss: 2573658.0000 - val_mae: 1049.1102 - val_mse: 2573658.0000 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 2427198.5000 - mae: 672.8822 - mse: 2427198.5000 - val_loss: 2528128.7500 - val_mae: 1040.7236 - val_mse: 2528128.7500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 2207862.7500 - mae: 651.9156 - mse: 2207862.7500 - val_loss: 2452696.5000 - val_mae: 1030.8582 - val_mse: 2452696.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 1860099.7500 - mae: 614.9278 - mse: 1860099.7500 - val_loss: 2280640.0000 - val_mae: 1008.2018 - val_mse: 2280640.0000 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 1374111.6250 - mae: 546.9799 - mse: 1374111.6250 - val_loss: 1920957.0000 - val_mae: 946.4420 - val_mse: 1920957.0000 - lr: 1.0000e-04 - 332ms/epoch - 111ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 1003944.0625 - mae: 477.0640 - mse: 1003944.0625 - val_loss: 1640733.1250 - val_mae: 867.3052 - val_mse: 1640733.1250 - lr: 1.0000e-04 - 327ms/epoch - 109ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 1262225.2500 - mae: 515.1176 - mse: 1262225.2500 - val_loss: 1625601.6250 - val_mae: 855.1923 - val_mse: 1625601.6250 - lr: 1.0000e-04 - 326ms/epoch - 109ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 1018682.7500 - mae: 474.5250 - mse: 1018682.7500 - val_loss: 1410823.2500 - val_mae: 803.8604 - val_mse: 1410823.2500 - lr: 1.0000e-04 - 325ms/epoch - 108ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 877461.0000 - mae: 434.3278 - mse: 877461.0000 - val_loss: 1273397.5000 - val_mae: 745.1984 - val_mse: 1273397.5000 - lr: 1.0000e-04 - 337ms/epoch - 112ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 934112.8125 - mae: 449.3627 - mse: 934112.8125 - val_loss: 1330373.5000 - val_mae: 752.9422 - val_mse: 1330373.5000 - lr: 1.0000e-04 - 264ms/epoch - 88ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 909834.9375 - mae: 444.3864 - mse: 909834.9375 - val_loss: 1230571.7500 - val_mae: 715.9581 - val_mse: 1230571.7500 - lr: 1.0000e-04 - 423ms/epoch - 141ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 826086.1250 - mae: 419.3976 - mse: 826086.1250 - val_loss: 1167368.8750 - val_mae: 704.3931 - val_mse: 1167368.8750 - lr: 1.0000e-04 - 396ms/epoch - 132ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 810931.5625 - mae: 417.2885 - mse: 810931.5625 - val_loss: 1227798.2500 - val_mae: 729.4901 - val_mse: 1227798.2500 - lr: 1.0000e-04 - 287ms/epoch - 96ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 811131.1875 - mae: 418.4394 - mse: 811131.1875 - val_loss: 1261394.6250 - val_mae: 740.9290 - val_mse: 1261394.6250 - lr: 1.0000e-04 - 287ms/epoch - 96ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 787672.2500 - mae: 411.0732 - mse: 787672.2500 - val_loss: 1179513.3750 - val_mae: 708.3832 - val_mse: 1179513.3750 - lr: 1.0000e-04 - 289ms/epoch - 96ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 764253.8125 - mae: 402.2546 - mse: 764253.8125 - val_loss: 1111215.7500 - val_mae: 681.2039 - val_mse: 1111215.7500 - lr: 1.0000e-04 - 322ms/epoch - 107ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 764688.8125 - mae: 399.4126 - mse: 764688.8125 - val_loss: 1095957.2500 - val_mae: 673.1570 - val_mse: 1095957.2500 - lr: 1.0000e-04 - 326ms/epoch - 109ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 755937.2500 - mae: 397.0150 - mse: 755937.2500 - val_loss: 1097041.7500 - val_mae: 670.1427 - val_mse: 1097041.7500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 742267.0625 - mae: 394.4951 - mse: 742267.0000 - val_loss: 1102824.8750 - val_mae: 675.7307 - val_mse: 1102824.8750 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 733102.2500 - mae: 392.4630 - mse: 733102.1250 - val_loss: 1104424.2500 - val_mae: 679.8796 - val_mse: 1104424.2500 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 729076.9375 - mae: 391.5542 - mse: 729076.9375 - val_loss: 1100118.6250 - val_mae: 678.5185 - val_mse: 1100118.6250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 720300.2500 - mae: 388.5472 - mse: 720300.2500 - val_loss: 1088920.3750 - val_mae: 672.3109 - val_mse: 1088920.3750 - lr: 1.0000e-04 - 323ms/epoch - 108ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 713648.4375 - mae: 385.7045 - mse: 713648.4375 - val_loss: 1073482.0000 - val_mae: 663.4613 - val_mse: 1073482.0000 - lr: 1.0000e-04 - 332ms/epoch - 111ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 709064.6250 - mae: 383.3427 - mse: 709064.6250 - val_loss: 1058648.3750 - val_mae: 656.1736 - val_mse: 1058648.3750 - lr: 1.0000e-04 - 327ms/epoch - 109ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 704137.3125 - mae: 381.5594 - mse: 704137.3125 - val_loss: 1055576.8750 - val_mae: 654.8808 - val_mse: 1055576.8750 - lr: 1.0000e-04 - 328ms/epoch - 109ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 698566.8125 - mae: 380.2901 - mse: 698566.8125 - val_loss: 1058636.7500 - val_mae: 656.9014 - val_mse: 1058636.7500 - lr: 1.0000e-04 - 212ms/epoch - 71ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 694279.1875 - mae: 379.4345 - mse: 694279.1875 - val_loss: 1056982.5000 - val_mae: 656.4639 - val_mse: 1056982.5000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 690002.6250 - mae: 378.0566 - mse: 690002.6250 - val_loss: 1050807.3750 - val_mae: 653.1271 - val_mse: 1050807.3750 - lr: 1.0000e-04 - 321ms/epoch - 107ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 685472.3750 - mae: 376.2118 - mse: 685472.3750 - val_loss: 1045339.8750 - val_mae: 649.4459 - val_mse: 1045339.8750 - lr: 1.0000e-04 - 330ms/epoch - 110ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 681597.5000 - mae: 374.6502 - mse: 681597.5000 - val_loss: 1040646.5000 - val_mae: 646.1945 - val_mse: 1040646.5000 - lr: 1.0000e-04 - 334ms/epoch - 111ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 678001.8750 - mae: 373.2684 - mse: 678001.8750 - val_loss: 1037373.8125 - val_mae: 644.2569 - val_mse: 1037373.8125 - lr: 1.0000e-04 - 330ms/epoch - 110ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 674371.8125 - mae: 372.1656 - mse: 674371.8125 - val_loss: 1038229.9375 - val_mae: 644.6902 - val_mse: 1038229.9375 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 671173.7500 - mae: 371.3550 - mse: 671173.7500 - val_loss: 1038239.2500 - val_mae: 644.6798 - val_mse: 1038239.2500 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 668217.6250 - mae: 370.4738 - mse: 668217.6250 - val_loss: 1036897.5625 - val_mae: 643.6577 - val_mse: 1036897.5625 - lr: 1.0000e-04 - 365ms/epoch - 122ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 665332.3750 - mae: 369.3905 - mse: 665332.3750 - val_loss: 1036258.0000 - val_mae: 642.8535 - val_mse: 1036258.0000 - lr: 1.0000e-04 - 337ms/epoch - 112ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 662728.6875 - mae: 368.3554 - mse: 662728.6875 - val_loss: 1034749.3125 - val_mae: 641.5391 - val_mse: 1034749.3125 - lr: 1.0000e-04 - 393ms/epoch - 131ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 660284.2500 - mae: 367.4880 - mse: 660284.2500 - val_loss: 1035220.1250 - val_mae: 641.4733 - val_mse: 1035220.1250 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 657951.6875 - mae: 366.8370 - mse: 657951.6875 - val_loss: 1035203.8125 - val_mae: 641.1409 - val_mse: 1035203.8125 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 655751.8750 - mae: 366.1690 - mse: 655751.8750 - val_loss: 1035823.9375 - val_mae: 641.0240 - val_mse: 1035823.9375 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 653685.1875 - mae: 365.5361 - mse: 653685.1875 - val_loss: 1036776.2500 - val_mae: 640.8524 - val_mse: 1036776.2500 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 651733.6250 - mae: 364.7596 - mse: 651733.6250 - val_loss: 1035822.3750 - val_mae: 639.4702 - val_mse: 1035822.3750 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 649917.0625 - mae: 364.2632 - mse: 649917.0625 - val_loss: 1037159.2500 - val_mae: 639.6172 - val_mse: 1037159.2500 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 648522.6250 - mae: 363.5793 - mse: 648522.6250 - val_loss: 1036526.1250 - val_mae: 638.7466 - val_mse: 1036526.1250 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 647321.8750 - mae: 363.5819 - mse: 647321.8125 - val_loss: 1038925.4375 - val_mae: 639.8340 - val_mse: 1038925.4375 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 647042.5000 - mae: 363.0316 - mse: 647042.5000 - val_loss: 1036173.8125 - val_mae: 638.0137 - val_mse: 1036173.8125 - lr: 1.0000e-04 - 211ms/epoch - 70ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 647261.5000 - mae: 363.7051 - mse: 647261.5000 - val_loss: 1042121.5625 - val_mae: 640.8240 - val_mse: 1042121.5625 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 647445.6875 - mae: 362.9576 - mse: 647445.6875 - val_loss: 1033280.7500 - val_mae: 635.7654 - val_mse: 1033280.7500 - lr: 1.0000e-04 - 330ms/epoch - 110ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 646662.0625 - mae: 363.3941 - mse: 646662.0625 - val_loss: 1045620.2500 - val_mae: 641.8148 - val_mse: 1045620.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 644578.1875 - mae: 362.0514 - mse: 644578.1875 - val_loss: 1031987.1875 - val_mae: 634.5500 - val_mse: 1031987.1875 - lr: 1.0000e-04 - 386ms/epoch - 129ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 641056.6875 - mae: 361.5290 - mse: 641056.6875 - val_loss: 1046679.0000 - val_mae: 642.0350 - val_mse: 1046679.0000 - lr: 1.0000e-04 - 248ms/epoch - 83ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 637726.8750 - mae: 360.3160 - mse: 637726.8750 - val_loss: 1034295.0625 - val_mae: 635.4218 - val_mse: 1034295.0625 - lr: 1.0000e-04 - 272ms/epoch - 91ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 635220.1875 - mae: 359.5296 - mse: 635220.1875 - val_loss: 1042418.6250 - val_mae: 639.4897 - val_mse: 1042418.6250 - lr: 1.0000e-04 - 257ms/epoch - 86ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 633762.1875 - mae: 359.2046 - mse: 633762.1875 - val_loss: 1038008.7500 - val_mae: 636.9062 - val_mse: 1038008.7500 - lr: 1.0000e-04 - 264ms/epoch - 88ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 632497.2500 - mae: 358.7686 - mse: 632497.2500 - val_loss: 1040534.3125 - val_mae: 638.0470 - val_mse: 1040534.3125 - lr: 1.0000e-04 - 294ms/epoch - 98ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 631389.0000 - mae: 358.5428 - mse: 631389.0000 - val_loss: 1039546.6875 - val_mae: 637.2487 - val_mse: 1039546.6875 - lr: 1.0000e-04 - 328ms/epoch - 109ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 630465.0000 - mae: 358.0876 - mse: 630465.0000 - val_loss: 1040069.3125 - val_mae: 637.1761 - val_mse: 1040069.3125 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 629650.0000 - mae: 358.1199 - mse: 629650.0000 - val_loss: 1039913.3125 - val_mae: 636.7571 - val_mse: 1039913.3125 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 629739.1875 - mae: 357.7172 - mse: 629739.1875 - val_loss: 1039272.2500 - val_mae: 636.0328 - val_mse: 1039272.2500 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 630233.1250 - mae: 358.6550 - mse: 630233.1250 - val_loss: 1041463.4375 - val_mae: 636.7700 - val_mse: 1041463.4375 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 633273.9375 - mae: 358.4086 - mse: 633273.9375 - val_loss: 1036496.0000 - val_mae: 633.6581 - val_mse: 1036496.0000 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 636169.1875 - mae: 360.8240 - mse: 636169.1875 - val_loss: 1046078.3750 - val_mae: 638.0380 - val_mse: 1046078.3750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 640456.8125 - mae: 360.3792 - mse: 640456.8125 - val_loss: 1034454.0000 - val_mae: 631.8564 - val_mse: 1034454.0000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 646405.5000 - mae: 364.8359 - mse: 646405.5000 - val_loss: 1058235.7500 - val_mae: 643.7094 - val_mse: 1058235.7500 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 641066.2500 - mae: 360.6196 - mse: 641066.2500 - val_loss: 1032134.1250 - val_mae: 630.8380 - val_mse: 1032134.1250 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 634370.4375 - mae: 360.0402 - mse: 634370.4375 - val_loss: 1058711.2500 - val_mae: 643.9243 - val_mse: 1058711.2500 - lr: 1.0000e-04 - 212ms/epoch - 71ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 625926.2500 - mae: 356.5281 - mse: 625926.2500 - val_loss: 1034327.5000 - val_mae: 632.0250 - val_mse: 1034327.5000 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 620631.9375 - mae: 355.0937 - mse: 620631.9375 - val_loss: 1050414.0000 - val_mae: 639.8788 - val_mse: 1050414.0000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 621213.4375 - mae: 356.2807 - mse: 621213.4375 - val_loss: 1043520.6250 - val_mae: 636.5881 - val_mse: 1043520.6250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 622850.5000 - mae: 355.5742 - mse: 622850.5000 - val_loss: 1038750.1250 - val_mae: 633.6756 - val_mse: 1038750.1250 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 621079.4375 - mae: 355.7906 - mse: 621079.4375 - val_loss: 1049285.0000 - val_mae: 638.0673 - val_mse: 1049285.0000 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 619150.4375 - mae: 354.5164 - mse: 619150.4375 - val_loss: 1038325.6875 - val_mae: 632.4244 - val_mse: 1038325.6875 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 616793.9375 - mae: 354.1562 - mse: 616793.9375 - val_loss: 1049256.1250 - val_mae: 637.7391 - val_mse: 1049256.1250 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 615756.3750 - mae: 354.2901 - mse: 615756.3750 - val_loss: 1045040.7500 - val_mae: 635.8856 - val_mse: 1045040.7500 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 615258.5625 - mae: 353.8292 - mse: 615258.5625 - val_loss: 1045442.6875 - val_mae: 635.9128 - val_mse: 1045442.6875 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 224/300\n",
            "3/3 - 0s - loss: 614614.3125 - mae: 354.0949 - mse: 614614.3125 - val_loss: 1047011.9375 - val_mae: 636.1300 - val_mse: 1047011.9375 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 614174.8125 - mae: 353.3964 - mse: 614174.8125 - val_loss: 1043426.1250 - val_mae: 634.0480 - val_mse: 1043426.1250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 613637.8125 - mae: 353.8742 - mse: 613637.8125 - val_loss: 1046778.2500 - val_mae: 635.5401 - val_mse: 1046778.2500 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 613887.8125 - mae: 353.2828 - mse: 613887.8125 - val_loss: 1043281.8125 - val_mae: 633.7326 - val_mse: 1043281.8125 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 228/300\n",
            "3/3 - 0s - loss: 613788.2500 - mae: 354.2917 - mse: 613788.2500 - val_loss: 1047379.8750 - val_mae: 635.7338 - val_mse: 1047379.8750 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 229/300\n",
            "3/3 - 0s - loss: 616377.5000 - mae: 353.7707 - mse: 616377.5000 - val_loss: 1043930.4375 - val_mae: 633.8018 - val_mse: 1043930.4375 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 230/300\n",
            "3/3 - 0s - loss: 621575.8750 - mae: 357.4935 - mse: 621575.8750 - val_loss: 1050259.3750 - val_mae: 636.9127 - val_mse: 1050259.3750 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 231/300\n",
            "3/3 - 0s - loss: 632377.0000 - mae: 357.9419 - mse: 632377.0000 - val_loss: 1044866.0625 - val_mae: 634.3921 - val_mse: 1044866.0625 - lr: 1.0000e-04 - 214ms/epoch - 71ms/step\n",
            "Epoch 232/300\n",
            "3/3 - 0s - loss: 660807.6875 - mae: 372.8784 - mse: 660807.6875 - val_loss: 1080700.6250 - val_mae: 652.8200 - val_mse: 1080700.6250 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 233/300\n",
            "3/3 - 0s - loss: 654614.1875 - mae: 364.3337 - mse: 654614.1875 - val_loss: 1035171.3750 - val_mae: 630.4311 - val_mse: 1035171.3750 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 234/300\n",
            "3/3 - 0s - loss: 651261.4375 - mae: 368.2137 - mse: 651261.4375 - val_loss: 1106772.0000 - val_mae: 665.8184 - val_mse: 1106772.0000 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 235/300\n",
            "3/3 - 0s - loss: 616939.1875 - mae: 353.9324 - mse: 616939.1875 - val_loss: 1036226.0000 - val_mae: 631.4399 - val_mse: 1036226.0000 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 236/300\n",
            "3/3 - 0s - loss: 608003.6875 - mae: 350.9890 - mse: 608003.6875 - val_loss: 1054635.8750 - val_mae: 640.2761 - val_mse: 1054635.8750 - lr: 1.0000e-04 - 211ms/epoch - 70ms/step\n",
            "Epoch 237/300\n",
            "3/3 - 0s - loss: 617438.9375 - mae: 357.1389 - mse: 617438.9375 - val_loss: 1054098.2500 - val_mae: 639.6389 - val_mse: 1054098.2500 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 238/300\n",
            "3/3 - 0s - loss: 622241.1875 - mae: 355.5175 - mse: 622241.1875 - val_loss: 1040524.7500 - val_mae: 632.4526 - val_mse: 1040524.7500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 239/300\n",
            "3/3 - 0s - loss: 619771.0000 - mae: 356.9629 - mse: 619771.0000 - val_loss: 1066787.5000 - val_mae: 644.0451 - val_mse: 1066787.5000 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 240/300\n",
            "3/3 - 0s - loss: 610268.4375 - mae: 351.6898 - mse: 610268.4375 - val_loss: 1037373.7500 - val_mae: 630.5980 - val_mse: 1037373.7500 - lr: 1.0000e-04 - 209ms/epoch - 70ms/step\n",
            "Epoch 241/300\n",
            "3/3 - 0s - loss: 604429.4375 - mae: 350.2371 - mse: 604429.4375 - val_loss: 1056703.5000 - val_mae: 639.8550 - val_mse: 1056703.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 242/300\n",
            "3/3 - 0s - loss: 606578.3125 - mae: 353.0457 - mse: 606578.3125 - val_loss: 1053091.2500 - val_mae: 638.7562 - val_mse: 1053091.2500 - lr: 1.0000e-04 - 192ms/epoch - 64ms/step\n",
            "Epoch 243/300\n",
            "3/3 - 0s - loss: 609265.5625 - mae: 351.9113 - mse: 609265.5625 - val_loss: 1046086.0625 - val_mae: 635.0651 - val_mse: 1046086.0625 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 244/300\n",
            "3/3 - 0s - loss: 607618.0000 - mae: 353.0089 - mse: 607618.0000 - val_loss: 1058064.2500 - val_mae: 638.9886 - val_mse: 1058064.2500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 245/300\n",
            "3/3 - 0s - loss: 605471.7500 - mae: 350.4776 - mse: 605471.7500 - val_loss: 1041937.8750 - val_mae: 631.7382 - val_mse: 1041937.8750 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 246/300\n",
            "3/3 - 0s - loss: 602349.6875 - mae: 350.4246 - mse: 602349.6875 - val_loss: 1057620.0000 - val_mae: 638.6956 - val_mse: 1057620.0000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 247/300\n",
            "3/3 - 0s - loss: 600934.1250 - mae: 350.3555 - mse: 600934.1250 - val_loss: 1050694.3750 - val_mae: 636.4977 - val_mse: 1050694.3750 - lr: 1.0000e-04 - 254ms/epoch - 85ms/step\n",
            "Epoch 248/300\n",
            "3/3 - 0s - loss: 600801.5625 - mae: 349.9940 - mse: 600801.5625 - val_loss: 1053237.5000 - val_mae: 637.9113 - val_mse: 1053237.5000 - lr: 1.0000e-04 - 266ms/epoch - 89ms/step\n",
            "Epoch 249/300\n",
            "3/3 - 0s - loss: 600394.6875 - mae: 350.6556 - mse: 600394.6875 - val_loss: 1055617.6250 - val_mae: 638.3451 - val_mse: 1055617.6250 - lr: 1.0000e-04 - 268ms/epoch - 89ms/step\n",
            "Epoch 250/300\n",
            "3/3 - 0s - loss: 599831.1875 - mae: 349.3383 - mse: 599831.1875 - val_loss: 1048670.0000 - val_mae: 634.4871 - val_mse: 1048670.0000 - lr: 1.0000e-04 - 251ms/epoch - 84ms/step\n",
            "Epoch 251/300\n",
            "3/3 - 0s - loss: 598869.1250 - mae: 349.5922 - mse: 598869.1250 - val_loss: 1054887.5000 - val_mae: 636.7375 - val_mse: 1054887.5000 - lr: 1.0000e-04 - 246ms/epoch - 82ms/step\n",
            "Epoch 252/300\n",
            "3/3 - 0s - loss: 598078.8125 - mae: 349.2425 - mse: 598078.8125 - val_loss: 1051227.5000 - val_mae: 635.4882 - val_mse: 1051227.5000 - lr: 1.0000e-04 - 272ms/epoch - 91ms/step\n",
            "Epoch 253/300\n",
            "3/3 - 0s - loss: 597430.2500 - mae: 349.2362 - mse: 597430.2500 - val_loss: 1055091.6250 - val_mae: 637.4309 - val_mse: 1055091.6250 - lr: 1.0000e-04 - 287ms/epoch - 96ms/step\n",
            "Epoch 254/300\n",
            "3/3 - 0s - loss: 596969.5625 - mae: 349.4937 - mse: 596969.5625 - val_loss: 1054670.3750 - val_mae: 637.1838 - val_mse: 1054670.3750 - lr: 1.0000e-04 - 285ms/epoch - 95ms/step\n",
            "Epoch 255/300\n",
            "3/3 - 0s - loss: 596574.6250 - mae: 348.7585 - mse: 596574.6250 - val_loss: 1054164.1250 - val_mae: 636.7365 - val_mse: 1054164.1250 - lr: 1.0000e-04 - 229ms/epoch - 76ms/step\n",
            "Epoch 256/300\n",
            "3/3 - 0s - loss: 596407.8750 - mae: 349.4319 - mse: 596407.8750 - val_loss: 1054219.5000 - val_mae: 636.6671 - val_mse: 1054219.5000 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 257/300\n",
            "3/3 - 0s - loss: 597210.8750 - mae: 348.5172 - mse: 597210.8750 - val_loss: 1054121.0000 - val_mae: 636.7182 - val_mse: 1054121.0000 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 258/300\n",
            "3/3 - 0s - loss: 599364.0000 - mae: 351.2167 - mse: 599364.0000 - val_loss: 1055542.7500 - val_mae: 637.4468 - val_mse: 1055542.7500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 259/300\n",
            "3/3 - 0s - loss: 607268.2500 - mae: 351.2522 - mse: 607268.2500 - val_loss: 1058980.1250 - val_mae: 639.1260 - val_mse: 1058980.1250 - lr: 1.0000e-04 - 192ms/epoch - 64ms/step\n",
            "Epoch 260/300\n",
            "3/3 - 0s - loss: 635407.7500 - mae: 365.9163 - mse: 635407.7500 - val_loss: 1070778.7500 - val_mae: 644.7166 - val_mse: 1070778.7500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 261/300\n",
            "3/3 - 0s - loss: 663547.3750 - mae: 366.6385 - mse: 663547.3750 - val_loss: 1042658.1250 - val_mae: 631.6611 - val_mse: 1042658.1250 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 262/300\n",
            "3/3 - 0s - loss: 693995.5000 - mae: 385.9359 - mse: 693995.5000 - val_loss: 1154056.8750 - val_mae: 682.1869 - val_mse: 1154056.8750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 263/300\n",
            "3/3 - 0s - loss: 633866.3125 - mae: 358.2305 - mse: 633866.3125 - val_loss: 1035583.7500 - val_mae: 630.8413 - val_mse: 1035583.7500 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 264/300\n",
            "3/3 - 0s - loss: 601018.4375 - mae: 349.7813 - mse: 601018.4375 - val_loss: 1086063.2500 - val_mae: 654.5610 - val_mse: 1086063.2500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 265/300\n",
            "3/3 - 0s - loss: 595340.1250 - mae: 350.4300 - mse: 595340.1250 - val_loss: 1057029.6250 - val_mae: 641.4209 - val_mse: 1057029.6250 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 266/300\n",
            "3/3 - 0s - loss: 602024.3750 - mae: 350.5540 - mse: 602024.3750 - val_loss: 1056708.5000 - val_mae: 640.5638 - val_mse: 1056708.5000 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 267/300\n",
            "3/3 - 0s - loss: 604499.7500 - mae: 353.4116 - mse: 604499.7500 - val_loss: 1068381.3750 - val_mae: 642.9825 - val_mse: 1068381.3750 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 268/300\n",
            "3/3 - 0s - loss: 601628.3750 - mae: 349.0398 - mse: 601628.3750 - val_loss: 1043631.5000 - val_mae: 631.2302 - val_mse: 1043631.5000 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 269/300\n",
            "3/3 - 0s - loss: 596479.7500 - mae: 349.1509 - mse: 596479.7500 - val_loss: 1069737.5000 - val_mae: 643.3607 - val_mse: 1069737.5000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 270/300\n",
            "3/3 - 0s - loss: 592358.0000 - mae: 347.7776 - mse: 592358.0000 - val_loss: 1054641.2500 - val_mae: 638.3272 - val_mse: 1054641.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 271/300\n",
            "3/3 - 0s - loss: 591188.2500 - mae: 347.8455 - mse: 591188.2500 - val_loss: 1066457.3750 - val_mae: 643.6248 - val_mse: 1066457.3750 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 272/300\n",
            "3/3 - 0s - loss: 592037.4375 - mae: 349.2060 - mse: 592037.4375 - val_loss: 1062554.7500 - val_mae: 640.6542 - val_mse: 1062554.7500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 273/300\n",
            "3/3 - 0s - loss: 593232.5000 - mae: 347.2301 - mse: 593232.5000 - val_loss: 1054185.8750 - val_mae: 635.9723 - val_mse: 1054185.8750 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 274/300\n",
            "3/3 - 0s - loss: 594402.1875 - mae: 349.4757 - mse: 594402.1875 - val_loss: 1063431.3750 - val_mae: 639.3339 - val_mse: 1063431.3750 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 275/300\n",
            "3/3 - 0s - loss: 595998.6250 - mae: 347.9097 - mse: 595998.6250 - val_loss: 1055926.3750 - val_mae: 637.0450 - val_mse: 1055926.3750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 276/300\n",
            "3/3 - 0s - loss: 601007.3125 - mae: 352.9861 - mse: 601007.3125 - val_loss: 1070362.1250 - val_mae: 644.1918 - val_mse: 1070362.1250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 277/300\n",
            "3/3 - 0s - loss: 607335.4375 - mae: 351.5794 - mse: 607335.4375 - val_loss: 1057060.2500 - val_mae: 638.5119 - val_mse: 1057060.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 278/300\n",
            "3/3 - 0s - loss: 623835.5000 - mae: 361.9346 - mse: 623835.5000 - val_loss: 1085225.6250 - val_mae: 650.7134 - val_mse: 1085225.6250 - lr: 1.0000e-04 - 211ms/epoch - 70ms/step\n",
            "Epoch 279/300\n",
            "3/3 - 0s - loss: 625837.3125 - mae: 356.1729 - mse: 625837.3125 - val_loss: 1047248.1875 - val_mae: 633.6755 - val_mse: 1047248.1875 - lr: 1.0000e-04 - 215ms/epoch - 72ms/step\n",
            "Epoch 280/300\n",
            "3/3 - 0s - loss: 634423.6875 - mae: 365.3014 - mse: 634423.6875 - val_loss: 1107352.7500 - val_mae: 661.6940 - val_mse: 1107352.7500 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 281/300\n",
            "3/3 - 0s - loss: 612648.7500 - mae: 352.8168 - mse: 612648.7500 - val_loss: 1045941.8750 - val_mae: 634.8928 - val_mse: 1045941.8750 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 282/300\n",
            "3/3 - 0s - loss: 596037.6875 - mae: 350.0147 - mse: 596037.6875 - val_loss: 1090765.5000 - val_mae: 655.4869 - val_mse: 1090765.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 283/300\n",
            "3/3 - 0s - loss: 587643.6875 - mae: 347.1030 - mse: 587643.6875 - val_loss: 1055346.2500 - val_mae: 638.9969 - val_mse: 1055346.2500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 284/300\n",
            "3/3 - 0s - loss: 588545.5000 - mae: 346.1957 - mse: 588545.5000 - val_loss: 1063636.6250 - val_mae: 641.5013 - val_mse: 1063636.6250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 285/300\n",
            "3/3 - 0s - loss: 591244.6875 - mae: 349.2630 - mse: 591244.6875 - val_loss: 1073702.1250 - val_mae: 644.8222 - val_mse: 1073702.1250 - lr: 1.0000e-04 - 192ms/epoch - 64ms/step\n",
            "Epoch 286/300\n",
            "3/3 - 0s - loss: 588884.5625 - mae: 346.1501 - mse: 588884.5625 - val_loss: 1053989.8750 - val_mae: 636.8459 - val_mse: 1053989.8750 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 287/300\n",
            "3/3 - 0s - loss: 585482.3125 - mae: 346.2808 - mse: 585482.3125 - val_loss: 1075100.3750 - val_mae: 645.8116 - val_mse: 1075100.3750 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 288/300\n",
            "3/3 - 0s - loss: 584402.4375 - mae: 346.5507 - mse: 584402.4375 - val_loss: 1064414.6250 - val_mae: 641.6798 - val_mse: 1064414.6250 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 289/300\n",
            "3/3 - 0s - loss: 584965.1875 - mae: 345.7336 - mse: 584965.1875 - val_loss: 1064421.3750 - val_mae: 641.5446 - val_mse: 1064421.3750 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 290/300\n",
            "3/3 - 0s - loss: 584986.8125 - mae: 347.0937 - mse: 584986.8125 - val_loss: 1069509.3750 - val_mae: 642.7077 - val_mse: 1069509.3750 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 291/300\n",
            "3/3 - 0s - loss: 584630.5625 - mae: 345.2282 - mse: 584630.5625 - val_loss: 1059512.8750 - val_mae: 638.4830 - val_mse: 1059512.8750 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 292/300\n",
            "3/3 - 0s - loss: 584224.5000 - mae: 346.5518 - mse: 584224.5000 - val_loss: 1069632.7500 - val_mae: 642.3912 - val_mse: 1069632.7500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 293/300\n",
            "3/3 - 0s - loss: 583887.4375 - mae: 345.0806 - mse: 583887.4375 - val_loss: 1062471.2500 - val_mae: 640.3097 - val_mse: 1062471.2500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 294/300\n",
            "3/3 - 0s - loss: 583994.4375 - mae: 346.9819 - mse: 583994.4375 - val_loss: 1072441.2500 - val_mae: 644.3060 - val_mse: 1072441.2500 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 295/300\n",
            "3/3 - 0s - loss: 584692.6875 - mae: 345.3312 - mse: 584692.6875 - val_loss: 1064757.1250 - val_mae: 641.2889 - val_mse: 1064757.1250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 296/300\n",
            "3/3 - 0s - loss: 587207.1875 - mae: 348.5589 - mse: 587207.1875 - val_loss: 1075124.5000 - val_mae: 644.9088 - val_mse: 1075124.5000 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 297/300\n",
            "3/3 - 0s - loss: 590511.0625 - mae: 346.8188 - mse: 590511.0625 - val_loss: 1065845.8750 - val_mae: 641.6675 - val_mse: 1065845.8750 - lr: 1.0000e-04 - 252ms/epoch - 84ms/step\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 298: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 601307.2500 - mae: 354.5736 - mse: 601307.2500 - val_loss: 1080902.6250 - val_mae: 647.6853 - val_mse: 1080902.6250 - lr: 1.0000e-04 - 287ms/epoch - 96ms/step\n",
            "Epoch 299/300\n",
            "3/3 - 0s - loss: 579388.1875 - mae: 344.7235 - mse: 579388.1875 - val_loss: 1066285.0000 - val_mae: 642.0468 - val_mse: 1066285.0000 - lr: 1.0000e-05 - 270ms/epoch - 90ms/step\n",
            "Epoch 300/300\n",
            "3/3 - 0s - loss: 579775.0000 - mae: 344.0854 - mse: 579775.0000 - val_loss: 1059126.2500 - val_mae: 639.4937 - val_mse: 1059126.2500 - lr: 1.0000e-05 - 252ms/epoch - 84ms/step\n",
            "Optimizing model by reducing: mape for epochs: 300, num_iter: 2, model: NBEATS_model\n",
            "Epoch 1/300\n",
            "3/3 - 47s - loss: 51.7204 - mae: 4376.7710 - mse: 54304996.0000 - val_loss: 4.7321 - val_mae: 1604.7571 - val_mse: 5793550.5000 - lr: 0.0010 - 47s/epoch - 16s/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 7.3863 - mae: 1010.0469 - mse: 4635206.5000 - val_loss: 4.9088 - val_mae: 1670.5492 - val_mse: 6411848.0000 - lr: 0.0010 - 493ms/epoch - 164ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 7.4903 - mae: 1012.0231 - mse: 4627088.5000 - val_loss: 4.8720 - val_mae: 1658.2759 - val_mse: 6318444.0000 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 7.2748 - mae: 961.6419 - mse: 4125130.0000 - val_loss: 4.8083 - val_mae: 1651.5698 - val_mse: 6031796.0000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 6.7037 - mae: 849.9944 - mse: 3129312.5000 - val_loss: 4.3622 - val_mae: 1491.8270 - val_mse: 5081772.0000 - lr: 0.0010 - 346ms/epoch - 115ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 6.2319 - mae: 793.8959 - mse: 2683328.5000 - val_loss: 3.5073 - val_mae: 1168.4967 - val_mse: 3056044.5000 - lr: 0.0010 - 371ms/epoch - 124ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 6.1799 - mae: 707.5356 - mse: 1859494.0000 - val_loss: 4.6510 - val_mae: 1479.0767 - val_mse: 3643102.7500 - lr: 0.0010 - 213ms/epoch - 71ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 5.3892 - mae: 712.8377 - mse: 2301043.7500 - val_loss: 3.9255 - val_mae: 1332.4753 - val_mse: 3464913.7500 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 5.1620 - mae: 692.6826 - mse: 2169247.7500 - val_loss: 2.4813 - val_mae: 838.2037 - val_mse: 1576842.5000 - lr: 0.0010 - 318ms/epoch - 106ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 3.9004 - mae: 460.8134 - mse: 947760.1875 - val_loss: 2.7267 - val_mae: 890.7723 - val_mse: 1590704.2500 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 3.2735 - mae: 418.8683 - mse: 783718.3750 - val_loss: 2.7466 - val_mae: 908.1440 - val_mse: 1978337.5000 - lr: 0.0010 - 272ms/epoch - 91ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 3.4388 - mae: 419.9068 - mse: 853173.8125 - val_loss: 2.2731 - val_mae: 753.3121 - val_mse: 1279667.0000 - lr: 0.0010 - 399ms/epoch - 133ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 2.8494 - mae: 387.6809 - mse: 737385.4375 - val_loss: 1.8407 - val_mae: 622.2792 - val_mse: 1036326.0625 - lr: 0.0010 - 421ms/epoch - 140ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 3.2231 - mae: 412.9089 - mse: 751543.0000 - val_loss: 2.7244 - val_mae: 926.6722 - val_mse: 1741359.8750 - lr: 0.0010 - 291ms/epoch - 97ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 3.3332 - mae: 436.8363 - mse: 912322.8125 - val_loss: 1.9725 - val_mae: 665.8591 - val_mse: 1139741.1250 - lr: 0.0010 - 296ms/epoch - 99ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 3.4733 - mae: 440.7838 - mse: 816827.1250 - val_loss: 2.7151 - val_mae: 886.3151 - val_mse: 1551040.1250 - lr: 0.0010 - 258ms/epoch - 86ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 3.4742 - mae: 427.4145 - mse: 852819.7500 - val_loss: 2.6528 - val_mae: 910.5923 - val_mse: 1723308.6250 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 3.1020 - mae: 404.2275 - mse: 812193.3750 - val_loss: 2.2237 - val_mae: 763.7268 - val_mse: 1406708.6250 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 2.9064 - mae: 395.2541 - mse: 747157.0625 - val_loss: 1.8667 - val_mae: 631.5424 - val_mse: 1045851.2500 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 2.9110 - mae: 400.8799 - mse: 786002.2500 - val_loss: 1.9281 - val_mae: 668.8013 - val_mse: 1183397.7500 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 2.7874 - mae: 371.4225 - mse: 691141.8125 - val_loss: 2.0877 - val_mae: 707.0101 - val_mse: 1184123.0000 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 2.7024 - mae: 375.3984 - mse: 695868.0000 - val_loss: 1.9132 - val_mae: 661.7850 - val_mse: 1132530.8750 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 2.9907 - mae: 405.2951 - mse: 803606.6875 - val_loss: 1.8953 - val_mae: 658.0875 - val_mse: 1151103.2500 - lr: 0.0010 - 191ms/epoch - 64ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 2.7791 - mae: 371.8816 - mse: 691283.8750 - val_loss: 2.2065 - val_mae: 735.4406 - val_mse: 1220319.2500 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 2.7881 - mae: 378.1061 - mse: 709832.0000 - val_loss: 1.7951 - val_mae: 619.5284 - val_mse: 1033751.8125 - lr: 0.0010 - 362ms/epoch - 121ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 2.6349 - mae: 365.9914 - mse: 685541.8125 - val_loss: 1.8887 - val_mae: 654.1241 - val_mse: 1118190.6250 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 2.6177 - mae: 359.8673 - mse: 669266.3750 - val_loss: 1.8247 - val_mae: 626.6027 - val_mse: 1040446.9375 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 2.6203 - mae: 361.7141 - mse: 664057.5000 - val_loss: 1.9398 - val_mae: 655.0503 - val_mse: 1060662.7500 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 2.6541 - mae: 365.8836 - mse: 680022.8750 - val_loss: 1.7838 - val_mae: 611.6541 - val_mse: 1006034.9375 - lr: 0.0010 - 335ms/epoch - 112ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 2.6420 - mae: 368.9735 - mse: 672485.2500 - val_loss: 2.0519 - val_mae: 706.6195 - val_mse: 1229086.3750 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 2.9442 - mae: 382.9304 - mse: 709615.3125 - val_loss: 2.1410 - val_mae: 745.5352 - val_mse: 1369330.2500 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 2.9324 - mae: 394.7143 - mse: 765922.6250 - val_loss: 2.4773 - val_mae: 810.3524 - val_mse: 1358704.1250 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 2.9737 - mae: 390.1859 - mse: 742925.3750 - val_loss: 1.9363 - val_mae: 667.0387 - val_mse: 1129976.7500 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 2.6757 - mae: 371.5167 - mse: 688014.4375 - val_loss: 1.9736 - val_mae: 682.3984 - val_mse: 1151321.5000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 2.6100 - mae: 360.3048 - mse: 672087.4375 - val_loss: 1.8727 - val_mae: 636.3531 - val_mse: 1026719.7500 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 2.6976 - mae: 365.9133 - mse: 669899.5625 - val_loss: 2.0380 - val_mae: 685.0179 - val_mse: 1107819.5000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 2.6599 - mae: 361.7174 - mse: 665639.6875 - val_loss: 1.9292 - val_mae: 670.5183 - val_mse: 1159792.1250 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 2.7353 - mae: 368.2282 - mse: 678912.0625 - val_loss: 1.9918 - val_mae: 690.5362 - val_mse: 1213723.5000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 2.6907 - mae: 368.8617 - mse: 700287.2500 - val_loss: 1.9965 - val_mae: 670.3320 - val_mse: 1080955.8750 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 2.6739 - mae: 364.4706 - mse: 668193.1250 - val_loss: 1.8753 - val_mae: 640.1407 - val_mse: 1037681.0000 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 2.5802 - mae: 358.8297 - mse: 653082.4375 - val_loss: 1.9340 - val_mae: 669.7515 - val_mse: 1143431.2500 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 2.6807 - mae: 359.9474 - mse: 653492.4375 - val_loss: 1.9673 - val_mae: 682.1373 - val_mse: 1202879.2500 - lr: 0.0010 - 215ms/epoch - 72ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 2.6829 - mae: 369.8790 - mse: 703847.1875 - val_loss: 2.1102 - val_mae: 702.6447 - val_mse: 1133017.5000 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 2.7050 - mae: 365.2489 - mse: 672534.4375 - val_loss: 1.7986 - val_mae: 620.1167 - val_mse: 1017354.6250 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 2.5534 - mae: 355.8928 - mse: 643605.2500 - val_loss: 2.0384 - val_mae: 704.1003 - val_mse: 1208883.8750 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 2.6237 - mae: 355.8453 - mse: 654836.8125 - val_loss: 1.8322 - val_mae: 635.8883 - val_mse: 1099743.5000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 2.6021 - mae: 359.9127 - mse: 669771.8750 - val_loss: 2.1068 - val_mae: 702.6879 - val_mse: 1133931.7500 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 2.6822 - mae: 360.4816 - mse: 662486.8125 - val_loss: 1.7931 - val_mae: 619.5695 - val_mse: 1020110.8750 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 2.5356 - mae: 353.8538 - mse: 638203.4375 - val_loss: 1.9961 - val_mae: 691.1148 - val_mse: 1190494.6250 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 2.6481 - mae: 352.3946 - mse: 641714.1250 - val_loss: 1.9201 - val_mae: 666.1450 - val_mse: 1171967.6250 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 2.6258 - mae: 362.9700 - mse: 685576.6875 - val_loss: 2.0152 - val_mae: 676.2709 - val_mse: 1086163.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 2.6757 - mae: 360.6080 - mse: 657247.8125 - val_loss: 1.8652 - val_mae: 637.4617 - val_mse: 1034196.5000 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 2.5712 - mae: 354.1109 - mse: 640472.1250 - val_loss: 1.9979 - val_mae: 691.0315 - val_mse: 1189019.6250 - lr: 0.0010 - 193ms/epoch - 64ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 2.6576 - mae: 353.5771 - mse: 640767.6250 - val_loss: 1.9615 - val_mae: 682.5000 - val_mse: 1216364.7500 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 2.6757 - mae: 366.1717 - mse: 693923.1875 - val_loss: 2.1994 - val_mae: 730.9606 - val_mse: 1183390.7500 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 2.6879 - mae: 362.1661 - mse: 666853.6875 - val_loss: 1.8107 - val_mae: 627.4743 - val_mse: 1035455.9375 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 2.5728 - mae: 354.7691 - mse: 633008.6250 - val_loss: 2.1281 - val_mae: 734.1690 - val_mse: 1285077.3750 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 2.6318 - mae: 358.2585 - mse: 675564.0000 - val_loss: 1.8199 - val_mae: 625.4955 - val_mse: 1055147.0000 - lr: 0.0010 - 245ms/epoch - 82ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 2.6212 - mae: 356.1418 - mse: 646464.1875 - val_loss: 2.1719 - val_mae: 723.7430 - val_mse: 1171068.2500 - lr: 0.0010 - 288ms/epoch - 96ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 2.6732 - mae: 364.3008 - mse: 677828.3125 - val_loss: 1.8952 - val_mae: 656.6728 - val_mse: 1114409.3750 - lr: 0.0010 - 263ms/epoch - 88ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 2.6911 - mae: 360.4057 - mse: 636656.7500 - val_loss: 2.1978 - val_mae: 754.6197 - val_mse: 1357703.2500 - lr: 0.0010 - 245ms/epoch - 82ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 2.7072 - mae: 374.9042 - mse: 731290.2500 - val_loss: 2.1388 - val_mae: 710.6783 - val_mse: 1158018.0000 - lr: 0.0010 - 281ms/epoch - 94ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 2.7642 - mae: 373.9080 - mse: 696034.8750 - val_loss: 1.7958 - val_mae: 615.6659 - val_mse: 1009664.7500 - lr: 0.0010 - 278ms/epoch - 93ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 2.4896 - mae: 343.5163 - mse: 616487.4375 - val_loss: 2.0619 - val_mae: 709.4934 - val_mse: 1218654.2500 - lr: 0.0010 - 297ms/epoch - 99ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 2.5444 - mae: 352.2900 - mse: 649411.4375 - val_loss: 1.7722 - val_mae: 612.8060 - val_mse: 1029041.3750 - lr: 0.0010 - 429ms/epoch - 143ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 2.4470 - mae: 341.1677 - mse: 623139.8125 - val_loss: 1.9672 - val_mae: 662.6018 - val_mse: 1071885.6250 - lr: 0.0010 - 216ms/epoch - 72ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 2.4773 - mae: 339.8071 - mse: 611474.1875 - val_loss: 1.8034 - val_mae: 622.9051 - val_mse: 1028205.9375 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 2.4237 - mae: 338.0836 - mse: 601500.1250 - val_loss: 1.9549 - val_mae: 676.3220 - val_mse: 1158475.1250 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 2.5192 - mae: 337.0258 - mse: 605059.0000 - val_loss: 1.9153 - val_mae: 664.4768 - val_mse: 1175851.3750 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 2.5365 - mae: 358.2928 - mse: 678998.6875 - val_loss: 1.8499 - val_mae: 631.6747 - val_mse: 1027251.5625 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 2.6530 - mae: 358.5148 - mse: 634780.3750 - val_loss: 2.0225 - val_mae: 680.7547 - val_mse: 1119393.7500 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 2.6736 - mae: 356.5330 - mse: 640396.4375 - val_loss: 2.2887 - val_mae: 783.7405 - val_mse: 1390157.0000 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 2.6842 - mae: 356.4219 - mse: 671254.9375 - val_loss: 1.8314 - val_mae: 633.7975 - val_mse: 1093253.0000 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 2.5559 - mae: 349.8468 - mse: 636901.5000 - val_loss: 2.1580 - val_mae: 718.2396 - val_mse: 1170193.3750 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 2.6278 - mae: 354.5092 - mse: 645327.6250 - val_loss: 1.9276 - val_mae: 666.5093 - val_mse: 1123648.0000 - lr: 0.0010 - 192ms/epoch - 64ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 2.5354 - mae: 344.0200 - mse: 611110.8750 - val_loss: 2.0249 - val_mae: 700.1546 - val_mse: 1220639.1250 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 2.4987 - mae: 346.7056 - mse: 648147.6250 - val_loss: 1.9975 - val_mae: 671.3502 - val_mse: 1095326.8750 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 2.5971 - mae: 350.6402 - mse: 627271.0625 - val_loss: 1.9308 - val_mae: 654.1726 - val_mse: 1060750.5000 - lr: 0.0010 - 216ms/epoch - 72ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 2.4893 - mae: 338.1811 - mse: 601176.8750 - val_loss: 2.0289 - val_mae: 701.6506 - val_mse: 1208698.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 2.5465 - mae: 341.8453 - mse: 616663.8750 - val_loss: 1.8799 - val_mae: 652.0342 - val_mse: 1126967.1250 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 2.4473 - mae: 338.9983 - mse: 630714.1250 - val_loss: 1.9462 - val_mae: 656.2831 - val_mse: 1071762.7500 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 2.4473 - mae: 332.8483 - mse: 592531.7500 - val_loss: 1.8637 - val_mae: 639.7534 - val_mse: 1052611.5000 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 2.3949 - mae: 332.6197 - mse: 592175.8750 - val_loss: 1.9239 - val_mae: 665.2695 - val_mse: 1124467.1250 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 2.4562 - mae: 330.1848 - mse: 584959.4375 - val_loss: 1.9145 - val_mae: 662.1836 - val_mse: 1154508.1250 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 2.4111 - mae: 336.9877 - mse: 625438.1250 - val_loss: 1.8688 - val_mae: 635.7762 - val_mse: 1033746.6250 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 2.4979 - mae: 336.6864 - mse: 590738.5625 - val_loss: 1.9767 - val_mae: 671.9343 - val_mse: 1098555.2500 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 345.9138 - mse: 623381.6250 - val_loss: 2.0396 - val_mae: 703.9041 - val_mse: 1207143.5000 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 2.5901 - mae: 340.9108 - mse: 607757.4375 - val_loss: 2.0570 - val_mae: 710.2721 - val_mse: 1268070.7500 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 2.6038 - mae: 357.5797 - mse: 685048.2500 - val_loss: 2.3029 - val_mae: 760.0319 - val_mse: 1257726.8750 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 2.6629 - mae: 359.6290 - mse: 664034.3750 - val_loss: 1.8295 - val_mae: 633.3703 - val_mse: 1076100.8750 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 2.4045 - mae: 332.1934 - mse: 589120.0000 - val_loss: 2.0545 - val_mae: 707.6285 - val_mse: 1213138.3750 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 2.4387 - mae: 343.4276 - mse: 641063.9375 - val_loss: 1.9001 - val_mae: 646.8281 - val_mse: 1059786.2500 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 2.4894 - mae: 339.8258 - mse: 604302.8125 - val_loss: 2.0105 - val_mae: 678.3021 - val_mse: 1122491.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 2.4456 - mae: 336.2598 - mse: 597244.5625 - val_loss: 1.9673 - val_mae: 677.3779 - val_mse: 1176955.0000 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 2.5824 - mae: 342.9437 - mse: 605786.7500 - val_loss: 2.1000 - val_mae: 725.6967 - val_mse: 1285193.2500 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 2.5025 - mae: 343.4965 - mse: 642264.0000 - val_loss: 2.0002 - val_mae: 673.7240 - val_mse: 1103448.3750 - lr: 0.0010 - 208ms/epoch - 69ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 2.5300 - mae: 342.6808 - mse: 612592.2500 - val_loss: 1.8776 - val_mae: 640.4292 - val_mse: 1045412.5625 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 2.3641 - mae: 323.7138 - mse: 574962.9375 - val_loss: 1.9279 - val_mae: 665.0436 - val_mse: 1131078.0000 - lr: 0.0010 - 194ms/epoch - 65ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 2.3072 - mae: 323.4520 - mse: 584310.0000 - val_loss: 1.8407 - val_mae: 632.6744 - val_mse: 1070216.0000 - lr: 0.0010 - 209ms/epoch - 70ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 2.2861 - mae: 320.8641 - mse: 579527.3125 - val_loss: 1.9129 - val_mae: 648.2748 - val_mse: 1062483.8750 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 2.2744 - mae: 316.3177 - mse: 559692.5625 - val_loss: 1.8540 - val_mae: 635.1628 - val_mse: 1050337.2500 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 2.2791 - mae: 318.5585 - mse: 559350.4375 - val_loss: 1.8916 - val_mae: 653.5435 - val_mse: 1108992.3750 - lr: 0.0010 - 192ms/epoch - 64ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 2.3800 - mae: 320.3281 - mse: 562980.1250 - val_loss: 2.0079 - val_mae: 692.4987 - val_mse: 1224821.7500 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 2.3946 - mae: 333.2344 - mse: 626533.1875 - val_loss: 1.8764 - val_mae: 638.3010 - val_mse: 1049970.8750 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 2.4952 - mae: 335.4154 - mse: 582008.4375 - val_loss: 2.1185 - val_mae: 707.9705 - val_mse: 1177323.6250 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 2.5853 - mae: 343.6469 - mse: 613241.5000 - val_loss: 2.1564 - val_mae: 740.6226 - val_mse: 1294573.3750 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 2.4419 - mae: 327.8840 - mse: 593025.6875 - val_loss: 1.8782 - val_mae: 644.2065 - val_mse: 1115901.6250 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 2.3345 - mae: 324.9388 - mse: 596592.0625 - val_loss: 1.9936 - val_mae: 671.1360 - val_mse: 1109042.2500 - lr: 0.0010 - 329ms/epoch - 110ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 2.3493 - mae: 323.1931 - mse: 574067.6875 - val_loss: 1.9174 - val_mae: 656.1173 - val_mse: 1098137.1250 - lr: 0.0010 - 278ms/epoch - 93ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 2.3720 - mae: 328.8348 - mse: 569232.1250 - val_loss: 2.0956 - val_mae: 719.8299 - val_mse: 1266603.2500 - lr: 0.0010 - 245ms/epoch - 82ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 2.5172 - mae: 345.5628 - mse: 647866.1250 - val_loss: 1.8665 - val_mae: 641.2441 - val_mse: 1096101.8750 - lr: 0.0010 - 248ms/epoch - 83ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 2.4215 - mae: 330.5473 - mse: 579067.9375 - val_loss: 2.1486 - val_mae: 717.0388 - val_mse: 1181437.5000 - lr: 0.0010 - 274ms/epoch - 91ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 2.5232 - mae: 335.3037 - mse: 599423.8750 - val_loss: 1.9710 - val_mae: 681.1171 - val_mse: 1175692.0000 - lr: 0.0010 - 297ms/epoch - 99ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 2.3422 - mae: 322.3938 - mse: 578022.2500 - val_loss: 1.9480 - val_mae: 670.5359 - val_mse: 1161522.2500 - lr: 0.0010 - 306ms/epoch - 102ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 2.3450 - mae: 334.8028 - mse: 618277.4375 - val_loss: 1.9834 - val_mae: 669.5429 - val_mse: 1105098.8750 - lr: 0.0010 - 297ms/epoch - 99ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 2.4301 - mae: 329.7384 - mse: 581075.3750 - val_loss: 1.9567 - val_mae: 663.5146 - val_mse: 1089996.3750 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 2.3808 - mae: 327.4133 - mse: 577191.1250 - val_loss: 2.1573 - val_mae: 741.6682 - val_mse: 1305514.7500 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 2.4909 - mae: 330.2480 - mse: 598468.5000 - val_loss: 1.8497 - val_mae: 635.9526 - val_mse: 1090265.1250 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 2.2908 - mae: 319.6689 - mse: 572949.3750 - val_loss: 2.1045 - val_mae: 706.8592 - val_mse: 1175564.1250 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 2.4200 - mae: 333.0804 - mse: 598216.8125 - val_loss: 1.8953 - val_mae: 655.0924 - val_mse: 1116477.6250 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 2.3369 - mae: 318.3021 - mse: 545037.9375 - val_loss: 2.0609 - val_mae: 705.7555 - val_mse: 1242937.0000 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 2.3765 - mae: 332.8945 - mse: 621238.3750 - val_loss: 1.8880 - val_mae: 642.7413 - val_mse: 1068931.7500 - lr: 0.0010 - 203ms/epoch - 68ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 2.4055 - mae: 330.2394 - mse: 586787.6250 - val_loss: 1.9790 - val_mae: 670.0671 - val_mse: 1105065.6250 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 2.3020 - mae: 315.8341 - mse: 550943.5625 - val_loss: 1.9914 - val_mae: 684.8156 - val_mse: 1189469.7500 - lr: 0.0010 - 216ms/epoch - 72ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 2.3202 - mae: 314.6100 - mse: 559425.5625 - val_loss: 1.9207 - val_mae: 662.3674 - val_mse: 1164454.0000 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 2.2601 - mae: 314.7197 - mse: 573354.1250 - val_loss: 1.9055 - val_mae: 647.3102 - val_mse: 1082992.3750 - lr: 0.0010 - 202ms/epoch - 67ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 2.2991 - mae: 310.6378 - mse: 536134.8750 - val_loss: 2.0390 - val_mae: 685.6101 - val_mse: 1134857.6250 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 2.3572 - mae: 323.6677 - mse: 578716.1250 - val_loss: 2.0097 - val_mae: 688.7292 - val_mse: 1190028.1250 - lr: 0.0010 - 212ms/epoch - 71ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 2.3737 - mae: 314.7276 - mse: 549578.6250 - val_loss: 2.0199 - val_mae: 692.4250 - val_mse: 1231678.8750 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 2.4294 - mae: 336.9377 - mse: 627199.7500 - val_loss: 2.0528 - val_mae: 686.6185 - val_mse: 1134953.5000 - lr: 0.0010 - 206ms/epoch - 69ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 2.3999 - mae: 328.1995 - mse: 589386.2500 - val_loss: 1.9321 - val_mae: 656.9628 - val_mse: 1101387.2500 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 2.3112 - mae: 317.9123 - mse: 550382.0625 - val_loss: 2.0055 - val_mae: 687.4698 - val_mse: 1192943.2500 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 2.2621 - mae: 315.5374 - mse: 559100.5625 - val_loss: 1.8571 - val_mae: 636.1456 - val_mse: 1087235.1250 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 2.2018 - mae: 308.8200 - mse: 551989.2500 - val_loss: 1.9375 - val_mae: 655.4587 - val_mse: 1087965.6250 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 2.2281 - mae: 303.9462 - mse: 527400.5000 - val_loss: 1.9265 - val_mae: 657.3223 - val_mse: 1100325.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 2.1594 - mae: 299.6687 - mse: 522203.2812 - val_loss: 1.8948 - val_mae: 649.5241 - val_mse: 1119367.6250 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 2.2036 - mae: 300.8265 - mse: 526335.8125 - val_loss: 1.9237 - val_mae: 662.3765 - val_mse: 1156173.5000 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 2.2158 - mae: 305.3371 - mse: 544019.5000 - val_loss: 1.8958 - val_mae: 647.9395 - val_mse: 1090620.5000 - lr: 0.0010 - 195ms/epoch - 65ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 2.1861 - mae: 311.0986 - mse: 551832.6250 - val_loss: 1.8866 - val_mae: 642.8756 - val_mse: 1087569.0000 - lr: 0.0010 - 197ms/epoch - 66ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 2.2346 - mae: 300.5796 - mse: 519677.1562 - val_loss: 2.1592 - val_mae: 719.4930 - val_mse: 1200146.6250 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 2.3459 - mae: 325.9008 - mse: 579990.0625 - val_loss: 1.9742 - val_mae: 674.9642 - val_mse: 1179988.2500 - lr: 0.0010 - 211ms/epoch - 70ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 2.4586 - mae: 340.3489 - mse: 611744.8750 - val_loss: 2.0690 - val_mae: 709.6544 - val_mse: 1270790.8750 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 2.4420 - mae: 345.4037 - mse: 651189.1250 - val_loss: 1.9630 - val_mae: 662.6506 - val_mse: 1094455.6250 - lr: 0.0010 - 226ms/epoch - 75ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 2.3980 - mae: 333.1512 - mse: 591440.1875 - val_loss: 1.9026 - val_mae: 642.5170 - val_mse: 1059655.6250 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 2.3558 - mae: 328.0807 - mse: 579833.7500 - val_loss: 2.0355 - val_mae: 691.3310 - val_mse: 1207920.8750 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 2.2759 - mae: 326.8046 - mse: 600146.4375 - val_loss: 1.8668 - val_mae: 630.8281 - val_mse: 1044685.8125 - lr: 0.0010 - 213ms/epoch - 71ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 2.2650 - mae: 319.2278 - mse: 564851.5000 - val_loss: 2.0890 - val_mae: 698.4905 - val_mse: 1175745.7500 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 2.3068 - mae: 327.0524 - mse: 579988.0000 - val_loss: 1.9913 - val_mae: 682.7840 - val_mse: 1191201.5000 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 2.3552 - mae: 324.8274 - mse: 575347.1875 - val_loss: 1.9859 - val_mae: 682.4951 - val_mse: 1223206.1250 - lr: 0.0010 - 210ms/epoch - 70ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 2.2934 - mae: 315.6165 - mse: 565345.3750 - val_loss: 1.9781 - val_mae: 664.5479 - val_mse: 1102189.8750 - lr: 0.0010 - 200ms/epoch - 67ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 2.2478 - mae: 312.2056 - mse: 555765.9375 - val_loss: 1.9033 - val_mae: 647.1943 - val_mse: 1086688.2500 - lr: 0.0010 - 196ms/epoch - 65ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 2.2861 - mae: 311.7484 - mse: 528045.7500 - val_loss: 2.0521 - val_mae: 700.5965 - val_mse: 1236334.6250 - lr: 0.0010 - 205ms/epoch - 68ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 2.2244 - mae: 310.5585 - mse: 563973.7500 - val_loss: 1.9363 - val_mae: 655.2473 - val_mse: 1119499.5000 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 2.2354 - mae: 310.1637 - mse: 549515.4375 - val_loss: 1.9924 - val_mae: 668.3343 - val_mse: 1119427.2500 - lr: 0.0010 - 201ms/epoch - 67ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 2.1725 - mae: 303.0735 - mse: 523702.0000 - val_loss: 1.9737 - val_mae: 670.3599 - val_mse: 1158104.7500 - lr: 0.0010 - 207ms/epoch - 69ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 2.1929 - mae: 299.9552 - mse: 510981.3750 - val_loss: 2.0254 - val_mae: 689.9981 - val_mse: 1235477.8750 - lr: 0.0010 - 198ms/epoch - 66ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 2.1723 - mae: 308.1245 - mse: 559603.2500 - val_loss: 1.9085 - val_mae: 646.6411 - val_mse: 1094295.3750 - lr: 0.0010 - 204ms/epoch - 68ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 2.2518 - mae: 298.7231 - mse: 507337.1875 - val_loss: 2.2015 - val_mae: 733.1286 - val_mse: 1213235.8750 - lr: 0.0010 - 199ms/epoch - 66ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 2.3449 - mae: 316.5194 - mse: 561159.5625 - val_loss: 2.0390 - val_mae: 697.3792 - val_mse: 1212484.5000 - lr: 0.0010 - 284ms/epoch - 95ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 2.3763 - mae: 314.2179 - mse: 546345.3125 - val_loss: 2.1795 - val_mae: 744.6934 - val_mse: 1368030.5000 - lr: 0.0010 - 257ms/epoch - 86ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 2.4298 - mae: 331.8994 - mse: 601389.5000 - val_loss: 2.3109 - val_mae: 759.5901 - val_mse: 1267078.3750 - lr: 0.0010 - 257ms/epoch - 86ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 2.5100 - mae: 343.7304 - mse: 630713.2500 - val_loss: 2.0571 - val_mae: 705.9598 - val_mse: 1236899.6250 - lr: 0.0010 - 299ms/epoch - 100ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 2.2773 - mae: 316.5384 - mse: 556796.5000 - val_loss: 1.9281 - val_mae: 657.0475 - val_mse: 1145253.2500 - lr: 0.0010 - 278ms/epoch - 93ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 2.3920 - mae: 321.5939 - mse: 548445.6250 - val_loss: 2.3868 - val_mae: 795.2292 - val_mse: 1405915.7500 - lr: 0.0010 - 271ms/epoch - 90ms/step\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 165: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 2.4992 - mae: 326.3943 - mse: 576088.9375 - val_loss: 2.2662 - val_mae: 775.2039 - val_mse: 1413079.0000 - lr: 0.0010 - 312ms/epoch - 104ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 2.4315 - mae: 316.1921 - mse: 540580.8125 - val_loss: 1.9671 - val_mae: 670.9642 - val_mse: 1168032.8750 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 2.1604 - mae: 300.6572 - mse: 521799.8438 - val_loss: 1.9596 - val_mae: 661.1688 - val_mse: 1105480.5000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 2.2058 - mae: 310.1769 - mse: 547265.6875 - val_loss: 1.9795 - val_mae: 667.5131 - val_mse: 1111197.7500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 2.0889 - mae: 286.7765 - mse: 500330.8438 - val_loss: 1.8863 - val_mae: 641.5013 - val_mse: 1076922.2500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 2.0650 - mae: 290.5143 - mse: 502292.0625 - val_loss: 1.9084 - val_mae: 651.9728 - val_mse: 1116254.3750 - lr: 1.0000e-04 - 209ms/epoch - 70ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 2.0568 - mae: 283.0493 - mse: 493640.5625 - val_loss: 1.8770 - val_mae: 639.2733 - val_mse: 1070191.3750 - lr: 1.0000e-04 - 305ms/epoch - 102ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 2.0003 - mae: 281.9034 - mse: 500127.1875 - val_loss: 1.9047 - val_mae: 644.8312 - val_mse: 1079843.6250 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 1.9961 - mae: 279.3011 - mse: 494763.5938 - val_loss: 1.9003 - val_mae: 645.8795 - val_mse: 1088226.2500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 1.9654 - mae: 275.3947 - mse: 484413.1250 - val_loss: 1.9163 - val_mae: 653.0916 - val_mse: 1111548.6250 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 1.9709 - mae: 274.9503 - mse: 483001.5625 - val_loss: 1.9121 - val_mae: 650.2878 - val_mse: 1099266.7500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 1.9418 - mae: 272.4636 - mse: 485173.3750 - val_loss: 1.9070 - val_mae: 646.6281 - val_mse: 1086341.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 1.9383 - mae: 271.4560 - mse: 483686.1875 - val_loss: 1.9051 - val_mae: 646.7087 - val_mse: 1087512.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 1.9256 - mae: 270.8603 - mse: 479527.3438 - val_loss: 1.9118 - val_mae: 649.4478 - val_mse: 1094922.6250 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 1.9171 - mae: 268.8278 - mse: 478314.3438 - val_loss: 1.9099 - val_mae: 647.7715 - val_mse: 1087284.1250 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 1.9116 - mae: 268.1292 - mse: 478623.0312 - val_loss: 1.9092 - val_mae: 647.6996 - val_mse: 1085688.0000 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 1.9041 - mae: 266.9583 - mse: 474777.8438 - val_loss: 1.9136 - val_mae: 650.0126 - val_mse: 1092697.6250 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 1.8978 - mae: 265.8338 - mse: 473875.6875 - val_loss: 1.9080 - val_mae: 647.5678 - val_mse: 1084104.0000 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 1.8942 - mae: 265.2438 - mse: 473659.3750 - val_loss: 1.9065 - val_mae: 647.0696 - val_mse: 1083527.1250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 1.8868 - mae: 264.2827 - mse: 469890.1250 - val_loss: 1.9066 - val_mae: 647.5781 - val_mse: 1088397.3750 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 1.8825 - mae: 263.4780 - mse: 469446.0000 - val_loss: 1.9055 - val_mae: 646.5911 - val_mse: 1082430.6250 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 1.8772 - mae: 262.7506 - mse: 467920.2812 - val_loss: 1.9075 - val_mae: 647.7432 - val_mse: 1086589.6250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 1.8728 - mae: 262.1702 - mse: 467075.8438 - val_loss: 1.9064 - val_mae: 647.8541 - val_mse: 1088579.7500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 1.8685 - mae: 261.5019 - mse: 466068.1562 - val_loss: 1.9099 - val_mae: 648.6675 - val_mse: 1088500.7500 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 1.8647 - mae: 260.9014 - mse: 466035.6562 - val_loss: 1.9089 - val_mae: 648.6717 - val_mse: 1090439.2500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 1.8613 - mae: 260.4500 - mse: 464428.9688 - val_loss: 1.9114 - val_mae: 649.6631 - val_mse: 1093123.5000 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 1.8558 - mae: 259.6576 - mse: 464300.7188 - val_loss: 1.9125 - val_mae: 650.0472 - val_mse: 1093604.2500 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 1.8534 - mae: 259.1520 - mse: 462991.4062 - val_loss: 1.9123 - val_mae: 650.1617 - val_mse: 1094804.6250 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 1.8490 - mae: 258.6116 - mse: 462930.8125 - val_loss: 1.9138 - val_mae: 650.6442 - val_mse: 1094596.0000 - lr: 1.0000e-04 - 224ms/epoch - 75ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 1.8469 - mae: 258.0238 - mse: 461316.1562 - val_loss: 1.9126 - val_mae: 650.6116 - val_mse: 1097236.7500 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 1.8441 - mae: 257.8371 - mse: 461175.2812 - val_loss: 1.9154 - val_mae: 651.6270 - val_mse: 1099537.0000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 1.8424 - mae: 257.2609 - mse: 460075.7812 - val_loss: 1.9134 - val_mae: 651.2883 - val_mse: 1099969.1250 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 1.8384 - mae: 257.0151 - mse: 459264.2188 - val_loss: 1.9186 - val_mae: 652.8491 - val_mse: 1103562.0000 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 1.8343 - mae: 256.2068 - mse: 459021.5625 - val_loss: 1.9149 - val_mae: 652.1802 - val_mse: 1104573.8750 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 1.8346 - mae: 255.9809 - mse: 457546.0938 - val_loss: 1.9208 - val_mae: 653.2617 - val_mse: 1103916.3750 - lr: 1.0000e-04 - 209ms/epoch - 70ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 1.8319 - mae: 256.2337 - mse: 458304.1250 - val_loss: 1.9203 - val_mae: 654.1617 - val_mse: 1110291.3750 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 1.8288 - mae: 255.3376 - mse: 456364.0000 - val_loss: 1.9176 - val_mae: 653.2094 - val_mse: 1106312.7500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 1.8272 - mae: 255.4937 - mse: 456358.6250 - val_loss: 1.9234 - val_mae: 655.3362 - val_mse: 1115738.1250 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 1.8242 - mae: 254.2366 - mse: 455639.0312 - val_loss: 1.9152 - val_mae: 652.0649 - val_mse: 1102106.7500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 1.8250 - mae: 254.0586 - mse: 454722.6875 - val_loss: 1.9212 - val_mae: 654.6950 - val_mse: 1113097.0000 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 1.8187 - mae: 254.1528 - mse: 455442.2500 - val_loss: 1.9204 - val_mae: 654.0472 - val_mse: 1108212.7500 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 1.8155 - mae: 253.5536 - mse: 453245.2812 - val_loss: 1.9215 - val_mae: 654.5502 - val_mse: 1111405.2500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 1.8120 - mae: 253.2232 - mse: 453860.9375 - val_loss: 1.9222 - val_mae: 655.3378 - val_mse: 1114915.7500 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 1.8107 - mae: 252.3763 - mse: 452446.1562 - val_loss: 1.9189 - val_mae: 653.7300 - val_mse: 1107513.0000 - lr: 1.0000e-04 - 275ms/epoch - 92ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 1.8093 - mae: 252.3793 - mse: 451850.9688 - val_loss: 1.9250 - val_mae: 656.4785 - val_mse: 1120033.0000 - lr: 1.0000e-04 - 270ms/epoch - 90ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 1.8077 - mae: 252.1077 - mse: 452169.1875 - val_loss: 1.9186 - val_mae: 653.3718 - val_mse: 1106992.8750 - lr: 1.0000e-04 - 263ms/epoch - 88ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 1.8094 - mae: 252.1248 - mse: 450480.4375 - val_loss: 1.9259 - val_mae: 656.7409 - val_mse: 1120452.3750 - lr: 1.0000e-04 - 284ms/epoch - 95ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 1.8043 - mae: 251.9340 - mse: 451453.9062 - val_loss: 1.9223 - val_mae: 655.2118 - val_mse: 1113950.0000 - lr: 1.0000e-04 - 276ms/epoch - 92ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 1.8012 - mae: 251.1382 - mse: 448949.1250 - val_loss: 1.9244 - val_mae: 655.9050 - val_mse: 1117937.6250 - lr: 1.0000e-04 - 311ms/epoch - 104ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 1.7972 - mae: 251.5979 - mse: 450665.8125 - val_loss: 1.9251 - val_mae: 656.4747 - val_mse: 1119701.2500 - lr: 1.0000e-04 - 293ms/epoch - 98ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 1.7955 - mae: 250.3650 - mse: 447795.1875 - val_loss: 1.9226 - val_mae: 655.5582 - val_mse: 1117808.8750 - lr: 1.0000e-04 - 208ms/epoch - 69ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 1.7911 - mae: 250.0781 - mse: 448063.7188 - val_loss: 1.9285 - val_mae: 657.7058 - val_mse: 1124134.7500 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 1.7886 - mae: 249.3089 - mse: 447434.7188 - val_loss: 1.9206 - val_mae: 655.1644 - val_mse: 1117246.8750 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 1.7877 - mae: 249.0679 - mse: 445974.6875 - val_loss: 1.9300 - val_mae: 657.9073 - val_mse: 1121373.5000 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 1.7871 - mae: 248.6175 - mse: 446068.0312 - val_loss: 1.9193 - val_mae: 655.4055 - val_mse: 1121788.0000 - lr: 1.0000e-04 - 206ms/epoch - 69ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 1.7850 - mae: 248.3438 - mse: 444781.3438 - val_loss: 1.9297 - val_mae: 657.7215 - val_mse: 1122688.2500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 1.7845 - mae: 248.9685 - mse: 446299.2188 - val_loss: 1.9243 - val_mae: 657.3482 - val_mse: 1126779.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 1.7870 - mae: 249.1242 - mse: 444608.6875 - val_loss: 1.9289 - val_mae: 657.7840 - val_mse: 1122152.6250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 1.7826 - mae: 249.4866 - mse: 446039.2812 - val_loss: 1.9286 - val_mae: 658.4775 - val_mse: 1130272.6250 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 224/300\n",
            "3/3 - 0s - loss: 1.7770 - mae: 247.5793 - mse: 444109.1562 - val_loss: 1.9229 - val_mae: 656.4713 - val_mse: 1124764.3750 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 1.7744 - mae: 247.2342 - mse: 442657.4375 - val_loss: 1.9341 - val_mae: 660.3651 - val_mse: 1135822.1250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 1.7749 - mae: 245.9994 - mse: 441005.6875 - val_loss: 1.9236 - val_mae: 656.3990 - val_mse: 1121461.7500 - lr: 1.0000e-04 - 203ms/epoch - 68ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 1.7763 - mae: 246.0980 - mse: 440723.0625 - val_loss: 1.9313 - val_mae: 659.9868 - val_mse: 1135257.2500 - lr: 1.0000e-04 - 196ms/epoch - 65ms/step\n",
            "Epoch 228/300\n",
            "3/3 - 0s - loss: 1.7775 - mae: 246.8708 - mse: 440323.7812 - val_loss: 1.9332 - val_mae: 658.8560 - val_mse: 1124334.6250 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 229/300\n",
            "3/3 - 0s - loss: 1.7687 - mae: 245.8531 - mse: 440276.8125 - val_loss: 1.9276 - val_mae: 658.8547 - val_mse: 1134255.3750 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 230/300\n",
            "3/3 - 0s - loss: 1.7667 - mae: 245.6436 - mse: 439779.3125 - val_loss: 1.9306 - val_mae: 658.5752 - val_mse: 1124422.6250 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 231/300\n",
            "3/3 - 0s - loss: 1.7625 - mae: 245.1028 - mse: 438223.1250 - val_loss: 1.9285 - val_mae: 658.8784 - val_mse: 1133200.6250 - lr: 1.0000e-04 - 204ms/epoch - 68ms/step\n",
            "Epoch 232/300\n",
            "3/3 - 0s - loss: 1.7575 - mae: 244.6434 - mse: 438676.0625 - val_loss: 1.9267 - val_mae: 657.7572 - val_mse: 1123864.8750 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 233/300\n",
            "3/3 - 0s - loss: 1.7536 - mae: 243.5814 - mse: 436896.3125 - val_loss: 1.9316 - val_mae: 659.3541 - val_mse: 1133226.1250 - lr: 1.0000e-04 - 205ms/epoch - 68ms/step\n",
            "Epoch 234/300\n",
            "3/3 - 0s - loss: 1.7512 - mae: 243.0352 - mse: 436747.8438 - val_loss: 1.9261 - val_mae: 658.1483 - val_mse: 1129616.7500 - lr: 1.0000e-04 - 197ms/epoch - 66ms/step\n",
            "Epoch 235/300\n",
            "3/3 - 0s - loss: 1.7513 - mae: 242.6600 - mse: 435981.5625 - val_loss: 1.9325 - val_mae: 659.3690 - val_mse: 1130192.5000 - lr: 1.0000e-04 - 198ms/epoch - 66ms/step\n",
            "Epoch 236/300\n",
            "3/3 - 0s - loss: 1.7543 - mae: 244.2354 - mse: 436885.9688 - val_loss: 1.9273 - val_mae: 659.1048 - val_mse: 1135588.7500 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 237/300\n",
            "3/3 - 0s - loss: 1.7580 - mae: 243.7396 - mse: 436529.6250 - val_loss: 1.9336 - val_mae: 659.3289 - val_mse: 1126082.3750 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 238/300\n",
            "3/3 - 0s - loss: 1.7546 - mae: 245.2265 - mse: 438929.1562 - val_loss: 1.9308 - val_mae: 659.5970 - val_mse: 1136865.2500 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 239/300\n",
            "3/3 - 0s - loss: 1.7494 - mae: 242.9155 - mse: 434947.3438 - val_loss: 1.9334 - val_mae: 659.9047 - val_mse: 1129343.6250 - lr: 1.0000e-04 - 214ms/epoch - 71ms/step\n",
            "Epoch 240/300\n",
            "3/3 - 0s - loss: 1.7451 - mae: 243.2903 - mse: 435672.0938 - val_loss: 1.9370 - val_mae: 661.7244 - val_mse: 1142303.2500 - lr: 1.0000e-04 - 218ms/epoch - 73ms/step\n",
            "Epoch 241/300\n",
            "3/3 - 0s - loss: 1.7444 - mae: 241.6766 - mse: 433916.2812 - val_loss: 1.9295 - val_mae: 658.3071 - val_mse: 1121537.8750 - lr: 1.0000e-04 - 214ms/epoch - 71ms/step\n",
            "Epoch 242/300\n",
            "3/3 - 0s - loss: 1.7485 - mae: 241.6617 - mse: 432592.0625 - val_loss: 1.9379 - val_mae: 662.2471 - val_mse: 1142575.5000 - lr: 1.0000e-04 - 201ms/epoch - 67ms/step\n",
            "Epoch 243/300\n",
            "3/3 - 0s - loss: 1.7488 - mae: 241.4039 - mse: 431829.4375 - val_loss: 1.9347 - val_mae: 659.5079 - val_mse: 1122835.5000 - lr: 1.0000e-04 - 207ms/epoch - 69ms/step\n",
            "Epoch 244/300\n",
            "3/3 - 0s - loss: 1.7479 - mae: 241.8945 - mse: 432696.9375 - val_loss: 1.9323 - val_mae: 660.5911 - val_mse: 1139510.2500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 245/300\n",
            "3/3 - 0s - loss: 1.7407 - mae: 241.2899 - mse: 431242.0000 - val_loss: 1.9395 - val_mae: 661.5433 - val_mse: 1126704.6250 - lr: 1.0000e-04 - 211ms/epoch - 70ms/step\n",
            "Epoch 246/300\n",
            "3/3 - 0s - loss: 1.7334 - mae: 240.2490 - mse: 431016.8750 - val_loss: 1.9336 - val_mae: 660.5739 - val_mse: 1136842.2500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 247/300\n",
            "3/3 - 0s - loss: 1.7277 - mae: 240.1060 - mse: 431121.8125 - val_loss: 1.9397 - val_mae: 661.9562 - val_mse: 1134605.5000 - lr: 1.0000e-04 - 202ms/epoch - 67ms/step\n",
            "Epoch 248/300\n",
            "3/3 - 0s - loss: 1.7239 - mae: 239.0425 - mse: 428796.1250 - val_loss: 1.9382 - val_mae: 661.1730 - val_mse: 1134440.7500 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 249/300\n",
            "3/3 - 0s - loss: 1.7215 - mae: 238.4474 - mse: 428333.2812 - val_loss: 1.9342 - val_mae: 660.3665 - val_mse: 1135683.2500 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 250/300\n",
            "3/3 - 0s - loss: 1.7231 - mae: 238.7698 - mse: 428882.3125 - val_loss: 1.9372 - val_mae: 660.7491 - val_mse: 1129119.3750 - lr: 1.0000e-04 - 193ms/epoch - 64ms/step\n",
            "Epoch 251/300\n",
            "3/3 - 0s - loss: 1.7182 - mae: 238.2673 - mse: 426863.5000 - val_loss: 1.9361 - val_mae: 661.3047 - val_mse: 1140110.1250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 252/300\n",
            "3/3 - 0s - loss: 1.7137 - mae: 237.2579 - mse: 426937.8750 - val_loss: 1.9369 - val_mae: 661.0676 - val_mse: 1132057.1250 - lr: 1.0000e-04 - 195ms/epoch - 65ms/step\n",
            "Epoch 253/300\n",
            "3/3 - 0s - loss: 1.7131 - mae: 236.9665 - mse: 426409.1562 - val_loss: 1.9369 - val_mae: 660.5477 - val_mse: 1129333.1250 - lr: 1.0000e-04 - 199ms/epoch - 66ms/step\n",
            "Epoch 254/300\n",
            "3/3 - 0s - loss: 1.7177 - mae: 237.6558 - mse: 426091.2500 - val_loss: 1.9376 - val_mae: 662.1179 - val_mse: 1144505.0000 - lr: 1.0000e-04 - 200ms/epoch - 67ms/step\n",
            "Epoch 255/300\n",
            "3/3 - 0s - loss: 1.7168 - mae: 237.2872 - mse: 425744.9062 - val_loss: 1.9389 - val_mae: 661.0591 - val_mse: 1127721.8750 - lr: 1.0000e-04 - 194ms/epoch - 65ms/step\n",
            "Epoch 256/300\n",
            "3/3 - 0s - loss: 1.7101 - mae: 236.3583 - mse: 424156.1250 - val_loss: 1.9363 - val_mae: 661.1757 - val_mse: 1138876.0000 - lr: 1.0000e-04 - 245ms/epoch - 82ms/step\n",
            "Epoch 257/300\n",
            "3/3 - 0s - loss: 1.7068 - mae: 235.9529 - mse: 423541.1562 - val_loss: 1.9391 - val_mae: 662.2984 - val_mse: 1139171.5000 - lr: 1.0000e-04 - 264ms/epoch - 88ms/step\n",
            "Epoch 258/300\n",
            "3/3 - 0s - loss: 1.7078 - mae: 235.7473 - mse: 423283.0312 - val_loss: 1.9426 - val_mae: 662.0842 - val_mse: 1128394.6250 - lr: 1.0000e-04 - 259ms/epoch - 86ms/step\n",
            "Epoch 259/300\n",
            "3/3 - 0s - loss: 1.7078 - mae: 236.1042 - mse: 422190.3125 - val_loss: 1.9351 - val_mae: 661.0506 - val_mse: 1139338.8750 - lr: 1.0000e-04 - 245ms/epoch - 82ms/step\n",
            "Epoch 260/300\n",
            "3/3 - 0s - loss: 1.7063 - mae: 235.5715 - mse: 422491.9375 - val_loss: 1.9398 - val_mae: 661.8925 - val_mse: 1134651.5000 - lr: 1.0000e-04 - 333ms/epoch - 111ms/step\n",
            "Epoch 261/300\n",
            "3/3 - 0s - loss: 1.7031 - mae: 235.3988 - mse: 421019.5625 - val_loss: 1.9427 - val_mae: 662.0915 - val_mse: 1128560.8750 - lr: 1.0000e-04 - 287ms/epoch - 96ms/step\n",
            "Epoch 262/300\n",
            "3/3 - 0s - loss: 1.6992 - mae: 234.7387 - mse: 420748.6562 - val_loss: 1.9395 - val_mae: 662.5657 - val_mse: 1146121.6250 - lr: 1.0000e-04 - 273ms/epoch - 91ms/step\n",
            "Epoch 263/300\n",
            "3/3 - 0s - loss: 1.7012 - mae: 235.4000 - mse: 421827.2812 - val_loss: 1.9437 - val_mae: 662.1309 - val_mse: 1125917.8750 - lr: 1.0000e-04 - 300ms/epoch - 100ms/step\n",
            "Epoch 264/300\n",
            "3/3 - 0s - loss: 1.7002 - mae: 234.9411 - mse: 419340.4375 - val_loss: 1.9431 - val_mae: 664.1065 - val_mse: 1152676.7500 - lr: 1.0000e-04 - 236ms/epoch - 79ms/step\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 265: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 1.7040 - mae: 235.7465 - mse: 420737.0000 - val_loss: 1.9419 - val_mae: 662.8615 - val_mse: 1139521.7500 - lr: 1.0000e-04 - 374ms/epoch - 125ms/step\n",
            "Optimizing model by reducing: mae for epochs: 300, num_iter: 2, model: LSTM_model\n",
            "Epoch 1/300\n",
            "3/3 - 2s - loss: 418.4691 - mae: 418.4691 - mse: 750919.6250 - val_loss: 887.1609 - val_mae: 887.1609 - val_mse: 1636137.1250 - lr: 0.0010 - 2s/epoch - 791ms/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 700.5757 - mae: 700.5757 - mse: 1858787.0000 - val_loss: 1457.8999 - val_mae: 1457.8999 - val_mse: 3394584.2500 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 424.1382 - mae: 424.1382 - mse: 758282.9375 - val_loss: 613.9655 - val_mae: 613.9655 - val_mse: 1023820.1250 - lr: 0.0010 - 59ms/epoch - 20ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 489.1116 - mae: 489.1116 - mse: 1108518.2500 - val_loss: 1133.1493 - val_mae: 1133.1493 - val_mse: 2195350.0000 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 494.0616 - mae: 494.0617 - mse: 1053515.3750 - val_loss: 833.2067 - val_mae: 833.2067 - val_mse: 1407746.0000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 361.5116 - mae: 361.5116 - mse: 663354.8125 - val_loss: 660.6962 - val_mae: 660.6962 - val_mse: 1150680.5000 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 427.8567 - mae: 427.8568 - mse: 831744.0625 - val_loss: 878.5920 - val_mae: 878.5920 - val_mse: 1615320.2500 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 402.5864 - mae: 402.5864 - mse: 741095.3125 - val_loss: 645.6425 - val_mae: 645.6425 - val_mse: 1121312.5000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 363.6601 - mae: 363.6601 - mse: 684886.1875 - val_loss: 678.7802 - val_mae: 678.7802 - val_mse: 1105491.3750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 390.7906 - mae: 390.7906 - mse: 754361.8750 - val_loss: 680.2742 - val_mae: 680.2742 - val_mse: 1107934.7500 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 359.0127 - mae: 359.0127 - mse: 665701.9375 - val_loss: 608.2067 - val_mae: 608.2067 - val_mse: 1042663.4375 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 372.7048 - mae: 372.7048 - mse: 687760.3125 - val_loss: 661.1396 - val_mae: 661.1396 - val_mse: 1151430.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 364.8050 - mae: 364.8050 - mse: 667792.8750 - val_loss: 605.2732 - val_mae: 605.2732 - val_mse: 1031336.0000 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 362.9184 - mae: 362.9184 - mse: 683125.1875 - val_loss: 639.9650 - val_mae: 639.9650 - val_mse: 1048731.0000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 362.2145 - mae: 362.2145 - mse: 677930.6250 - val_loss: 609.9816 - val_mae: 609.9816 - val_mse: 1021932.2500 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 358.9554 - mae: 358.9554 - mse: 662152.1250 - val_loss: 614.7313 - val_mae: 614.7313 - val_mse: 1059436.8750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 359.3230 - mae: 359.3230 - mse: 661639.0625 - val_loss: 605.1661 - val_mae: 605.1661 - val_mse: 1030408.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 358.0310 - mae: 358.0310 - mse: 667603.2500 - val_loss: 616.1102 - val_mae: 616.1102 - val_mse: 1024812.7500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 357.9368 - mae: 357.9368 - mse: 666450.5625 - val_loss: 606.6706 - val_mae: 606.6706 - val_mse: 1022632.5000 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 357.2747 - mae: 357.2747 - mse: 660051.1875 - val_loss: 606.7728 - val_mae: 606.7728 - val_mse: 1037951.1875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 356.7765 - mae: 356.7765 - mse: 659696.9375 - val_loss: 605.2367 - val_mae: 605.2367 - val_mse: 1025021.3750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 357.0830 - mae: 357.0830 - mse: 664003.5625 - val_loss: 608.8886 - val_mae: 608.8886 - val_mse: 1021940.1875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 356.5483 - mae: 356.5483 - mse: 661437.2500 - val_loss: 605.0969 - val_mae: 605.0969 - val_mse: 1025968.7500 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 356.7903 - mae: 356.7903 - mse: 659643.4375 - val_loss: 605.1260 - val_mae: 605.1260 - val_mse: 1029572.3125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 356.4882 - mae: 356.4882 - mse: 660923.1875 - val_loss: 606.6703 - val_mae: 606.6703 - val_mse: 1022809.7500 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 356.6888 - mae: 356.6888 - mse: 662173.2500 - val_loss: 605.9432 - val_mae: 605.9432 - val_mse: 1023653.1875 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 356.4141 - mae: 356.4141 - mse: 659758.5625 - val_loss: 605.2204 - val_mae: 605.2204 - val_mse: 1030048.1250 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 356.6086 - mae: 356.6086 - mse: 659561.0625 - val_loss: 605.0897 - val_mae: 605.0897 - val_mse: 1026789.1250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 356.5708 - mae: 356.5708 - mse: 661640.9375 - val_loss: 606.3839 - val_mae: 606.3839 - val_mse: 1023214.7500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 356.3864 - mae: 356.3864 - mse: 660327.3750 - val_loss: 605.1289 - val_mae: 605.1289 - val_mse: 1026816.2500 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 356.4674 - mae: 356.4674 - mse: 659581.2500 - val_loss: 605.1036 - val_mae: 605.1036 - val_mse: 1027878.4375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 356.4102 - mae: 356.4102 - mse: 660286.3750 - val_loss: 605.6082 - val_mae: 605.6082 - val_mse: 1024625.3125 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 356.4055 - mae: 356.4055 - mse: 660598.1250 - val_loss: 605.5931 - val_mae: 605.5931 - val_mse: 1024742.0625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 356.3634 - mae: 356.3634 - mse: 660056.8750 - val_loss: 605.2250 - val_mae: 605.2250 - val_mse: 1026715.1875 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 356.3948 - mae: 356.3948 - mse: 659673.4375 - val_loss: 605.2006 - val_mae: 605.2006 - val_mse: 1027405.0625 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 356.3778 - mae: 356.3778 - mse: 659895.9375 - val_loss: 605.5021 - val_mae: 605.5021 - val_mse: 1025306.2500 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 356.3994 - mae: 356.3994 - mse: 660611.1250 - val_loss: 605.7327 - val_mae: 605.7327 - val_mse: 1024686.7500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 356.3387 - mae: 356.3387 - mse: 659932.9375 - val_loss: 605.2905 - val_mae: 605.2905 - val_mse: 1027096.3125 - lr: 0.0010 - 43ms/epoch - 14ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 356.3796 - mae: 356.3796 - mse: 659488.1875 - val_loss: 605.2780 - val_mae: 605.2780 - val_mse: 1027863.1250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 356.3558 - mae: 356.3558 - mse: 659816.4375 - val_loss: 605.5668 - val_mae: 605.5668 - val_mse: 1025486.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 356.3659 - mae: 356.3659 - mse: 660361.6875 - val_loss: 605.7871 - val_mae: 605.7871 - val_mse: 1024822.6250 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 356.3255 - mae: 356.3255 - mse: 659950.5000 - val_loss: 605.3776 - val_mae: 605.3776 - val_mse: 1026995.3750 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 356.3740 - mae: 356.3740 - mse: 659322.1875 - val_loss: 605.4001 - val_mae: 605.4001 - val_mse: 1026933.3750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 356.3772 - mae: 356.3772 - mse: 660448.6250 - val_loss: 606.1301 - val_mae: 606.1301 - val_mse: 1024255.4375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 356.3292 - mae: 356.3292 - mse: 660095.8125 - val_loss: 605.4888 - val_mae: 605.4888 - val_mse: 1026443.6250 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 356.3469 - mae: 356.3469 - mse: 659259.9375 - val_loss: 605.4435 - val_mae: 605.4435 - val_mse: 1027006.5625 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 356.3539 - mae: 356.3539 - mse: 660280.8125 - val_loss: 606.0297 - val_mae: 606.0297 - val_mse: 1024529.0625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 356.3076 - mae: 356.3076 - mse: 659956.9375 - val_loss: 605.5306 - val_mae: 605.5306 - val_mse: 1026538.8125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 356.3214 - mae: 356.3214 - mse: 659200.0625 - val_loss: 605.4974 - val_mae: 605.4974 - val_mse: 1027047.5625 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 356.3276 - mae: 356.3276 - mse: 660108.1875 - val_loss: 605.9229 - val_mae: 605.9229 - val_mse: 1024897.0000 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 356.2785 - mae: 356.2785 - mse: 659736.1250 - val_loss: 605.5412 - val_mae: 605.5412 - val_mse: 1027044.5625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 356.3183 - mae: 356.3183 - mse: 659084.5000 - val_loss: 605.6057 - val_mae: 605.6057 - val_mse: 1026630.0625 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 356.3533 - mae: 356.3533 - mse: 660449.5000 - val_loss: 606.3528 - val_mae: 606.3528 - val_mse: 1024225.6875 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 356.2616 - mae: 356.2616 - mse: 659693.9375 - val_loss: 605.5687 - val_mae: 605.5687 - val_mse: 1027590.5625 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 356.3727 - mae: 356.3727 - mse: 658893.6875 - val_loss: 605.6718 - val_mae: 605.6718 - val_mse: 1026596.3750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 356.4705 - mae: 356.4705 - mse: 661036.9375 - val_loss: 606.2514 - val_mae: 606.2514 - val_mse: 1024514.8125 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 356.2433 - mae: 356.2433 - mse: 659013.1875 - val_loss: 605.6121 - val_mae: 605.6121 - val_mse: 1028458.3125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 356.2543 - mae: 356.2543 - mse: 659235.8750 - val_loss: 605.8106 - val_mae: 605.8106 - val_mse: 1025997.7500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 356.2727 - mae: 356.2727 - mse: 659876.5625 - val_loss: 605.9019 - val_mae: 605.9019 - val_mse: 1025629.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 356.2128 - mae: 356.2128 - mse: 659168.8125 - val_loss: 605.7224 - val_mae: 605.7224 - val_mse: 1026914.3125 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 356.2361 - mae: 356.2361 - mse: 659396.5625 - val_loss: 605.7877 - val_mae: 605.7877 - val_mse: 1026513.0000 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 356.2098 - mae: 356.2098 - mse: 659154.1250 - val_loss: 605.6886 - val_mae: 605.6886 - val_mse: 1028145.8125 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 356.2679 - mae: 356.2679 - mse: 658810.3750 - val_loss: 605.9680 - val_mae: 605.9680 - val_mse: 1025662.3750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 356.4891 - mae: 356.4891 - mse: 661069.0625 - val_loss: 606.0844 - val_mae: 606.0844 - val_mse: 1025245.6875 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 356.4036 - mae: 356.4036 - mse: 658604.4375 - val_loss: 605.7808 - val_mae: 605.7808 - val_mse: 1027148.2500 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 356.5110 - mae: 356.5110 - mse: 661174.8125 - val_loss: 606.4731 - val_mae: 606.4731 - val_mse: 1024466.1250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 356.2242 - mae: 356.2242 - mse: 658689.9375 - val_loss: 605.7565 - val_mae: 605.7565 - val_mse: 1028243.5000 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 356.2267 - mae: 356.2267 - mse: 659449.1875 - val_loss: 606.0291 - val_mae: 606.0291 - val_mse: 1025709.6875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 356.1754 - mae: 356.1754 - mse: 659165.3125 - val_loss: 605.7863 - val_mae: 605.7863 - val_mse: 1027861.6875 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 356.2485 - mae: 356.2485 - mse: 658616.9375 - val_loss: 605.9722 - val_mae: 605.9722 - val_mse: 1026151.8125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 356.4098 - mae: 356.4098 - mse: 660686.1875 - val_loss: 606.0219 - val_mae: 606.0219 - val_mse: 1025949.0000 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 356.4602 - mae: 356.4602 - mse: 658449.0625 - val_loss: 605.9688 - val_mae: 605.9688 - val_mse: 1026344.6250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 356.8806 - mae: 356.8806 - mse: 662723.1250 - val_loss: 606.8509 - val_mae: 606.8509 - val_mse: 1024095.7500 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 356.4571 - mae: 356.4571 - mse: 658430.9375 - val_loss: 605.8391 - val_mae: 605.8391 - val_mse: 1028010.4375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 356.4769 - mae: 356.4769 - mse: 661000.8125 - val_loss: 606.3050 - val_mae: 606.3050 - val_mse: 1025072.7500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 356.3056 - mae: 356.3056 - mse: 658400.9375 - val_loss: 605.9023 - val_mae: 605.9023 - val_mse: 1027214.5625 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 356.4056 - mae: 356.4056 - mse: 660673.8750 - val_loss: 606.1713 - val_mae: 606.1713 - val_mse: 1025555.8750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 356.3434 - mae: 356.3434 - mse: 658357.5625 - val_loss: 605.9341 - val_mae: 605.9341 - val_mse: 1027022.6875 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 356.5162 - mae: 356.5162 - mse: 661182.9375 - val_loss: 606.3434 - val_mae: 606.3434 - val_mse: 1025074.1875 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 356.3519 - mae: 356.3519 - mse: 658327.5625 - val_loss: 605.9541 - val_mae: 605.9541 - val_mse: 1026958.8750 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 356.5665 - mae: 356.5665 - mse: 661409.8750 - val_loss: 606.3563 - val_mae: 606.3563 - val_mse: 1025088.7500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 356.4204 - mae: 356.4204 - mse: 658301.8125 - val_loss: 606.0137 - val_mae: 606.0137 - val_mse: 1026594.3750 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 356.8161 - mae: 356.8161 - mse: 662468.6250 - val_loss: 606.7366 - val_mae: 606.7366 - val_mse: 1024432.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 356.4905 - mae: 356.4905 - mse: 658302.1250 - val_loss: 605.9465 - val_mae: 605.9465 - val_mse: 1027366.0000 - lr: 0.0010 - 55ms/epoch - 18ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 356.7285 - mae: 356.7285 - mse: 662104.1875 - val_loss: 606.6175 - val_mae: 606.6175 - val_mse: 1024656.6250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 356.4727 - mae: 356.4727 - mse: 658266.5625 - val_loss: 605.9953 - val_mae: 605.9953 - val_mse: 1026965.3125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 356.8254 - mae: 356.8254 - mse: 662516.8125 - val_loss: 606.5875 - val_mae: 606.5875 - val_mse: 1024746.0625 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 356.6600 - mae: 356.6600 - mse: 658342.7500 - val_loss: 606.1230 - val_mae: 606.1230 - val_mse: 1026181.9375 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 357.5494 - mae: 357.5494 - mse: 665237.6250 - val_loss: 606.3876 - val_mae: 606.3876 - val_mse: 1025203.0625 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 358.2787 - mae: 358.2787 - mse: 660038.1250 - val_loss: 606.5524 - val_mae: 606.5524 - val_mse: 1024865.3125 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 362.0380 - mae: 362.0380 - mse: 679331.1875 - val_loss: 606.7249 - val_mae: 606.7249 - val_mse: 1024594.1250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 366.5207 - mae: 366.5207 - mse: 674381.6250 - val_loss: 607.6577 - val_mae: 607.6577 - val_mse: 1038255.3125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 364.6821 - mae: 364.6821 - mse: 687645.1875 - val_loss: 609.8000 - val_mae: 609.8000 - val_mse: 1023112.5000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 364.8531 - mae: 364.8531 - mse: 671255.3125 - val_loss: 607.2886 - val_mae: 607.2886 - val_mse: 1036915.8750 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 363.3859 - mae: 363.3859 - mse: 683870.2500 - val_loss: 610.1462 - val_mae: 610.1462 - val_mse: 1023160.1250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 362.5403 - mae: 362.5403 - mse: 666870.6875 - val_loss: 606.6798 - val_mae: 606.6798 - val_mse: 1033912.4375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 362.0116 - mae: 362.0116 - mse: 679759.6250 - val_loss: 608.9028 - val_mae: 608.9028 - val_mse: 1023178.9375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 362.2455 - mae: 362.2455 - mse: 666239.0625 - val_loss: 606.2613 - val_mae: 606.2613 - val_mse: 1031351.5000 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 363.0932 - mae: 363.0932 - mse: 682838.3125 - val_loss: 608.5197 - val_mae: 608.5197 - val_mse: 1023285.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 364.5626 - mae: 364.5626 - mse: 670532.1250 - val_loss: 606.8796 - val_mae: 606.8796 - val_mse: 1034958.1875 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 364.1525 - mae: 364.1525 - mse: 686039.5000 - val_loss: 609.9959 - val_mae: 609.9959 - val_mse: 1023189.1875 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 363.8328 - mae: 363.8328 - mse: 669239.0625 - val_loss: 606.9841 - val_mae: 606.9841 - val_mse: 1035441.1250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 362.9268 - mae: 362.9268 - mse: 682504.5625 - val_loss: 610.0553 - val_mae: 610.0553 - val_mse: 1023209.6250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 362.0247 - mae: 362.0247 - mse: 665882.6875 - val_loss: 606.4960 - val_mae: 606.4960 - val_mse: 1032698.7500 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 362.0344 - mae: 362.0344 - mse: 679794.4375 - val_loss: 609.2899 - val_mae: 609.2899 - val_mse: 1023197.8125 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 361.7019 - mae: 361.7019 - mse: 665269.7500 - val_loss: 606.1700 - val_mae: 606.1700 - val_mse: 1030660.6875 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 362.8660 - mae: 362.8660 - mse: 682155.7500 - val_loss: 609.0083 - val_mae: 609.0083 - val_mse: 1023235.0625 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 363.3511 - mae: 363.3511 - mse: 668229.5625 - val_loss: 606.8287 - val_mae: 606.8287 - val_mse: 1034605.3750 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 362.7150 - mae: 362.7150 - mse: 681872.8125 - val_loss: 609.9612 - val_mae: 609.9612 - val_mse: 1023232.7500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 361.8486 - mae: 361.8486 - mse: 665545.4375 - val_loss: 606.4644 - val_mae: 606.4644 - val_mse: 1032506.3125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 361.9385 - mae: 361.9385 - mse: 679513.0000 - val_loss: 610.3168 - val_mae: 610.3168 - val_mse: 1023283.8125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 360.3252 - mae: 360.3252 - mse: 663027.4375 - val_loss: 606.1740 - val_mae: 606.1740 - val_mse: 1030702.1875 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 360.9690 - mae: 360.9690 - mse: 676477.3125 - val_loss: 609.3594 - val_mae: 609.3594 - val_mse: 1023232.7500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 359.9711 - mae: 359.9711 - mse: 662421.6250 - val_loss: 606.0234 - val_mae: 606.0234 - val_mse: 1028966.8125 - lr: 0.0010 - 57ms/epoch - 19ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 361.4940 - mae: 361.4940 - mse: 677991.2500 - val_loss: 609.1964 - val_mae: 609.1964 - val_mse: 1023251.5625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 360.9121 - mae: 360.9121 - mse: 663895.5000 - val_loss: 606.1821 - val_mae: 606.1821 - val_mse: 1030682.8750 - lr: 0.0010 - 63ms/epoch - 21ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 361.7406 - mae: 361.7406 - mse: 678833.6250 - val_loss: 610.0286 - val_mae: 610.0286 - val_mse: 1023275.7500 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 360.2915 - mae: 360.2915 - mse: 662930.8125 - val_loss: 606.1782 - val_mae: 606.1782 - val_mse: 1030593.1875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 361.0400 - mae: 361.0400 - mse: 676681.4375 - val_loss: 609.6119 - val_mae: 609.6119 - val_mse: 1023260.0625 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 359.8073 - mae: 359.8073 - mse: 662157.1875 - val_loss: 606.0613 - val_mae: 606.0613 - val_mse: 1029440.6250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 361.0748 - mae: 361.0748 - mse: 676739.0000 - val_loss: 609.2988 - val_mae: 609.2988 - val_mse: 1023274.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 360.1165 - mae: 360.1165 - mse: 662595.5625 - val_loss: 606.0775 - val_mae: 606.0775 - val_mse: 1029633.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 361.3439 - mae: 361.3439 - mse: 677564.2500 - val_loss: 609.2599 - val_mae: 609.2599 - val_mse: 1023284.8750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 360.6098 - mae: 360.6098 - mse: 663370.3750 - val_loss: 606.1089 - val_mae: 606.1089 - val_mse: 1029911.5625 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 361.8462 - mae: 361.8462 - mse: 679083.6875 - val_loss: 609.3059 - val_mae: 609.3059 - val_mse: 1023293.6250 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 361.3850 - mae: 361.3850 - mse: 664634.2500 - val_loss: 606.2195 - val_mae: 606.2195 - val_mse: 1030618.4375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 362.5529 - mae: 362.5529 - mse: 681221.2500 - val_loss: 609.2363 - val_mae: 609.2363 - val_mse: 1023311.8750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 128: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 362.5517 - mae: 362.5517 - mse: 666660.6250 - val_loss: 607.1282 - val_mae: 607.1282 - val_mse: 1035814.8125 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 356.0920 - mae: 356.0920 - mse: 657731.0625 - val_loss: 606.0937 - val_mae: 606.0937 - val_mse: 1027583.5625 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 356.0383 - mae: 356.0383 - mse: 658980.9375 - val_loss: 607.4927 - val_mae: 607.4927 - val_mse: 1024017.0625 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 356.2980 - mae: 356.2980 - mse: 660237.3750 - val_loss: 608.5786 - val_mae: 608.5786 - val_mse: 1023436.6250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 356.3036 - mae: 356.3036 - mse: 660217.3750 - val_loss: 608.0071 - val_mae: 608.0071 - val_mse: 1023670.0000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 356.0801 - mae: 356.0801 - mse: 659229.0000 - val_loss: 606.7311 - val_mae: 606.7311 - val_mse: 1024941.9375 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 355.9205 - mae: 355.9205 - mse: 658307.5625 - val_loss: 606.1038 - val_mae: 606.1038 - val_mse: 1027476.5000 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 355.9581 - mae: 355.9581 - mse: 657791.0625 - val_loss: 606.1408 - val_mae: 606.1408 - val_mse: 1030006.0625 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 356.0503 - mae: 356.0503 - mse: 657691.6250 - val_loss: 606.1649 - val_mae: 606.1649 - val_mse: 1030190.1875 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 355.9783 - mae: 355.9783 - mse: 657813.3125 - val_loss: 606.0521 - val_mae: 606.0521 - val_mse: 1028303.0000 - lr: 1.0000e-04 - 58ms/epoch - 19ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 355.9329 - mae: 355.9329 - mse: 658157.5000 - val_loss: 606.2188 - val_mae: 606.2188 - val_mse: 1026500.6250 - lr: 1.0000e-04 - 61ms/epoch - 20ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 355.9525 - mae: 355.9525 - mse: 658463.7500 - val_loss: 606.3983 - val_mae: 606.3983 - val_mse: 1025751.3750 - lr: 1.0000e-04 - 72ms/epoch - 24ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 355.9555 - mae: 355.9555 - mse: 658522.8750 - val_loss: 606.3759 - val_mae: 606.3759 - val_mse: 1025831.0625 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 355.9334 - mae: 355.9334 - mse: 658365.1875 - val_loss: 606.2168 - val_mae: 606.2168 - val_mse: 1026528.1250 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 355.9120 - mae: 355.9120 - mse: 658107.7500 - val_loss: 606.1049 - val_mae: 606.1049 - val_mse: 1027532.5625 - lr: 1.0000e-04 - 71ms/epoch - 24ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 355.9209 - mae: 355.9209 - mse: 657949.4375 - val_loss: 606.0572 - val_mae: 606.0572 - val_mse: 1028183.5000 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 355.9296 - mae: 355.9296 - mse: 657912.8750 - val_loss: 606.0580 - val_mae: 606.0580 - val_mse: 1028202.0625 - lr: 1.0000e-04 - 74ms/epoch - 25ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 355.9258 - mae: 355.9258 - mse: 657956.5625 - val_loss: 606.0828 - val_mae: 606.0828 - val_mse: 1027831.8750 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 355.9214 - mae: 355.9214 - mse: 658033.4375 - val_loss: 606.1198 - val_mae: 606.1198 - val_mse: 1027397.5000 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 355.9181 - mae: 355.9181 - mse: 658105.4375 - val_loss: 606.1508 - val_mae: 606.1508 - val_mse: 1027052.7500 - lr: 1.0000e-04 - 60ms/epoch - 20ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 355.9167 - mae: 355.9167 - mse: 658162.7500 - val_loss: 606.1637 - val_mae: 606.1637 - val_mse: 1026937.4375 - lr: 1.0000e-04 - 63ms/epoch - 21ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 355.9132 - mae: 355.9132 - mse: 658132.2500 - val_loss: 606.1505 - val_mae: 606.1505 - val_mse: 1027071.7500 - lr: 1.0000e-04 - 76ms/epoch - 25ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 355.9132 - mae: 355.9132 - mse: 658104.8750 - val_loss: 606.1411 - val_mae: 606.1411 - val_mse: 1027181.2500 - lr: 1.0000e-04 - 72ms/epoch - 24ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 355.9134 - mae: 355.9134 - mse: 658086.8750 - val_loss: 606.1368 - val_mae: 606.1368 - val_mse: 1027241.6875 - lr: 1.0000e-04 - 70ms/epoch - 23ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 355.9133 - mae: 355.9133 - mse: 658079.8750 - val_loss: 606.1364 - val_mae: 606.1364 - val_mse: 1027252.2500 - lr: 1.0000e-04 - 58ms/epoch - 19ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 355.9130 - mae: 355.9130 - mse: 658081.0625 - val_loss: 606.1392 - val_mae: 606.1392 - val_mse: 1027227.5625 - lr: 1.0000e-04 - 76ms/epoch - 25ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 355.9124 - mae: 355.9124 - mse: 658088.3750 - val_loss: 606.1443 - val_mae: 606.1443 - val_mse: 1027176.6250 - lr: 1.0000e-04 - 58ms/epoch - 19ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 355.9116 - mae: 355.9116 - mse: 658100.0000 - val_loss: 606.1511 - val_mae: 606.1511 - val_mse: 1027107.3750 - lr: 1.0000e-04 - 72ms/epoch - 24ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 355.9107 - mae: 355.9107 - mse: 658112.5000 - val_loss: 606.1569 - val_mae: 606.1569 - val_mse: 1027050.6250 - lr: 1.0000e-04 - 66ms/epoch - 22ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 355.9099 - mae: 355.9099 - mse: 658120.3750 - val_loss: 606.1536 - val_mae: 606.1536 - val_mse: 1027094.6875 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 355.9087 - mae: 355.9087 - mse: 658079.9375 - val_loss: 606.1394 - val_mae: 606.1394 - val_mse: 1027260.0000 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 355.9092 - mae: 355.9092 - mse: 658053.9375 - val_loss: 606.1319 - val_mae: 606.1319 - val_mse: 1027355.5625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 355.9094 - mae: 355.9094 - mse: 658041.8125 - val_loss: 606.1295 - val_mae: 606.1295 - val_mse: 1027392.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 355.9091 - mae: 355.9091 - mse: 658039.0000 - val_loss: 606.1307 - val_mae: 606.1307 - val_mse: 1027384.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 355.9086 - mae: 355.9086 - mse: 658043.3750 - val_loss: 606.1346 - val_mae: 606.1346 - val_mse: 1027343.8750 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 355.9079 - mae: 355.9079 - mse: 658052.8125 - val_loss: 606.1408 - val_mae: 606.1408 - val_mse: 1027280.8750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 355.9071 - mae: 355.9071 - mse: 658066.0000 - val_loss: 606.1483 - val_mae: 606.1483 - val_mse: 1027201.8125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 355.9061 - mae: 355.9061 - mse: 658081.1250 - val_loss: 606.1557 - val_mae: 606.1557 - val_mse: 1027125.2500 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 355.9052 - mae: 355.9052 - mse: 658094.4375 - val_loss: 606.1623 - val_mae: 606.1623 - val_mse: 1027057.7500 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 355.9048 - mae: 355.9048 - mse: 658105.3125 - val_loss: 606.1597 - val_mae: 606.1597 - val_mse: 1027094.5625 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 355.9030 - mae: 355.9030 - mse: 658064.1250 - val_loss: 606.1454 - val_mae: 606.1454 - val_mse: 1027262.9375 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 355.9035 - mae: 355.9035 - mse: 658037.6250 - val_loss: 606.1375 - val_mae: 606.1375 - val_mse: 1027361.3125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 355.9037 - mae: 355.9037 - mse: 658024.5625 - val_loss: 606.1346 - val_mae: 606.1346 - val_mse: 1027401.5625 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 355.9034 - mae: 355.9034 - mse: 658021.0000 - val_loss: 606.1356 - val_mae: 606.1356 - val_mse: 1027397.9375 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 355.9029 - mae: 355.9029 - mse: 658024.4375 - val_loss: 606.1392 - val_mae: 606.1392 - val_mse: 1027362.1250 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 355.9023 - mae: 355.9023 - mse: 658032.9375 - val_loss: 606.1451 - val_mae: 606.1451 - val_mse: 1027304.2500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 355.9015 - mae: 355.9015 - mse: 658045.0625 - val_loss: 606.1521 - val_mae: 606.1521 - val_mse: 1027230.8125 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 355.9005 - mae: 355.9005 - mse: 658058.4375 - val_loss: 606.1588 - val_mae: 606.1588 - val_mse: 1027161.6250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 355.8997 - mae: 355.8997 - mse: 658069.8750 - val_loss: 606.1646 - val_mae: 606.1646 - val_mse: 1027103.6250 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 355.8993 - mae: 355.8993 - mse: 658079.5625 - val_loss: 606.1621 - val_mae: 606.1621 - val_mse: 1027141.5000 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 355.8976 - mae: 355.8976 - mse: 658038.4375 - val_loss: 606.1477 - val_mae: 606.1477 - val_mse: 1027315.1875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 355.8982 - mae: 355.8982 - mse: 658010.8750 - val_loss: 606.1394 - val_mae: 606.1394 - val_mse: 1027420.8750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 355.8983 - mae: 355.8983 - mse: 657996.3750 - val_loss: 606.1359 - val_mae: 606.1359 - val_mse: 1027469.7500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 355.8981 - mae: 355.8981 - mse: 657991.0000 - val_loss: 606.1362 - val_mae: 606.1362 - val_mse: 1027475.4375 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 355.8978 - mae: 355.8978 - mse: 657992.5000 - val_loss: 606.1391 - val_mae: 606.1391 - val_mse: 1027450.0625 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 355.8972 - mae: 355.8972 - mse: 657998.5625 - val_loss: 606.1441 - val_mae: 606.1441 - val_mse: 1027402.5625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 355.8965 - mae: 355.8965 - mse: 658008.5000 - val_loss: 606.1503 - val_mae: 606.1503 - val_mse: 1027338.6250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 355.8957 - mae: 355.8957 - mse: 658021.1875 - val_loss: 606.1577 - val_mae: 606.1577 - val_mse: 1027263.2500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 355.8947 - mae: 355.8947 - mse: 658034.5000 - val_loss: 606.1646 - val_mae: 606.1646 - val_mse: 1027194.3750 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 355.8940 - mae: 355.8940 - mse: 658045.5625 - val_loss: 606.1624 - val_mae: 606.1624 - val_mse: 1027231.0625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 355.8927 - mae: 355.8927 - mse: 658005.1875 - val_loss: 606.1489 - val_mae: 606.1489 - val_mse: 1027396.8750 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 355.8932 - mae: 355.8932 - mse: 657980.8125 - val_loss: 606.1415 - val_mae: 606.1415 - val_mse: 1027490.3125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 355.8933 - mae: 355.8933 - mse: 657968.5625 - val_loss: 606.1390 - val_mae: 606.1390 - val_mse: 1027529.0000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 355.8931 - mae: 355.8931 - mse: 657964.8750 - val_loss: 606.1400 - val_mae: 606.1400 - val_mse: 1027527.2500 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 355.8926 - mae: 355.8926 - mse: 657967.1875 - val_loss: 606.1436 - val_mae: 606.1436 - val_mse: 1027495.5625 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 355.8921 - mae: 355.8921 - mse: 657974.5625 - val_loss: 606.1491 - val_mae: 606.1491 - val_mse: 1027442.9375 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 355.8913 - mae: 355.8913 - mse: 657985.0625 - val_loss: 606.1557 - val_mae: 606.1557 - val_mse: 1027375.5000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 355.8905 - mae: 355.8905 - mse: 657998.0000 - val_loss: 606.1633 - val_mae: 606.1633 - val_mse: 1027298.0000 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 355.8895 - mae: 355.8895 - mse: 658011.0625 - val_loss: 606.1701 - val_mae: 606.1701 - val_mse: 1027230.8125 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 355.8887 - mae: 355.8887 - mse: 658020.0625 - val_loss: 606.1663 - val_mae: 606.1663 - val_mse: 1027284.9375 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 355.8875 - mae: 355.8875 - mse: 657974.9375 - val_loss: 606.1505 - val_mae: 606.1505 - val_mse: 1027475.9375 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 355.8881 - mae: 355.8881 - mse: 657947.2500 - val_loss: 606.1418 - val_mae: 606.1418 - val_mse: 1027586.8125 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 355.8882 - mae: 355.8882 - mse: 657932.8125 - val_loss: 606.1385 - val_mae: 606.1385 - val_mse: 1027637.8125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 355.8880 - mae: 355.8880 - mse: 657927.4375 - val_loss: 606.1387 - val_mae: 606.1387 - val_mse: 1027643.1875 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 355.8876 - mae: 355.8876 - mse: 657928.8750 - val_loss: 606.1419 - val_mae: 606.1419 - val_mse: 1027615.4375 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 355.8870 - mae: 355.8870 - mse: 657935.2500 - val_loss: 606.1472 - val_mae: 606.1472 - val_mse: 1027564.1250 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 355.8862 - mae: 355.8862 - mse: 657944.8750 - val_loss: 606.1537 - val_mae: 606.1537 - val_mse: 1027498.2500 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 355.8853 - mae: 355.8853 - mse: 657957.0000 - val_loss: 606.1610 - val_mae: 606.1610 - val_mse: 1027422.1250 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 355.8844 - mae: 355.8844 - mse: 657970.8125 - val_loss: 606.1689 - val_mae: 606.1689 - val_mse: 1027340.1875 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 355.8832 - mae: 355.8832 - mse: 657983.2500 - val_loss: 606.1750 - val_mae: 606.1750 - val_mse: 1027280.8750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 355.8825 - mae: 355.8825 - mse: 657990.0625 - val_loss: 606.1700 - val_mae: 606.1700 - val_mse: 1027348.9375 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 355.8813 - mae: 355.8813 - mse: 657941.3125 - val_loss: 606.1525 - val_mae: 606.1525 - val_mse: 1027559.8125 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 355.8819 - mae: 355.8819 - mse: 657910.6250 - val_loss: 606.1425 - val_mae: 606.1425 - val_mse: 1027689.5000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 355.8821 - mae: 355.8821 - mse: 657893.6250 - val_loss: 606.1378 - val_mae: 606.1378 - val_mse: 1027755.5000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 355.8820 - mae: 355.8820 - mse: 657886.1250 - val_loss: 606.1371 - val_mae: 606.1371 - val_mse: 1027773.9375 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 355.8816 - mae: 355.8816 - mse: 657885.1875 - val_loss: 606.1393 - val_mae: 606.1393 - val_mse: 1027758.0625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 355.8810 - mae: 355.8810 - mse: 657889.1250 - val_loss: 606.1435 - val_mae: 606.1435 - val_mse: 1027716.8125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 355.8802 - mae: 355.8802 - mse: 657896.8125 - val_loss: 606.1494 - val_mae: 606.1494 - val_mse: 1027657.5625 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 355.8795 - mae: 355.8795 - mse: 657907.3750 - val_loss: 606.1561 - val_mae: 606.1561 - val_mse: 1027584.8750 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 355.8785 - mae: 355.8785 - mse: 657920.2500 - val_loss: 606.1640 - val_mae: 606.1640 - val_mse: 1027504.3125 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 355.8775 - mae: 355.8775 - mse: 657934.6250 - val_loss: 606.1724 - val_mae: 606.1724 - val_mse: 1027416.3125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 355.8765 - mae: 355.8765 - mse: 657949.2500 - val_loss: 606.1801 - val_mae: 606.1801 - val_mse: 1027336.9375 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 355.8758 - mae: 355.8758 - mse: 657960.3750 - val_loss: 606.1729 - val_mae: 606.1729 - val_mse: 1027430.7500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 355.8740 - mae: 355.8740 - mse: 657892.2500 - val_loss: 606.1469 - val_mae: 606.1469 - val_mse: 1027742.4375 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 355.8752 - mae: 355.8752 - mse: 657848.7500 - val_loss: 606.1308 - val_mae: 606.1308 - val_mse: 1027947.5000 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 355.8772 - mae: 355.8772 - mse: 657824.1875 - val_loss: 606.1378 - val_mae: 606.1378 - val_mse: 1027871.6875 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 224/300\n",
            "3/3 - 0s - loss: 355.8764 - mae: 355.8764 - mse: 657893.1875 - val_loss: 606.1710 - val_mae: 606.1710 - val_mse: 1027485.5000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 355.8746 - mae: 355.8746 - mse: 657953.4375 - val_loss: 606.1839 - val_mae: 606.1839 - val_mse: 1027345.5000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 355.8711 - mae: 355.8711 - mse: 657921.0000 - val_loss: 606.1721 - val_mae: 606.1721 - val_mse: 1027490.1875 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 355.8713 - mae: 355.8713 - mse: 657898.7500 - val_loss: 606.1655 - val_mae: 606.1655 - val_mse: 1027576.0625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 228: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 355.8712 - mae: 355.8712 - mse: 657887.6250 - val_loss: 606.1635 - val_mae: 606.1635 - val_mse: 1027610.2500 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Optimizing model by reducing: mse for epochs: 300, num_iter: 2, model: LSTM_model\n",
            "Epoch 1/300\n",
            "3/3 - 2s - loss: 737302.9375 - mae: 412.6761 - mse: 737302.9375 - val_loss: 1498681.6250 - val_mae: 828.8226 - val_mse: 1498681.6250 - lr: 0.0010 - 2s/epoch - 519ms/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 1838874.5000 - mae: 688.1706 - mse: 1838874.5000 - val_loss: 3086449.0000 - val_mae: 1373.5823 - val_mse: 3086449.0000 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 709914.5625 - mae: 400.8854 - mse: 709914.6250 - val_loss: 1026931.0000 - val_mae: 618.9139 - val_mse: 1026931.0000 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 1011979.5625 - mae: 463.5753 - mse: 1011979.5625 - val_loss: 1967403.5000 - val_mae: 1054.1156 - val_mse: 1967403.5000 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 1053476.3750 - mae: 490.5433 - mse: 1053476.3750 - val_loss: 1521908.6250 - val_mae: 882.9738 - val_mse: 1521908.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 694734.4375 - mae: 374.2874 - mse: 694734.4375 - val_loss: 1030553.7500 - val_mae: 605.2635 - val_mse: 1030553.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 740229.8125 - mae: 392.4318 - mse: 740229.8125 - val_loss: 1499731.3750 - val_mae: 829.3405 - val_mse: 1499731.3750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 829458.0000 - mae: 431.9156 - mse: 829458.0000 - val_loss: 1493075.6250 - val_mae: 826.4254 - val_mse: 1493075.6250 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 698022.4375 - mae: 383.2880 - mse: 698022.4375 - val_loss: 1077490.6250 - val_mae: 623.2870 - val_mse: 1077490.6250 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 674381.0625 - mae: 360.1053 - mse: 674381.0625 - val_loss: 1063575.5000 - val_mae: 651.5313 - val_mse: 1063575.5000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 727495.0000 - mae: 380.5305 - mse: 727495.0000 - val_loss: 1102568.1250 - val_mae: 677.0627 - val_mse: 1102568.1250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 682238.6250 - mae: 364.7069 - mse: 682238.5625 - val_loss: 1022164.5625 - val_mae: 611.1635 - val_mse: 1022164.5625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 663427.6250 - mae: 359.8945 - mse: 663427.6250 - val_loss: 1100949.1250 - val_mae: 635.3345 - val_mse: 1100949.1250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 687076.3750 - mae: 373.4189 - mse: 687076.3750 - val_loss: 1143976.7500 - val_mae: 657.3265 - val_mse: 1143976.7500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 668708.2500 - mae: 365.1162 - mse: 668708.2500 - val_loss: 1052920.0000 - val_mae: 612.1200 - val_mse: 1052920.0000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 661727.3750 - mae: 356.8073 - mse: 661727.3750 - val_loss: 1022180.4375 - val_mae: 611.2112 - val_mse: 1022180.4375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 671240.2500 - mae: 359.2852 - mse: 671240.3125 - val_loss: 1024144.5000 - val_mae: 615.2191 - val_mse: 1024144.5000 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 661861.1875 - mae: 356.7426 - mse: 661861.1875 - val_loss: 1028200.1875 - val_mae: 605.0870 - val_mse: 1028200.1875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 661557.7500 - mae: 358.8636 - mse: 661557.7500 - val_loss: 1061332.8750 - val_mae: 615.8069 - val_mse: 1061332.8750 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 664127.5000 - mae: 361.2364 - mse: 664127.5000 - val_loss: 1056687.0000 - val_mae: 613.7983 - val_mse: 1056687.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 659619.6875 - mae: 357.4517 - mse: 659619.6875 - val_loss: 1029574.2500 - val_mae: 605.2571 - val_mse: 1029574.2500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 661213.8750 - mae: 356.5218 - mse: 661213.8750 - val_loss: 1022762.9375 - val_mae: 606.5388 - val_mse: 1022762.9375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 660728.6875 - mae: 356.4356 - mse: 660728.6875 - val_loss: 1026511.3750 - val_mae: 605.1639 - val_mse: 1026511.3750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 659458.8750 - mae: 356.9121 - mse: 659458.8750 - val_loss: 1040057.0625 - val_mae: 607.7304 - val_mse: 1040057.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 660405.6250 - mae: 358.2169 - mse: 660405.5625 - val_loss: 1043229.4375 - val_mae: 608.7550 - val_mse: 1043229.4375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 659440.0000 - mae: 357.1715 - mse: 659440.0000 - val_loss: 1032355.3125 - val_mae: 605.7794 - val_mse: 1032355.3125 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 659623.3750 - mae: 356.4325 - mse: 659623.3750 - val_loss: 1026798.9375 - val_mae: 605.2324 - val_mse: 1026798.9375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 659574.4375 - mae: 356.3725 - mse: 659574.4375 - val_loss: 1029333.5000 - val_mae: 605.3621 - val_mse: 1029333.5000 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 659264.0625 - mae: 356.7765 - mse: 659264.0625 - val_loss: 1035921.8125 - val_mae: 606.5632 - val_mse: 1035921.8125 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 659447.7500 - mae: 357.2191 - mse: 659447.7500 - val_loss: 1036658.5625 - val_mae: 606.7764 - val_mse: 1036658.5625 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 659193.7500 - mae: 356.7902 - mse: 659193.7500 - val_loss: 1031674.8750 - val_mae: 605.7681 - val_mse: 1031674.8750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 659262.2500 - mae: 356.4565 - mse: 659262.2500 - val_loss: 1029467.0000 - val_mae: 605.4590 - val_mse: 1029467.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 659173.6250 - mae: 356.4846 - mse: 659173.5625 - val_loss: 1031717.0000 - val_mae: 605.8152 - val_mse: 1031717.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 659131.9375 - mae: 356.7802 - mse: 659132.0000 - val_loss: 1034606.5625 - val_mae: 606.3867 - val_mse: 1034606.5625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 659118.8750 - mae: 356.8526 - mse: 659118.8750 - val_loss: 1033755.0000 - val_mae: 606.2354 - val_mse: 1033755.0000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 659049.5625 - mae: 356.6177 - mse: 659049.5625 - val_loss: 1031413.6250 - val_mae: 605.8271 - val_mse: 1031413.6250 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 659045.3125 - mae: 356.5076 - mse: 659045.3125 - val_loss: 1031294.3750 - val_mae: 605.8295 - val_mse: 1031294.3750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 658986.8125 - mae: 356.6105 - mse: 658986.8125 - val_loss: 1032994.7500 - val_mae: 606.1640 - val_mse: 1032994.7500 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 658969.6875 - mae: 356.7335 - mse: 658969.6875 - val_loss: 1033629.0625 - val_mae: 606.3112 - val_mse: 1033629.0625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 658923.9375 - mae: 356.6637 - mse: 658923.9375 - val_loss: 1032530.4375 - val_mae: 606.1336 - val_mse: 1032530.4375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 658895.0000 - mae: 356.5606 - mse: 658895.0000 - val_loss: 1031910.8750 - val_mae: 606.0535 - val_mse: 1031910.8750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 658855.8125 - mae: 356.5748 - mse: 658855.8125 - val_loss: 1032619.6250 - val_mae: 606.2169 - val_mse: 1032619.6250 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 658820.5625 - mae: 356.6471 - mse: 658820.5625 - val_loss: 1033309.0000 - val_mae: 606.3715 - val_mse: 1033309.0000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 658782.5000 - mae: 356.6410 - mse: 658782.5000 - val_loss: 1032967.3125 - val_mae: 606.3438 - val_mse: 1032967.3125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 658744.7500 - mae: 356.5777 - mse: 658744.7500 - val_loss: 1032487.1875 - val_mae: 606.2898 - val_mse: 1032487.1875 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 658708.3125 - mae: 356.5655 - mse: 658708.3125 - val_loss: 1032725.3125 - val_mae: 606.3657 - val_mse: 1032725.3125 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 658671.1875 - mae: 356.6024 - mse: 658671.1875 - val_loss: 1033154.6250 - val_mae: 606.4747 - val_mse: 1033154.6250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 658635.0625 - mae: 356.6088 - mse: 658635.0625 - val_loss: 1033050.5625 - val_mae: 606.4881 - val_mse: 1033050.5625 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 658597.8125 - mae: 356.5734 - mse: 658597.8750 - val_loss: 1032746.1250 - val_mae: 606.4651 - val_mse: 1032746.1250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 658561.8750 - mae: 356.5590 - mse: 658561.8750 - val_loss: 1032824.1250 - val_mae: 606.5117 - val_mse: 1032824.1250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 658525.2500 - mae: 356.5750 - mse: 658525.2500 - val_loss: 1033076.0000 - val_mae: 606.5901 - val_mse: 1033076.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 658489.7500 - mae: 356.5775 - mse: 658489.7500 - val_loss: 1033043.2500 - val_mae: 606.6176 - val_mse: 1033043.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 658453.2500 - mae: 356.5550 - mse: 658453.1875 - val_loss: 1032877.2500 - val_mae: 606.6216 - val_mse: 1032877.2500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 658418.5625 - mae: 356.5453 - mse: 658418.5000 - val_loss: 1032932.1875 - val_mae: 606.6658 - val_mse: 1032932.1875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 658386.0625 - mae: 356.5532 - mse: 658386.0625 - val_loss: 1033064.3750 - val_mae: 606.7203 - val_mse: 1033064.3750 - lr: 0.0010 - 62ms/epoch - 21ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 658354.8125 - mae: 356.5501 - mse: 658354.8750 - val_loss: 1033018.6875 - val_mae: 606.7461 - val_mse: 1033018.6875 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 658324.2500 - mae: 356.5368 - mse: 658324.1875 - val_loss: 1032946.3750 - val_mae: 606.7673 - val_mse: 1032946.3750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 658294.1250 - mae: 356.5334 - mse: 658294.0625 - val_loss: 1033007.5625 - val_mae: 606.8090 - val_mse: 1033007.5625 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 658264.5000 - mae: 356.5370 - mse: 658264.5000 - val_loss: 1033075.0625 - val_mae: 606.8497 - val_mse: 1033075.0625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 658234.8750 - mae: 356.5331 - mse: 658234.8750 - val_loss: 1033041.8125 - val_mae: 606.8746 - val_mse: 1033041.8125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 658205.5625 - mae: 356.5248 - mse: 658205.5625 - val_loss: 1033016.2500 - val_mae: 606.9000 - val_mse: 1033016.2500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 658176.4375 - mae: 356.5237 - mse: 658176.4375 - val_loss: 1033064.1875 - val_mae: 606.9373 - val_mse: 1033064.1875 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 658147.5625 - mae: 356.5250 - mse: 658147.5625 - val_loss: 1033094.6875 - val_mae: 606.9725 - val_mse: 1033094.6875 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 658118.8750 - mae: 356.5204 - mse: 658118.8125 - val_loss: 1033073.8125 - val_mae: 606.9984 - val_mse: 1033073.8125 - lr: 0.0010 - 59ms/epoch - 20ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 658090.1250 - mae: 356.5152 - mse: 658090.1250 - val_loss: 1033076.1875 - val_mae: 607.0283 - val_mse: 1033076.1875 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 658061.9375 - mae: 356.5148 - mse: 658061.9375 - val_loss: 1033106.4375 - val_mae: 607.0629 - val_mse: 1033106.4375 - lr: 0.0010 - 55ms/epoch - 18ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 658037.1875 - mae: 356.5138 - mse: 658037.2500 - val_loss: 1033090.0625 - val_mae: 607.0891 - val_mse: 1033090.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 658013.3750 - mae: 356.5069 - mse: 658013.3750 - val_loss: 1033057.3750 - val_mae: 607.1116 - val_mse: 1033057.3750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 657989.9375 - mae: 356.5063 - mse: 657989.9375 - val_loss: 1033096.0625 - val_mae: 607.1465 - val_mse: 1033096.0625 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 657966.3125 - mae: 356.5087 - mse: 657966.3750 - val_loss: 1033125.3750 - val_mae: 607.1795 - val_mse: 1033125.3750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 657943.0625 - mae: 356.5051 - mse: 657943.0625 - val_loss: 1033101.5625 - val_mae: 607.2023 - val_mse: 1033101.5625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 657920.0000 - mae: 356.5011 - mse: 657920.0000 - val_loss: 1033102.2500 - val_mae: 607.2297 - val_mse: 1033102.2500 - lr: 0.0010 - 43ms/epoch - 14ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 657897.0625 - mae: 356.5017 - mse: 657897.0625 - val_loss: 1033134.5625 - val_mae: 607.2624 - val_mse: 1033134.5625 - lr: 0.0010 - 55ms/epoch - 18ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 657874.5000 - mae: 356.5012 - mse: 657874.4375 - val_loss: 1033136.9375 - val_mae: 607.2896 - val_mse: 1033136.9375 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 657851.9375 - mae: 356.4978 - mse: 657851.8750 - val_loss: 1033124.6250 - val_mae: 607.3138 - val_mse: 1033124.6250 - lr: 0.0010 - 57ms/epoch - 19ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 657829.2500 - mae: 356.4962 - mse: 657829.2500 - val_loss: 1033137.1875 - val_mae: 607.3423 - val_mse: 1033137.1875 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 657807.4375 - mae: 356.4966 - mse: 657807.4375 - val_loss: 1033156.5000 - val_mae: 607.3718 - val_mse: 1033156.5000 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 657785.2500 - mae: 356.4951 - mse: 657785.2500 - val_loss: 1033154.0000 - val_mae: 607.3972 - val_mse: 1033154.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 657763.3125 - mae: 356.4930 - mse: 657763.2500 - val_loss: 1033156.3125 - val_mae: 607.4234 - val_mse: 1033156.3125 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 657741.5625 - mae: 356.4926 - mse: 657741.5000 - val_loss: 1033172.5000 - val_mae: 607.4519 - val_mse: 1033172.5000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 657719.6875 - mae: 356.4921 - mse: 657719.7500 - val_loss: 1033182.2500 - val_mae: 607.4790 - val_mse: 1033182.2500 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 657698.0000 - mae: 356.4905 - mse: 657698.0625 - val_loss: 1033184.2500 - val_mae: 607.5051 - val_mse: 1033184.2500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 657676.6250 - mae: 356.4892 - mse: 657676.6875 - val_loss: 1033193.0625 - val_mae: 607.5326 - val_mse: 1033193.0625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 657654.9375 - mae: 356.4886 - mse: 657654.9375 - val_loss: 1033205.6875 - val_mae: 607.5607 - val_mse: 1033205.6875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 657633.8750 - mae: 356.4876 - mse: 657633.8750 - val_loss: 1033210.8750 - val_mae: 607.5870 - val_mse: 1033210.8750 - lr: 0.0010 - 56ms/epoch - 19ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 657612.6875 - mae: 356.4864 - mse: 657612.6875 - val_loss: 1033218.8125 - val_mae: 607.6138 - val_mse: 1033218.8125 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 657591.6875 - mae: 356.4858 - mse: 657591.6875 - val_loss: 1033230.6875 - val_mae: 607.6412 - val_mse: 1033230.6875 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 657570.9375 - mae: 356.4851 - mse: 657570.8125 - val_loss: 1033238.7500 - val_mae: 607.6679 - val_mse: 1033238.7500 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 657549.8750 - mae: 356.4839 - mse: 657549.9375 - val_loss: 1033260.3125 - val_mae: 607.6969 - val_mse: 1033260.3125 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 657528.6875 - mae: 356.4857 - mse: 657528.6875 - val_loss: 1033287.2500 - val_mae: 607.7271 - val_mse: 1033287.2500 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 657507.3750 - mae: 356.4836 - mse: 657507.3750 - val_loss: 1033279.4375 - val_mae: 607.7505 - val_mse: 1033279.4375 - lr: 0.0010 - 59ms/epoch - 20ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 657486.2500 - mae: 356.4815 - mse: 657486.1875 - val_loss: 1033291.0000 - val_mae: 607.7777 - val_mse: 1033291.0000 - lr: 0.0010 - 72ms/epoch - 24ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 657464.9375 - mae: 356.4825 - mse: 657464.9375 - val_loss: 1033317.6250 - val_mae: 607.8077 - val_mse: 1033317.6250 - lr: 0.0010 - 57ms/epoch - 19ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 657443.5625 - mae: 356.4815 - mse: 657443.5625 - val_loss: 1033321.3125 - val_mae: 607.8331 - val_mse: 1033321.3125 - lr: 0.0010 - 60ms/epoch - 20ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 657422.4375 - mae: 356.4797 - mse: 657422.5000 - val_loss: 1033329.6250 - val_mae: 607.8592 - val_mse: 1033329.6250 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 657401.3750 - mae: 356.4800 - mse: 657401.3750 - val_loss: 1033352.3750 - val_mae: 607.8881 - val_mse: 1033352.3750 - lr: 0.0010 - 60ms/epoch - 20ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 657380.4375 - mae: 356.4796 - mse: 657380.4375 - val_loss: 1033361.5000 - val_mae: 607.9138 - val_mse: 1033361.5000 - lr: 0.0010 - 62ms/epoch - 21ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 657359.5000 - mae: 356.4782 - mse: 657359.5000 - val_loss: 1033366.1875 - val_mae: 607.9374 - val_mse: 1033366.1875 - lr: 0.0010 - 56ms/epoch - 19ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 657338.9375 - mae: 356.4781 - mse: 657338.9375 - val_loss: 1033382.8125 - val_mae: 607.9634 - val_mse: 1033382.8125 - lr: 0.0010 - 74ms/epoch - 25ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 657318.3125 - mae: 356.4779 - mse: 657318.3125 - val_loss: 1033393.0000 - val_mae: 607.9878 - val_mse: 1033393.0000 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 657297.9375 - mae: 356.4770 - mse: 657298.0000 - val_loss: 1033399.9375 - val_mae: 608.0113 - val_mse: 1033399.9375 - lr: 0.0010 - 60ms/epoch - 20ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 657277.5625 - mae: 356.4774 - mse: 657277.5625 - val_loss: 1033414.3125 - val_mae: 608.0361 - val_mse: 1033414.3125 - lr: 0.0010 - 55ms/epoch - 18ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 657257.5625 - mae: 356.4781 - mse: 657257.5625 - val_loss: 1033425.8750 - val_mae: 608.0602 - val_mse: 1033425.8750 - lr: 0.0010 - 63ms/epoch - 21ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 657237.5000 - mae: 356.4780 - mse: 657237.5000 - val_loss: 1033434.5625 - val_mae: 608.0836 - val_mse: 1033434.5625 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 657217.7500 - mae: 356.4785 - mse: 657217.7500 - val_loss: 1033447.6250 - val_mae: 608.1075 - val_mse: 1033447.6250 - lr: 0.0010 - 43ms/epoch - 14ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 657197.8750 - mae: 356.4791 - mse: 657197.8750 - val_loss: 1033460.0000 - val_mae: 608.1313 - val_mse: 1033460.0000 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 657178.3750 - mae: 356.4793 - mse: 657178.3750 - val_loss: 1033469.7500 - val_mae: 608.1541 - val_mse: 1033469.7500 - lr: 0.0010 - 61ms/epoch - 20ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 657158.7500 - mae: 356.4799 - mse: 657158.8125 - val_loss: 1033482.1875 - val_mae: 608.1774 - val_mse: 1033482.1875 - lr: 0.0010 - 60ms/epoch - 20ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 657139.5000 - mae: 356.4808 - mse: 657139.4375 - val_loss: 1033494.3125 - val_mae: 608.2004 - val_mse: 1033494.3125 - lr: 0.0010 - 66ms/epoch - 22ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 657120.1875 - mae: 356.4814 - mse: 657120.1250 - val_loss: 1033505.1250 - val_mae: 608.2230 - val_mse: 1033505.1250 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 657100.9375 - mae: 356.4821 - mse: 657100.8750 - val_loss: 1033518.3125 - val_mae: 608.2457 - val_mse: 1033518.3125 - lr: 0.0010 - 71ms/epoch - 24ms/step\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 112: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 657081.6250 - mae: 356.4832 - mse: 657081.6250 - val_loss: 1033531.1875 - val_mae: 608.2682 - val_mse: 1033531.1875 - lr: 0.0010 - 59ms/epoch - 20ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 657073.1875 - mae: 356.4843 - mse: 657073.1875 - val_loss: 1033535.4375 - val_mae: 608.2710 - val_mse: 1033535.4375 - lr: 1.0000e-04 - 58ms/epoch - 19ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 657071.5000 - mae: 356.4850 - mse: 657071.5000 - val_loss: 1033547.3125 - val_mae: 608.2756 - val_mse: 1033547.3125 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 657069.4375 - mae: 356.4860 - mse: 657069.4375 - val_loss: 1033561.4375 - val_mae: 608.2805 - val_mse: 1033561.4375 - lr: 1.0000e-04 - 65ms/epoch - 22ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 657067.3750 - mae: 356.4870 - mse: 657067.3750 - val_loss: 1033574.2500 - val_mae: 608.2852 - val_mse: 1033574.2500 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 657065.6250 - mae: 356.4877 - mse: 657065.5625 - val_loss: 1033582.1250 - val_mae: 608.2889 - val_mse: 1033582.1250 - lr: 1.0000e-04 - 71ms/epoch - 24ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 657063.6875 - mae: 356.4881 - mse: 657063.6875 - val_loss: 1033585.4375 - val_mae: 608.2916 - val_mse: 1033585.4375 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 657061.7500 - mae: 356.4881 - mse: 657061.7500 - val_loss: 1033584.1875 - val_mae: 608.2933 - val_mse: 1033584.1875 - lr: 1.0000e-04 - 64ms/epoch - 21ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 657059.8125 - mae: 356.4879 - mse: 657059.8125 - val_loss: 1033580.5000 - val_mae: 608.2945 - val_mse: 1033580.5000 - lr: 1.0000e-04 - 58ms/epoch - 19ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 657057.8125 - mae: 356.4876 - mse: 657057.8125 - val_loss: 1033576.6250 - val_mae: 608.2956 - val_mse: 1033576.6250 - lr: 1.0000e-04 - 57ms/epoch - 19ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 657055.8750 - mae: 356.4874 - mse: 657055.8750 - val_loss: 1033573.4375 - val_mae: 608.2970 - val_mse: 1033573.4375 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 657053.9375 - mae: 356.4873 - mse: 657053.9375 - val_loss: 1033572.7500 - val_mae: 608.2988 - val_mse: 1033572.7500 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 657052.0625 - mae: 356.4873 - mse: 657052.1250 - val_loss: 1033573.9375 - val_mae: 608.3011 - val_mse: 1033573.9375 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 657050.0000 - mae: 356.4875 - mse: 657049.9375 - val_loss: 1033576.6250 - val_mae: 608.3036 - val_mse: 1033576.6250 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 657047.9375 - mae: 356.4877 - mse: 657047.9375 - val_loss: 1033579.9375 - val_mae: 608.3064 - val_mse: 1033579.9375 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 657046.1875 - mae: 356.4880 - mse: 657046.1875 - val_loss: 1033583.5625 - val_mae: 608.3091 - val_mse: 1033583.5625 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 657044.0000 - mae: 356.4882 - mse: 657044.0000 - val_loss: 1033586.5625 - val_mae: 608.3118 - val_mse: 1033586.5625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 657042.1250 - mae: 356.4884 - mse: 657042.1250 - val_loss: 1033588.6250 - val_mae: 608.3143 - val_mse: 1033588.6250 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 657040.1875 - mae: 356.4885 - mse: 657040.1250 - val_loss: 1033589.3750 - val_mae: 608.3165 - val_mse: 1033589.3750 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 657038.1250 - mae: 356.4886 - mse: 657038.1250 - val_loss: 1033589.8125 - val_mae: 608.3187 - val_mse: 1033589.8125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 657036.1250 - mae: 356.4886 - mse: 657036.2500 - val_loss: 1033590.1875 - val_mae: 608.3208 - val_mse: 1033590.1875 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 657034.0625 - mae: 356.4886 - mse: 657034.1875 - val_loss: 1033590.6250 - val_mae: 608.3232 - val_mse: 1033590.6250 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 657032.0625 - mae: 356.4886 - mse: 657032.0625 - val_loss: 1033591.9375 - val_mae: 608.3256 - val_mse: 1033591.9375 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 657030.0000 - mae: 356.4888 - mse: 657030.0000 - val_loss: 1033593.5000 - val_mae: 608.3280 - val_mse: 1033593.5000 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 657027.8750 - mae: 356.4889 - mse: 657027.8750 - val_loss: 1033595.0625 - val_mae: 608.3304 - val_mse: 1033595.0625 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 657026.0000 - mae: 356.4890 - mse: 657026.0000 - val_loss: 1033596.5625 - val_mae: 608.3329 - val_mse: 1033596.5625 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 657023.8750 - mae: 356.4891 - mse: 657023.8750 - val_loss: 1033598.1875 - val_mae: 608.3354 - val_mse: 1033598.1875 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 657021.8125 - mae: 356.4893 - mse: 657021.8125 - val_loss: 1033599.9375 - val_mae: 608.3380 - val_mse: 1033599.9375 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 657019.6875 - mae: 356.4894 - mse: 657019.6875 - val_loss: 1033601.5625 - val_mae: 608.3405 - val_mse: 1033601.5625 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 657017.6250 - mae: 356.4895 - mse: 657017.6250 - val_loss: 1033603.0000 - val_mae: 608.3429 - val_mse: 1033603.0000 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 657015.5625 - mae: 356.4896 - mse: 657015.5625 - val_loss: 1033604.0625 - val_mae: 608.3454 - val_mse: 1033604.0625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 657013.5000 - mae: 356.4897 - mse: 657013.5000 - val_loss: 1033605.0625 - val_mae: 608.3478 - val_mse: 1033605.0625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 657011.5000 - mae: 356.4898 - mse: 657011.5000 - val_loss: 1033606.9375 - val_mae: 608.3504 - val_mse: 1033606.9375 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 657009.1875 - mae: 356.4900 - mse: 657009.2500 - val_loss: 1033608.3750 - val_mae: 608.3529 - val_mse: 1033608.3750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 657007.3125 - mae: 356.4901 - mse: 657007.3125 - val_loss: 1033610.5625 - val_mae: 608.3557 - val_mse: 1033610.5625 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 657005.1250 - mae: 356.4902 - mse: 657005.1250 - val_loss: 1033611.8125 - val_mae: 608.3581 - val_mse: 1033611.8125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 657003.0625 - mae: 356.4903 - mse: 657003.0625 - val_loss: 1033612.9375 - val_mae: 608.3605 - val_mse: 1033612.9375 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 657000.8750 - mae: 356.4904 - mse: 657000.8750 - val_loss: 1033614.3750 - val_mae: 608.3631 - val_mse: 1033614.3750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 656998.7500 - mae: 356.4905 - mse: 656998.6875 - val_loss: 1033616.0000 - val_mae: 608.3658 - val_mse: 1033616.0000 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 656996.6875 - mae: 356.4907 - mse: 656996.6875 - val_loss: 1033617.2500 - val_mae: 608.3681 - val_mse: 1033617.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 656994.3750 - mae: 356.4908 - mse: 656994.3750 - val_loss: 1033619.1250 - val_mae: 608.3708 - val_mse: 1033619.1250 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 656992.3750 - mae: 356.4909 - mse: 656992.3750 - val_loss: 1033621.0625 - val_mae: 608.3735 - val_mse: 1033621.0625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 656990.1250 - mae: 356.4910 - mse: 656990.1875 - val_loss: 1033622.0000 - val_mae: 608.3760 - val_mse: 1033622.0000 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 656987.8750 - mae: 356.4911 - mse: 656987.8750 - val_loss: 1033623.8125 - val_mae: 608.3785 - val_mse: 1033623.8125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 656985.8125 - mae: 356.4913 - mse: 656985.8125 - val_loss: 1033625.5625 - val_mae: 608.3812 - val_mse: 1033625.5625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 656983.6250 - mae: 356.4914 - mse: 656983.5625 - val_loss: 1033626.8125 - val_mae: 608.3838 - val_mse: 1033626.8125 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 656981.5000 - mae: 356.4915 - mse: 656981.5000 - val_loss: 1033628.1875 - val_mae: 608.3863 - val_mse: 1033628.1875 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 656979.3750 - mae: 356.4916 - mse: 656979.3750 - val_loss: 1033629.5000 - val_mae: 608.3889 - val_mse: 1033629.5000 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 656977.0625 - mae: 356.4917 - mse: 656977.0625 - val_loss: 1033631.1250 - val_mae: 608.3914 - val_mse: 1033631.1250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 656974.8125 - mae: 356.4919 - mse: 656974.6875 - val_loss: 1033632.1250 - val_mae: 608.3941 - val_mse: 1033632.1250 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 656972.6875 - mae: 356.4919 - mse: 656972.6250 - val_loss: 1033633.7500 - val_mae: 608.3967 - val_mse: 1033633.7500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 656970.3125 - mae: 356.4921 - mse: 656970.3125 - val_loss: 1033635.5625 - val_mae: 608.3994 - val_mse: 1033635.5625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 656968.1250 - mae: 356.4922 - mse: 656968.1250 - val_loss: 1033637.4375 - val_mae: 608.4020 - val_mse: 1033637.4375 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 656966.0000 - mae: 356.4924 - mse: 656966.0000 - val_loss: 1033639.2500 - val_mae: 608.4048 - val_mse: 1033639.2500 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 656963.6250 - mae: 356.4925 - mse: 656963.6250 - val_loss: 1033640.4375 - val_mae: 608.4073 - val_mse: 1033640.4375 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 656961.5000 - mae: 356.4926 - mse: 656961.5000 - val_loss: 1033641.8750 - val_mae: 608.4100 - val_mse: 1033641.8750 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 656959.1875 - mae: 356.4927 - mse: 656959.1875 - val_loss: 1033643.6250 - val_mae: 608.4125 - val_mse: 1033643.6250 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 656957.1250 - mae: 356.4929 - mse: 656957.0625 - val_loss: 1033645.3750 - val_mae: 608.4153 - val_mse: 1033645.3750 - lr: 1.0000e-04 - 43ms/epoch - 14ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 656954.6250 - mae: 356.4930 - mse: 656954.6250 - val_loss: 1033646.9375 - val_mae: 608.4182 - val_mse: 1033646.9375 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 656952.4375 - mae: 356.4930 - mse: 656952.4375 - val_loss: 1033648.1875 - val_mae: 608.4207 - val_mse: 1033648.1875 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 656950.1250 - mae: 356.4932 - mse: 656950.0625 - val_loss: 1033649.6250 - val_mae: 608.4234 - val_mse: 1033649.6250 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 656948.0625 - mae: 356.4933 - mse: 656948.0625 - val_loss: 1033650.6250 - val_mae: 608.4258 - val_mse: 1033650.6250 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 656945.5000 - mae: 356.4934 - mse: 656945.5000 - val_loss: 1033652.5625 - val_mae: 608.4286 - val_mse: 1033652.5625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 656943.3125 - mae: 356.4936 - mse: 656943.3125 - val_loss: 1033654.8125 - val_mae: 608.4314 - val_mse: 1033654.8125 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 656941.0000 - mae: 356.4937 - mse: 656941.0000 - val_loss: 1033656.6875 - val_mae: 608.4343 - val_mse: 1033656.6875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 656938.6250 - mae: 356.4938 - mse: 656938.5625 - val_loss: 1033658.5000 - val_mae: 608.4369 - val_mse: 1033658.5000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 656936.4375 - mae: 356.4940 - mse: 656936.4375 - val_loss: 1033660.1875 - val_mae: 608.4396 - val_mse: 1033660.1875 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 656934.1875 - mae: 356.4941 - mse: 656934.1875 - val_loss: 1033661.8125 - val_mae: 608.4424 - val_mse: 1033661.8125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 656931.7500 - mae: 356.4942 - mse: 656931.7500 - val_loss: 1033663.3750 - val_mae: 608.4451 - val_mse: 1033663.3750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 656929.4375 - mae: 356.4943 - mse: 656929.4375 - val_loss: 1033664.7500 - val_mae: 608.4478 - val_mse: 1033664.7500 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 656927.2500 - mae: 356.4945 - mse: 656927.2500 - val_loss: 1033666.2500 - val_mae: 608.4506 - val_mse: 1033666.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 656924.7500 - mae: 356.4946 - mse: 656924.7500 - val_loss: 1033667.8750 - val_mae: 608.4532 - val_mse: 1033667.8750 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 656922.4375 - mae: 356.4947 - mse: 656922.4375 - val_loss: 1033669.1250 - val_mae: 608.4558 - val_mse: 1033669.1250 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 656920.1250 - mae: 356.4948 - mse: 656920.1250 - val_loss: 1033671.5000 - val_mae: 608.4587 - val_mse: 1033671.5000 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 656917.7500 - mae: 356.4949 - mse: 656917.8750 - val_loss: 1033673.5625 - val_mae: 608.4615 - val_mse: 1033673.5625 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 656915.5625 - mae: 356.4951 - mse: 656915.5625 - val_loss: 1033675.4375 - val_mae: 608.4643 - val_mse: 1033675.4375 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 656913.0625 - mae: 356.4952 - mse: 656913.0625 - val_loss: 1033677.2500 - val_mae: 608.4670 - val_mse: 1033677.2500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 656910.5625 - mae: 356.4953 - mse: 656910.5625 - val_loss: 1033678.2500 - val_mae: 608.4697 - val_mse: 1033678.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 656908.2500 - mae: 356.4954 - mse: 656908.2500 - val_loss: 1033680.6250 - val_mae: 608.4726 - val_mse: 1033680.6250 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 656905.8750 - mae: 356.4955 - mse: 656905.8750 - val_loss: 1033681.6875 - val_mae: 608.4753 - val_mse: 1033681.6875 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 656903.5000 - mae: 356.4957 - mse: 656903.5000 - val_loss: 1033683.3125 - val_mae: 608.4780 - val_mse: 1033683.3125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 656901.1875 - mae: 356.4958 - mse: 656901.1875 - val_loss: 1033684.3750 - val_mae: 608.4807 - val_mse: 1033684.3750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 656898.8125 - mae: 356.4960 - mse: 656898.8125 - val_loss: 1033686.6875 - val_mae: 608.4833 - val_mse: 1033686.6875 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 656896.2500 - mae: 356.4961 - mse: 656896.2500 - val_loss: 1033689.0000 - val_mae: 608.4865 - val_mse: 1033689.0000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 656893.9375 - mae: 356.4963 - mse: 656893.9375 - val_loss: 1033690.8125 - val_mae: 608.4892 - val_mse: 1033690.8125 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 656891.6875 - mae: 356.4965 - mse: 656891.6250 - val_loss: 1033692.6250 - val_mae: 608.4920 - val_mse: 1033692.6250 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 656889.2500 - mae: 356.4966 - mse: 656889.2500 - val_loss: 1033694.4375 - val_mae: 608.4949 - val_mse: 1033694.4375 - lr: 1.0000e-04 - 60ms/epoch - 20ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 656886.6875 - mae: 356.4967 - mse: 656886.6875 - val_loss: 1033696.2500 - val_mae: 608.4977 - val_mse: 1033696.2500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 656884.3125 - mae: 356.4968 - mse: 656884.3125 - val_loss: 1033697.6875 - val_mae: 608.5004 - val_mse: 1033697.6875 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 656882.0000 - mae: 356.4970 - mse: 656882.0000 - val_loss: 1033699.5625 - val_mae: 608.5033 - val_mse: 1033699.5625 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 656879.4375 - mae: 356.4971 - mse: 656879.3750 - val_loss: 1033701.2500 - val_mae: 608.5060 - val_mse: 1033701.2500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 656877.1250 - mae: 356.4972 - mse: 656877.1250 - val_loss: 1033703.0000 - val_mae: 608.5090 - val_mse: 1033703.0000 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 656874.5000 - mae: 356.4973 - mse: 656874.4375 - val_loss: 1033704.6875 - val_mae: 608.5117 - val_mse: 1033704.6875 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 656872.1250 - mae: 356.4975 - mse: 656872.1250 - val_loss: 1033706.6250 - val_mae: 608.5146 - val_mse: 1033706.6250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 656869.6250 - mae: 356.4977 - mse: 656869.6250 - val_loss: 1033709.0625 - val_mae: 608.5175 - val_mse: 1033709.0625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 656867.0000 - mae: 356.4978 - mse: 656867.0625 - val_loss: 1033710.1875 - val_mae: 608.5201 - val_mse: 1033710.1875 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 656864.6875 - mae: 356.4979 - mse: 656864.6875 - val_loss: 1033712.0000 - val_mae: 608.5232 - val_mse: 1033712.0000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 656862.1875 - mae: 356.4980 - mse: 656862.1250 - val_loss: 1033713.7500 - val_mae: 608.5259 - val_mse: 1033713.7500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 656859.7500 - mae: 356.4981 - mse: 656859.7500 - val_loss: 1033715.1875 - val_mae: 608.5286 - val_mse: 1033715.1875 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 656857.3125 - mae: 356.4983 - mse: 656857.3125 - val_loss: 1033717.0000 - val_mae: 608.5316 - val_mse: 1033717.0000 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 212: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 656854.8125 - mae: 356.4985 - mse: 656854.8125 - val_loss: 1033719.8125 - val_mae: 608.5346 - val_mse: 1033719.8125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Optimizing model by reducing: mape for epochs: 300, num_iter: 2, model: LSTM_model\n",
            "Epoch 1/300\n",
            "3/3 - 2s - loss: 3.2157 - mae: 420.6124 - mse: 741697.8750 - val_loss: 2.1878 - val_mae: 728.2292 - val_mse: 1192437.7500 - lr: 0.0010 - 2s/epoch - 545ms/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 3.0399 - mae: 406.2654 - mse: 781888.3125 - val_loss: 1.8281 - val_mae: 623.4860 - val_mse: 1030382.0625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 2.7410 - mae: 391.4260 - mse: 737309.1875 - val_loss: 2.2339 - val_mae: 765.8599 - val_mse: 1359918.8750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 2.8006 - mae: 361.3180 - mse: 667639.6875 - val_loss: 2.0690 - val_mae: 692.4905 - val_mse: 1128348.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 2.7926 - mae: 376.6652 - mse: 709113.4375 - val_loss: 1.8010 - val_mae: 616.0867 - val_mse: 1024574.5625 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 2.6332 - mae: 369.6244 - mse: 682690.6250 - val_loss: 1.9772 - val_mae: 681.1597 - val_mse: 1190241.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 2.6994 - mae: 358.6334 - mse: 661769.8750 - val_loss: 1.8920 - val_mae: 641.0940 - val_mse: 1049972.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 2.6633 - mae: 366.1402 - mse: 686501.5000 - val_loss: 1.7924 - val_mae: 613.7576 - val_mse: 1023274.8750 - lr: 0.0010 - 43ms/epoch - 14ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 2.5965 - mae: 361.3280 - mse: 665647.1875 - val_loss: 1.8228 - val_mae: 629.3174 - val_mse: 1089494.1250 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 2.6226 - mae: 357.8337 - mse: 664747.1250 - val_loss: 1.8677 - val_mae: 634.4125 - val_mse: 1041903.9375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 2.6191 - mae: 359.7324 - mse: 669830.5625 - val_loss: 1.7577 - val_mae: 605.3376 - val_mse: 1024578.6250 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 2.5854 - mae: 358.4416 - mse: 661027.5000 - val_loss: 1.7607 - val_mae: 608.0154 - val_mse: 1041695.9375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 2.5889 - mae: 357.9870 - mse: 667208.1250 - val_loss: 1.8234 - val_mae: 622.2664 - val_mse: 1029556.5625 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 2.5874 - mae: 356.7466 - mse: 661214.1875 - val_loss: 1.7560 - val_mae: 606.0988 - val_mse: 1035355.0625 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 2.5840 - mae: 356.6363 - mse: 659716.3125 - val_loss: 1.7616 - val_mae: 606.2050 - val_mse: 1023367.8750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 2.5780 - mae: 357.5319 - mse: 665499.6250 - val_loss: 1.7786 - val_mae: 610.2344 - val_mse: 1022295.9375 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 2.5749 - mae: 356.4088 - mse: 659655.9375 - val_loss: 1.7560 - val_mae: 606.1012 - val_mse: 1035261.8125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 2.5814 - mae: 356.8164 - mse: 662521.5625 - val_loss: 1.7886 - val_mae: 612.8494 - val_mse: 1023362.0000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 2.5787 - mae: 356.6711 - mse: 661613.8125 - val_loss: 1.7550 - val_mae: 605.1112 - val_mse: 1028355.1875 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 2.5764 - mae: 356.4301 - mse: 660353.3750 - val_loss: 1.7653 - val_mae: 607.1088 - val_mse: 1022950.4375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 2.5751 - mae: 356.7839 - mse: 662515.0625 - val_loss: 1.7635 - val_mae: 606.7141 - val_mse: 1023298.3750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 2.5733 - mae: 356.3437 - mse: 660051.9375 - val_loss: 1.7559 - val_mae: 605.1876 - val_mse: 1027046.5625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 2.5760 - mae: 356.7395 - mse: 662425.8750 - val_loss: 1.7726 - val_mae: 608.8195 - val_mse: 1022494.8750 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 2.5782 - mae: 357.9880 - mse: 666061.3125 - val_loss: 1.7570 - val_mae: 605.3695 - val_mse: 1025893.1250 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 2.5792 - mae: 357.1185 - mse: 663971.1875 - val_loss: 1.7733 - val_mae: 609.0093 - val_mse: 1022533.6875 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 2.5739 - mae: 356.3505 - mse: 659464.7500 - val_loss: 1.7554 - val_mae: 605.7899 - val_mse: 1033491.5625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 2.5801 - mae: 356.9933 - mse: 663455.1250 - val_loss: 1.7851 - val_mae: 611.9677 - val_mse: 1023231.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 2.5765 - mae: 356.3705 - mse: 659834.3750 - val_loss: 1.7556 - val_mae: 605.9446 - val_mse: 1034283.6875 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 2.5797 - mae: 356.7387 - mse: 662299.3125 - val_loss: 1.7851 - val_mae: 611.9851 - val_mse: 1023254.7500 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 2.5770 - mae: 356.4014 - mse: 660181.5000 - val_loss: 1.7551 - val_mae: 605.6329 - val_mse: 1032472.4375 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 2.5782 - mae: 356.6678 - mse: 662025.3750 - val_loss: 1.7801 - val_mae: 610.7041 - val_mse: 1022825.8750 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 2.5756 - mae: 356.3415 - mse: 659877.3750 - val_loss: 1.7551 - val_mae: 605.5845 - val_mse: 1032138.8125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 2.5781 - mae: 356.7169 - mse: 662314.9375 - val_loss: 1.7786 - val_mae: 610.3276 - val_mse: 1022750.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 2.5751 - mae: 356.3124 - mse: 659654.4375 - val_loss: 1.7552 - val_mae: 605.6841 - val_mse: 1032791.6250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 2.5787 - mae: 356.7931 - mse: 662657.8125 - val_loss: 1.7800 - val_mae: 610.7095 - val_mse: 1022863.0625 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 2.5752 - mae: 356.2965 - mse: 659430.8125 - val_loss: 1.7557 - val_mae: 606.0626 - val_mse: 1034813.3125 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 2.5801 - mae: 356.9184 - mse: 663156.6875 - val_loss: 1.7824 - val_mae: 611.3124 - val_mse: 1023067.6875 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 2.5756 - mae: 356.3024 - mse: 659309.8125 - val_loss: 1.7561 - val_mae: 606.2761 - val_mse: 1035780.1250 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 2.5809 - mae: 357.0094 - mse: 663513.8125 - val_loss: 1.7819 - val_mae: 611.2018 - val_mse: 1023051.3125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 2.5754 - mae: 356.3151 - mse: 659184.8125 - val_loss: 1.7561 - val_mae: 606.2666 - val_mse: 1035712.7500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 2.5814 - mae: 357.2368 - mse: 664386.2500 - val_loss: 1.7873 - val_mae: 612.6144 - val_mse: 1023632.6875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 2.5768 - mae: 356.4545 - mse: 659075.4375 - val_loss: 1.7567 - val_mae: 606.5845 - val_mse: 1037053.9375 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 2.5834 - mae: 357.6021 - mse: 665761.2500 - val_loss: 1.7888 - val_mae: 613.0463 - val_mse: 1023858.8125 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 2.5778 - mae: 356.7246 - mse: 659143.1250 - val_loss: 1.7577 - val_mae: 607.0375 - val_mse: 1038573.5000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 2.5862 - mae: 358.1479 - mse: 667711.0000 - val_loss: 1.7964 - val_mae: 615.1108 - val_mse: 1024969.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 2.5801 - mae: 357.0195 - mse: 659356.3750 - val_loss: 1.7599 - val_mae: 607.9311 - val_mse: 1041393.0000 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 2.5900 - mae: 358.6906 - mse: 669625.0625 - val_loss: 1.8028 - val_mae: 616.8528 - val_mse: 1026078.0000 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 2.5834 - mae: 357.6281 - mse: 659965.8125 - val_loss: 1.7601 - val_mae: 608.0076 - val_mse: 1041638.1875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 2.5941 - mae: 359.5497 - mse: 672463.0625 - val_loss: 1.8059 - val_mae: 617.7182 - val_mse: 1026684.8125 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 2.5864 - mae: 358.2563 - mse: 660726.1875 - val_loss: 1.7616 - val_mae: 608.6206 - val_mse: 1043540.8750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 2.5992 - mae: 360.6023 - mse: 675693.9375 - val_loss: 1.8097 - val_mae: 618.7597 - val_mse: 1027461.6250 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 2.5907 - mae: 359.2022 - mse: 662003.6875 - val_loss: 1.7647 - val_mae: 609.7834 - val_mse: 1046918.5000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 2.6064 - mae: 361.8921 - mse: 679499.3125 - val_loss: 1.8216 - val_mae: 622.0459 - val_mse: 1030047.6250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 2.5971 - mae: 360.3651 - mse: 663752.6875 - val_loss: 1.7719 - val_mae: 612.3453 - val_mse: 1053597.1250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 2.6152 - mae: 363.2666 - mse: 683514.3750 - val_loss: 1.8374 - val_mae: 626.4059 - val_mse: 1034023.0625 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 2.6039 - mae: 361.3549 - mse: 665420.7500 - val_loss: 1.7843 - val_mae: 616.5777 - val_mse: 1063465.1250 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 2.6258 - mae: 364.7887 - mse: 688097.1250 - val_loss: 1.8639 - val_mae: 633.7222 - val_mse: 1041852.8125 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 2.6157 - mae: 362.9259 - mse: 668481.2500 - val_loss: 1.8056 - val_mae: 623.7471 - val_mse: 1078789.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 2.6417 - mae: 366.8850 - mse: 694181.5625 - val_loss: 1.8924 - val_mae: 641.6416 - val_mse: 1051450.0000 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 2.6266 - mae: 363.9048 - mse: 670465.0625 - val_loss: 1.8318 - val_mae: 632.6533 - val_mse: 1096378.3750 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 2.6529 - mae: 367.5369 - mse: 696206.5625 - val_loss: 1.9318 - val_mae: 652.8641 - val_mse: 1066213.1250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 2.6348 - mae: 363.2706 - mse: 669338.8125 - val_loss: 1.8550 - val_mae: 640.6922 - val_mse: 1111752.8750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 2.6530 - mae: 365.4008 - mse: 690179.8125 - val_loss: 1.9445 - val_mae: 656.5337 - val_mse: 1071357.6250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 2.6353 - mae: 361.9966 - mse: 666751.8125 - val_loss: 1.8741 - val_mae: 647.2120 - val_mse: 1124610.3750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 2.6536 - mae: 363.3982 - mse: 683918.1250 - val_loss: 1.9644 - val_mae: 662.2433 - val_mse: 1079868.6250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 2.6392 - mae: 360.3981 - mse: 663672.4375 - val_loss: 1.8625 - val_mae: 643.2974 - val_mse: 1116947.3750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 2.6433 - mae: 361.5427 - mse: 678074.8125 - val_loss: 1.9441 - val_mae: 656.4104 - val_mse: 1071170.7500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 2.6299 - mae: 358.9960 - mse: 661482.5625 - val_loss: 1.8395 - val_mae: 635.3507 - val_mse: 1101578.5000 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 2.6296 - mae: 360.2403 - mse: 674091.1250 - val_loss: 1.9162 - val_mae: 648.4105 - val_mse: 1060206.0000 - lr: 0.0010 - 67ms/epoch - 22ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 2.6189 - mae: 358.1617 - mse: 660366.9375 - val_loss: 1.8214 - val_mae: 629.1606 - val_mse: 1089645.3750 - lr: 0.0010 - 56ms/epoch - 19ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 2.6188 - mae: 359.1551 - mse: 670565.3750 - val_loss: 1.8948 - val_mae: 642.3425 - val_mse: 1052298.3750 - lr: 0.0010 - 63ms/epoch - 21ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 2.6107 - mae: 357.6268 - mse: 659721.3125 - val_loss: 1.8020 - val_mae: 622.5682 - val_mse: 1076632.1250 - lr: 0.0010 - 64ms/epoch - 21ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 2.6090 - mae: 358.7299 - mse: 669296.8125 - val_loss: 1.8749 - val_mae: 636.7755 - val_mse: 1045437.8125 - lr: 0.0010 - 62ms/epoch - 21ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 2.6035 - mae: 357.5030 - mse: 659623.0625 - val_loss: 1.7933 - val_mae: 619.6899 - val_mse: 1070604.5000 - lr: 0.0010 - 62ms/epoch - 21ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 2.6047 - mae: 358.6356 - mse: 669174.2500 - val_loss: 1.8624 - val_mae: 633.3284 - val_mse: 1041363.9375 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 2.5991 - mae: 357.6038 - mse: 659775.6250 - val_loss: 1.7902 - val_mae: 618.6514 - val_mse: 1068361.8750 - lr: 0.0010 - 56ms/epoch - 19ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 2.6042 - mae: 358.9227 - mse: 670376.0000 - val_loss: 1.8642 - val_mae: 633.8369 - val_mse: 1041940.3750 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 2.5994 - mae: 357.7201 - mse: 659923.0000 - val_loss: 1.7928 - val_mae: 619.5464 - val_mse: 1070329.8750 - lr: 0.0010 - 64ms/epoch - 21ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 2.6062 - mae: 359.1037 - mse: 671002.4375 - val_loss: 1.8691 - val_mae: 635.2124 - val_mse: 1043548.1875 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 2.6011 - mae: 357.9016 - mse: 660148.2500 - val_loss: 1.7990 - val_mae: 621.6276 - val_mse: 1074767.7500 - lr: 0.0010 - 66ms/epoch - 22ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 2.6101 - mae: 359.3746 - mse: 671881.0000 - val_loss: 1.8795 - val_mae: 638.0955 - val_mse: 1047032.7500 - lr: 0.0010 - 76ms/epoch - 25ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 2.6043 - mae: 358.0080 - mse: 660257.3750 - val_loss: 1.8046 - val_mae: 623.4708 - val_mse: 1078566.8750 - lr: 0.0010 - 57ms/epoch - 19ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 2.6134 - mae: 359.6238 - mse: 672632.9375 - val_loss: 1.8824 - val_mae: 638.9111 - val_mse: 1048012.9375 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 2.6052 - mae: 358.0964 - mse: 660369.3750 - val_loss: 1.8062 - val_mae: 624.0470 - val_mse: 1079729.5000 - lr: 0.0010 - 60ms/epoch - 20ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 2.6143 - mae: 359.6545 - mse: 672714.6250 - val_loss: 1.8846 - val_mae: 639.5174 - val_mse: 1048748.3750 - lr: 0.0010 - 74ms/epoch - 25ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 2.6058 - mae: 358.0662 - mse: 660307.0625 - val_loss: 1.8091 - val_mae: 625.0398 - val_mse: 1081711.8750 - lr: 0.0010 - 61ms/epoch - 20ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 2.6153 - mae: 359.5575 - mse: 672360.6250 - val_loss: 1.8849 - val_mae: 639.5964 - val_mse: 1048843.0000 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 2.6060 - mae: 358.0681 - mse: 660296.5625 - val_loss: 1.8103 - val_mae: 625.4545 - val_mse: 1082557.5000 - lr: 0.0010 - 71ms/epoch - 24ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 2.6155 - mae: 359.5192 - mse: 672220.0625 - val_loss: 1.8868 - val_mae: 640.1521 - val_mse: 1049524.1250 - lr: 0.0010 - 62ms/epoch - 21ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 2.6066 - mae: 357.9658 - mse: 660128.6875 - val_loss: 1.8091 - val_mae: 625.0417 - val_mse: 1081781.8750 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 2.6141 - mae: 359.2734 - mse: 671373.8750 - val_loss: 1.8853 - val_mae: 639.7268 - val_mse: 1048999.1250 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 2.6061 - mae: 357.8518 - mse: 659949.8125 - val_loss: 1.8080 - val_mae: 624.6909 - val_mse: 1081116.8750 - lr: 0.0010 - 61ms/epoch - 20ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 2.6129 - mae: 359.0702 - mse: 670636.0625 - val_loss: 1.8850 - val_mae: 639.6586 - val_mse: 1048919.2500 - lr: 0.0010 - 58ms/epoch - 19ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 2.6059 - mae: 357.6865 - mse: 659701.2500 - val_loss: 1.8026 - val_mae: 622.8530 - val_mse: 1077448.6250 - lr: 0.0010 - 57ms/epoch - 19ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 2.6098 - mae: 358.8744 - mse: 669998.1875 - val_loss: 1.8780 - val_mae: 637.7106 - val_mse: 1046568.0625 - lr: 0.0010 - 61ms/epoch - 20ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 2.6034 - mae: 357.6005 - mse: 659597.5000 - val_loss: 1.7979 - val_mae: 621.3073 - val_mse: 1074276.2500 - lr: 0.0010 - 72ms/epoch - 24ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 2.6073 - mae: 358.7421 - mse: 669586.8750 - val_loss: 1.8698 - val_mae: 635.4526 - val_mse: 1043885.7500 - lr: 0.0010 - 65ms/epoch - 22ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 2.6004 - mae: 357.5170 - mse: 659500.6875 - val_loss: 1.7930 - val_mae: 619.6935 - val_mse: 1070849.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 2.6053 - mae: 358.8262 - mse: 670008.2500 - val_loss: 1.8679 - val_mae: 634.9272 - val_mse: 1043287.1875 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 2.5995 - mae: 357.5589 - mse: 659550.5000 - val_loss: 1.7928 - val_mae: 619.6466 - val_mse: 1070754.0000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 2.6054 - mae: 358.8761 - mse: 670208.2500 - val_loss: 1.8667 - val_mae: 634.6199 - val_mse: 1042946.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 2.5987 - mae: 357.5331 - mse: 659506.3125 - val_loss: 1.7911 - val_mae: 619.0742 - val_mse: 1069503.2500 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 2.6049 - mae: 358.9491 - mse: 670504.5000 - val_loss: 1.8672 - val_mae: 634.7617 - val_mse: 1043117.5000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 2.5987 - mae: 357.5826 - mse: 659557.3750 - val_loss: 1.7927 - val_mae: 619.6226 - val_mse: 1070706.2500 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 2.6060 - mae: 358.9885 - mse: 670628.4375 - val_loss: 1.8679 - val_mae: 634.9553 - val_mse: 1043349.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 2.5984 - mae: 357.5132 - mse: 659448.2500 - val_loss: 1.7909 - val_mae: 619.0218 - val_mse: 1069390.1250 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 2.6050 - mae: 358.8933 - mse: 670304.1250 - val_loss: 1.8631 - val_mae: 633.6389 - val_mse: 1041817.1875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 2.5964 - mae: 357.3578 - mse: 659243.0625 - val_loss: 1.7876 - val_mae: 617.9170 - val_mse: 1066938.3750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 2.6035 - mae: 358.8573 - mse: 670220.6875 - val_loss: 1.8590 - val_mae: 632.5222 - val_mse: 1040563.7500 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 2.5949 - mae: 357.3152 - mse: 659175.3750 - val_loss: 1.7868 - val_mae: 617.6520 - val_mse: 1066322.7500 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 2.6031 - mae: 358.8694 - mse: 670271.2500 - val_loss: 1.8591 - val_mae: 632.5624 - val_mse: 1040614.8750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 2.5950 - mae: 357.3461 - mse: 659193.2500 - val_loss: 1.7859 - val_mae: 617.3561 - val_mse: 1065630.3750 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 2.6026 - mae: 358.8483 - mse: 670205.1250 - val_loss: 1.8568 - val_mae: 631.9375 - val_mse: 1039930.6875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 2.5942 - mae: 357.3147 - mse: 659135.4375 - val_loss: 1.7847 - val_mae: 616.9691 - val_mse: 1064735.5000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 2.6016 - mae: 358.7417 - mse: 669834.6250 - val_loss: 1.8517 - val_mae: 630.5400 - val_mse: 1038412.5625 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 2.5929 - mae: 357.3232 - mse: 659132.2500 - val_loss: 1.7842 - val_mae: 616.8035 - val_mse: 1064337.5000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 2.6015 - mae: 358.8403 - mse: 670187.8125 - val_loss: 1.8518 - val_mae: 630.5856 - val_mse: 1038466.6875 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 2.5928 - mae: 357.3306 - mse: 659124.1875 - val_loss: 1.7832 - val_mae: 616.4706 - val_mse: 1063534.8750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 119: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 2.6004 - mae: 358.6854 - mse: 669647.8125 - val_loss: 1.8473 - val_mae: 629.3627 - val_mse: 1037176.1250 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 2.6037 - mae: 359.0494 - mse: 669216.3750 - val_loss: 1.7982 - val_mae: 615.7996 - val_mse: 1025791.8125 - lr: 1.0000e-04 - 60ms/epoch - 20ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 2.5770 - mae: 356.2033 - mse: 660106.8125 - val_loss: 1.7560 - val_mae: 605.4211 - val_mse: 1026675.8125 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 2.5734 - mae: 356.2161 - mse: 657900.8750 - val_loss: 1.7564 - val_mae: 606.9008 - val_mse: 1038286.4375 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 2.5835 - mae: 356.6273 - mse: 657978.0000 - val_loss: 1.7549 - val_mae: 606.1464 - val_mse: 1034958.5625 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 2.5771 - mae: 356.0117 - mse: 658294.5625 - val_loss: 1.7584 - val_mae: 605.8734 - val_mse: 1025284.3125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 2.5712 - mae: 356.2774 - mse: 660835.3750 - val_loss: 1.7751 - val_mae: 609.7294 - val_mse: 1023250.6250 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 2.5743 - mae: 356.7221 - mse: 662473.0625 - val_loss: 1.7781 - val_mae: 610.4549 - val_mse: 1023377.6250 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 2.5734 - mae: 356.3526 - mse: 661016.8750 - val_loss: 1.7661 - val_mae: 607.5918 - val_mse: 1023550.0000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 2.5703 - mae: 355.9575 - mse: 659070.7500 - val_loss: 1.7567 - val_mae: 605.5331 - val_mse: 1026204.8750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 2.5710 - mae: 355.8980 - mse: 658339.8125 - val_loss: 1.7551 - val_mae: 605.2933 - val_mse: 1027783.8750 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 2.5718 - mae: 355.9228 - mse: 658514.5625 - val_loss: 1.7571 - val_mae: 605.6027 - val_mse: 1025990.8750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 2.5706 - mae: 355.9883 - mse: 659271.6250 - val_loss: 1.7621 - val_mae: 606.6828 - val_mse: 1024151.1250 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 2.5706 - mae: 356.1216 - mse: 660088.1250 - val_loss: 1.7660 - val_mae: 607.5552 - val_mse: 1023575.5000 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 2.5708 - mae: 356.1311 - mse: 660125.4375 - val_loss: 1.7644 - val_mae: 607.2026 - val_mse: 1023768.4375 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 2.5704 - mae: 356.0220 - mse: 659532.3750 - val_loss: 1.7604 - val_mae: 606.3029 - val_mse: 1024580.0625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9461 - mse: 659012.7500 - val_loss: 1.7581 - val_mae: 605.8152 - val_mse: 1025440.6875 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 2.5704 - mae: 355.9399 - mse: 658913.1875 - val_loss: 1.7585 - val_mae: 605.8984 - val_mse: 1025265.1875 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 2.5703 - mae: 355.9720 - mse: 659188.1875 - val_loss: 1.7605 - val_mae: 606.3260 - val_mse: 1024557.1250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 2.5703 - mae: 356.0191 - mse: 659529.1250 - val_loss: 1.7621 - val_mae: 606.6849 - val_mse: 1024162.5000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 2.5703 - mae: 356.0314 - mse: 659608.3125 - val_loss: 1.7619 - val_mae: 606.6486 - val_mse: 1024198.4375 - lr: 1.0000e-04 - 43ms/epoch - 14ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 356.0041 - mse: 659426.6875 - val_loss: 1.7606 - val_mae: 606.3500 - val_mse: 1024532.6875 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9685 - mse: 659185.8750 - val_loss: 1.7593 - val_mae: 606.0743 - val_mse: 1024930.8125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9601 - mse: 659118.4375 - val_loss: 1.7595 - val_mae: 606.1096 - val_mse: 1024874.5000 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9768 - mse: 659248.8125 - val_loss: 1.7604 - val_mae: 606.3033 - val_mse: 1024597.1875 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9982 - mse: 659396.8750 - val_loss: 1.7611 - val_mae: 606.4719 - val_mse: 1024392.8750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 356.0034 - mse: 659431.3125 - val_loss: 1.7611 - val_mae: 606.4554 - val_mse: 1024414.3125 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9898 - mse: 659340.5000 - val_loss: 1.7604 - val_mae: 606.3074 - val_mse: 1024597.3750 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9777 - mse: 659260.8750 - val_loss: 1.7601 - val_mae: 606.2336 - val_mse: 1024699.8750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9753 - mse: 659245.2500 - val_loss: 1.7601 - val_mae: 606.2413 - val_mse: 1024690.7500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9806 - mse: 659282.1875 - val_loss: 1.7604 - val_mae: 606.3098 - val_mse: 1024600.0000 - lr: 1.0000e-04 - 57ms/epoch - 19ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9896 - mse: 659345.5000 - val_loss: 1.7608 - val_mae: 606.4014 - val_mse: 1024488.0625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9937 - mse: 659374.3750 - val_loss: 1.7608 - val_mae: 606.3984 - val_mse: 1024493.5625 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9863 - mse: 659325.7500 - val_loss: 1.7605 - val_mae: 606.3279 - val_mse: 1024582.8750 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9785 - mse: 659274.0000 - val_loss: 1.7602 - val_mae: 606.2655 - val_mse: 1024666.8750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9774 - mse: 659267.3125 - val_loss: 1.7603 - val_mae: 606.2846 - val_mse: 1024643.1875 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 2.5702 - mae: 355.9839 - mse: 659312.5625 - val_loss: 1.7606 - val_mae: 606.3651 - val_mse: 1024542.8125 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9898 - mse: 659355.3125 - val_loss: 1.7609 - val_mae: 606.4119 - val_mse: 1024486.4375 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9898 - mse: 659355.6250 - val_loss: 1.7607 - val_mae: 606.3878 - val_mse: 1024518.1250 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9830 - mse: 659311.1250 - val_loss: 1.7605 - val_mae: 606.3229 - val_mse: 1024601.7500 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9819 - mse: 659304.8125 - val_loss: 1.7605 - val_mae: 606.3436 - val_mse: 1024577.0000 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9825 - mse: 659310.1250 - val_loss: 1.7606 - val_mae: 606.3467 - val_mse: 1024574.7500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9814 - mse: 659304.1250 - val_loss: 1.7605 - val_mae: 606.3365 - val_mse: 1024588.8125 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9854 - mse: 659332.7500 - val_loss: 1.7608 - val_mae: 606.3984 - val_mse: 1024514.0000 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9886 - mse: 659356.3750 - val_loss: 1.7608 - val_mae: 606.3984 - val_mse: 1024516.1250 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9833 - mse: 659322.1250 - val_loss: 1.7606 - val_mae: 606.3545 - val_mse: 1024572.0000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9802 - mse: 659301.5000 - val_loss: 1.7605 - val_mae: 606.3384 - val_mse: 1024593.8750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9836 - mse: 659326.9375 - val_loss: 1.7608 - val_mae: 606.3972 - val_mse: 1024522.5000 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9868 - mse: 659349.0625 - val_loss: 1.7608 - val_mae: 606.4082 - val_mse: 1024510.6250 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9843 - mse: 659333.3750 - val_loss: 1.7607 - val_mae: 606.3755 - val_mse: 1024551.6875 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9809 - mse: 659312.8750 - val_loss: 1.7606 - val_mae: 606.3592 - val_mse: 1024573.9375 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9803 - mse: 659309.6250 - val_loss: 1.7606 - val_mae: 606.3632 - val_mse: 1024570.5625 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9815 - mse: 659319.5625 - val_loss: 1.7607 - val_mae: 606.3823 - val_mse: 1024548.1250 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9841 - mse: 659339.0625 - val_loss: 1.7608 - val_mae: 606.4124 - val_mse: 1024513.0625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9845 - mse: 659342.5625 - val_loss: 1.7608 - val_mae: 606.4018 - val_mse: 1024527.6250 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9832 - mse: 659335.7500 - val_loss: 1.7608 - val_mae: 606.4016 - val_mse: 1024529.1250 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9829 - mse: 659335.0625 - val_loss: 1.7608 - val_mae: 606.3971 - val_mse: 1024536.1250 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9821 - mse: 659332.3125 - val_loss: 1.7608 - val_mae: 606.4015 - val_mse: 1024532.2500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9824 - mse: 659334.6250 - val_loss: 1.7608 - val_mae: 606.4006 - val_mse: 1024535.5625 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9819 - mse: 659333.8125 - val_loss: 1.7608 - val_mae: 606.4074 - val_mse: 1024528.3125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9825 - mse: 659337.5625 - val_loss: 1.7607 - val_mae: 606.3934 - val_mse: 1024547.1875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9778 - mse: 659308.1875 - val_loss: 1.7606 - val_mae: 606.3668 - val_mse: 1024581.8750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9807 - mse: 659329.5625 - val_loss: 1.7609 - val_mae: 606.4217 - val_mse: 1024516.0000 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9856 - mse: 659363.6250 - val_loss: 1.7610 - val_mae: 606.4427 - val_mse: 1024492.3125 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9807 - mse: 659332.0625 - val_loss: 1.7607 - val_mae: 606.3759 - val_mse: 1024575.5625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9712 - mse: 659269.2500 - val_loss: 1.7604 - val_mae: 606.3105 - val_mse: 1024660.0625 - lr: 1.0000e-04 - 43ms/epoch - 14ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9729 - mse: 659281.8750 - val_loss: 1.7606 - val_mae: 606.3677 - val_mse: 1024589.0625 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 2.5701 - mae: 355.9836 - mse: 659357.1250 - val_loss: 1.7611 - val_mae: 606.4711 - val_mse: 1024465.0000 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9878 - mse: 659385.5000 - val_loss: 1.7611 - val_mae: 606.4725 - val_mse: 1024465.2500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9808 - mse: 659340.3125 - val_loss: 1.7607 - val_mae: 606.3896 - val_mse: 1024567.0000 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9697 - mse: 659266.9375 - val_loss: 1.7604 - val_mae: 606.3126 - val_mse: 1024665.8125 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9705 - mse: 659273.1250 - val_loss: 1.7606 - val_mae: 606.3627 - val_mse: 1024603.8125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9819 - mse: 659352.3750 - val_loss: 1.7611 - val_mae: 606.4709 - val_mse: 1024474.2500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9875 - mse: 659391.8125 - val_loss: 1.7611 - val_mae: 606.4813 - val_mse: 1024463.1875 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9806 - mse: 659346.5000 - val_loss: 1.7608 - val_mae: 606.4095 - val_mse: 1024551.0000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9703 - mse: 659279.4375 - val_loss: 1.7605 - val_mae: 606.3397 - val_mse: 1024639.4375 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9720 - mse: 659291.7500 - val_loss: 1.7607 - val_mae: 606.3976 - val_mse: 1024568.7500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9796 - mse: 659344.3750 - val_loss: 1.7610 - val_mae: 606.4518 - val_mse: 1024505.3125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9799 - mse: 659348.3125 - val_loss: 1.7609 - val_mae: 606.4363 - val_mse: 1024525.2500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9737 - mse: 659308.1250 - val_loss: 1.7607 - val_mae: 606.3853 - val_mse: 1024589.3750 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9758 - mse: 659324.6875 - val_loss: 1.7609 - val_mae: 606.4340 - val_mse: 1024531.3125 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9769 - mse: 659333.1250 - val_loss: 1.7608 - val_mae: 606.4233 - val_mse: 1024545.9375 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9735 - mse: 659311.7500 - val_loss: 1.7607 - val_mae: 606.3911 - val_mse: 1024587.1250 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9742 - mse: 659318.6250 - val_loss: 1.7609 - val_mae: 606.4276 - val_mse: 1024544.6875 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9766 - mse: 659335.7500 - val_loss: 1.7609 - val_mae: 606.4395 - val_mse: 1024532.7500 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9753 - mse: 659328.5000 - val_loss: 1.7608 - val_mae: 606.4233 - val_mse: 1024553.5000 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9724 - mse: 659312.3750 - val_loss: 1.7608 - val_mae: 606.4101 - val_mse: 1024571.9375 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9714 - mse: 659306.9375 - val_loss: 1.7608 - val_mae: 606.4085 - val_mse: 1024575.4375 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9715 - mse: 659308.9375 - val_loss: 1.7608 - val_mae: 606.4158 - val_mse: 1024569.0625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9723 - mse: 659316.6250 - val_loss: 1.7609 - val_mae: 606.4294 - val_mse: 1024553.6875 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9726 - mse: 659319.1875 - val_loss: 1.7608 - val_mae: 606.4243 - val_mse: 1024562.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9709 - mse: 659310.2500 - val_loss: 1.7608 - val_mae: 606.4187 - val_mse: 1024570.5625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9705 - mse: 659309.7500 - val_loss: 1.7608 - val_mae: 606.4234 - val_mse: 1024567.0000 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 2.5700 - mae: 355.9700 - mse: 659306.6250 - val_loss: 1.7608 - val_mae: 606.4114 - val_mse: 1024583.2500 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9701 - mse: 659310.3125 - val_loss: 1.7609 - val_mae: 606.4327 - val_mse: 1024559.0000 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9711 - mse: 659317.6250 - val_loss: 1.7609 - val_mae: 606.4332 - val_mse: 1024560.7500 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9687 - mse: 659302.8125 - val_loss: 1.7608 - val_mae: 606.4079 - val_mse: 1024593.1250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9701 - mse: 659315.1250 - val_loss: 1.7610 - val_mae: 606.4526 - val_mse: 1024541.3125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9733 - mse: 659337.5625 - val_loss: 1.7610 - val_mae: 606.4699 - val_mse: 1024522.8125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9724 - mse: 659333.1875 - val_loss: 1.7610 - val_mae: 606.4567 - val_mse: 1024540.4375 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 219: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9687 - mse: 659309.5625 - val_loss: 1.7608 - val_mae: 606.4216 - val_mse: 1024584.7500 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9586 - mse: 659256.3125 - val_loss: 1.7608 - val_mae: 606.4238 - val_mse: 1024582.5625 - lr: 1.0000e-05 - 53ms/epoch - 18ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9594 - mse: 659261.8750 - val_loss: 1.7609 - val_mae: 606.4340 - val_mse: 1024570.5625 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9606 - mse: 659270.8125 - val_loss: 1.7609 - val_mae: 606.4466 - val_mse: 1024555.1250 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9622 - mse: 659281.5625 - val_loss: 1.7610 - val_mae: 606.4595 - val_mse: 1024539.6875 - lr: 1.0000e-05 - 57ms/epoch - 19ms/step\n",
            "Epoch 224/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9634 - mse: 659289.8750 - val_loss: 1.7610 - val_mae: 606.4696 - val_mse: 1024528.0000 - lr: 1.0000e-05 - 48ms/epoch - 16ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9644 - mse: 659296.8125 - val_loss: 1.7611 - val_mae: 606.4780 - val_mse: 1024518.1250 - lr: 1.0000e-05 - 39ms/epoch - 13ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9652 - mse: 659302.5000 - val_loss: 1.7611 - val_mae: 606.4850 - val_mse: 1024510.1250 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9659 - mse: 659307.4375 - val_loss: 1.7611 - val_mae: 606.4911 - val_mse: 1024502.8750 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 228/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9664 - mse: 659311.6875 - val_loss: 1.7611 - val_mae: 606.4965 - val_mse: 1024497.1250 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 229/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9670 - mse: 659315.3125 - val_loss: 1.7612 - val_mae: 606.5013 - val_mse: 1024491.4375 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 230/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9675 - mse: 659318.8125 - val_loss: 1.7612 - val_mae: 606.5057 - val_mse: 1024486.7500 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 231/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9676 - mse: 659320.1250 - val_loss: 1.7612 - val_mae: 606.5065 - val_mse: 1024486.2500 - lr: 1.0000e-05 - 41ms/epoch - 14ms/step\n",
            "Epoch 232/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9674 - mse: 659319.1250 - val_loss: 1.7612 - val_mae: 606.5043 - val_mse: 1024488.6875 - lr: 1.0000e-05 - 41ms/epoch - 14ms/step\n",
            "Epoch 233/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9673 - mse: 659318.1250 - val_loss: 1.7612 - val_mae: 606.5038 - val_mse: 1024489.8125 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 234/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9672 - mse: 659318.0000 - val_loss: 1.7612 - val_mae: 606.5042 - val_mse: 1024488.8750 - lr: 1.0000e-05 - 39ms/epoch - 13ms/step\n",
            "Epoch 235/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9673 - mse: 659318.7500 - val_loss: 1.7612 - val_mae: 606.5056 - val_mse: 1024487.6875 - lr: 1.0000e-05 - 39ms/epoch - 13ms/step\n",
            "Epoch 236/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9674 - mse: 659319.8125 - val_loss: 1.7612 - val_mae: 606.5075 - val_mse: 1024486.0000 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 237/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9677 - mse: 659321.5000 - val_loss: 1.7612 - val_mae: 606.5096 - val_mse: 1024482.9375 - lr: 1.0000e-05 - 43ms/epoch - 14ms/step\n",
            "Epoch 238/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9676 - mse: 659321.4375 - val_loss: 1.7612 - val_mae: 606.5087 - val_mse: 1024484.7500 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 239/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9676 - mse: 659321.1250 - val_loss: 1.7612 - val_mae: 606.5089 - val_mse: 1024484.5625 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 240/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9676 - mse: 659321.4375 - val_loss: 1.7612 - val_mae: 606.5099 - val_mse: 1024483.3125 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 241/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9674 - mse: 659320.7500 - val_loss: 1.7612 - val_mae: 606.5082 - val_mse: 1024485.7500 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 242/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9672 - mse: 659319.9375 - val_loss: 1.7612 - val_mae: 606.5078 - val_mse: 1024486.3750 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 243/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9673 - mse: 659320.0000 - val_loss: 1.7612 - val_mae: 606.5085 - val_mse: 1024486.0000 - lr: 1.0000e-05 - 43ms/epoch - 14ms/step\n",
            "Epoch 244/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9673 - mse: 659320.7500 - val_loss: 1.7612 - val_mae: 606.5098 - val_mse: 1024484.1250 - lr: 1.0000e-05 - 42ms/epoch - 14ms/step\n",
            "Epoch 245/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9675 - mse: 659322.0625 - val_loss: 1.7612 - val_mae: 606.5119 - val_mse: 1024482.5625 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 246/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9677 - mse: 659323.8125 - val_loss: 1.7612 - val_mae: 606.5142 - val_mse: 1024479.6250 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 247/300\n",
            "3/3 - 0s - loss: 2.5699 - mae: 355.9677 - mse: 659323.6250 - val_loss: 1.7612 - val_mae: 606.5134 - val_mse: 1024481.1875 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 248/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9673 - mse: 659321.5000 - val_loss: 1.7612 - val_mae: 606.5099 - val_mse: 1024485.3750 - lr: 1.0000e-05 - 53ms/epoch - 18ms/step\n",
            "Epoch 249/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9670 - mse: 659319.6875 - val_loss: 1.7612 - val_mae: 606.5083 - val_mse: 1024487.2500 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 250/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9669 - mse: 659318.8125 - val_loss: 1.7612 - val_mae: 606.5079 - val_mse: 1024487.5625 - lr: 1.0000e-05 - 40ms/epoch - 13ms/step\n",
            "Epoch 251/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9669 - mse: 659319.1250 - val_loss: 1.7612 - val_mae: 606.5089 - val_mse: 1024487.1875 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 252/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9670 - mse: 659319.9375 - val_loss: 1.7612 - val_mae: 606.5102 - val_mse: 1024485.8125 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 253/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9672 - mse: 659321.2500 - val_loss: 1.7612 - val_mae: 606.5123 - val_mse: 1024483.3750 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 254/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9673 - mse: 659322.6875 - val_loss: 1.7612 - val_mae: 606.5146 - val_mse: 1024480.8750 - lr: 1.0000e-05 - 40ms/epoch - 13ms/step\n",
            "Epoch 255/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659324.6250 - val_loss: 1.7612 - val_mae: 606.5172 - val_mse: 1024477.8750 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 256/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659324.6875 - val_loss: 1.7612 - val_mae: 606.5162 - val_mse: 1024479.5000 - lr: 1.0000e-05 - 42ms/epoch - 14ms/step\n",
            "Epoch 257/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659324.3125 - val_loss: 1.7612 - val_mae: 606.5166 - val_mse: 1024479.5000 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 258/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659325.0000 - val_loss: 1.7612 - val_mae: 606.5178 - val_mse: 1024478.4375 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 259/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659326.0000 - val_loss: 1.7612 - val_mae: 606.5197 - val_mse: 1024476.3750 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 260/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659325.4375 - val_loss: 1.7612 - val_mae: 606.5182 - val_mse: 1024478.1250 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 261/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659325.0000 - val_loss: 1.7612 - val_mae: 606.5181 - val_mse: 1024478.3125 - lr: 1.0000e-05 - 40ms/epoch - 13ms/step\n",
            "Epoch 262/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659325.1250 - val_loss: 1.7612 - val_mae: 606.5189 - val_mse: 1024477.5000 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 263/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659326.0000 - val_loss: 1.7612 - val_mae: 606.5203 - val_mse: 1024476.1875 - lr: 1.0000e-05 - 48ms/epoch - 16ms/step\n",
            "Epoch 264/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659327.2500 - val_loss: 1.7613 - val_mae: 606.5226 - val_mse: 1024474.4375 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 265/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659326.8750 - val_loss: 1.7612 - val_mae: 606.5210 - val_mse: 1024476.0625 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 266/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659326.1875 - val_loss: 1.7612 - val_mae: 606.5209 - val_mse: 1024476.5000 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 267/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659326.6875 - val_loss: 1.7612 - val_mae: 606.5220 - val_mse: 1024475.4375 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 268/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659327.7500 - val_loss: 1.7613 - val_mae: 606.5238 - val_mse: 1024473.7500 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 269/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659327.0000 - val_loss: 1.7612 - val_mae: 606.5220 - val_mse: 1024475.6875 - lr: 1.0000e-05 - 53ms/epoch - 18ms/step\n",
            "Epoch 270/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9673 - mse: 659326.1875 - val_loss: 1.7612 - val_mae: 606.5215 - val_mse: 1024475.8125 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 271/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9674 - mse: 659326.3750 - val_loss: 1.7613 - val_mae: 606.5225 - val_mse: 1024476.2500 - lr: 1.0000e-05 - 41ms/epoch - 14ms/step\n",
            "Epoch 272/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9674 - mse: 659327.2500 - val_loss: 1.7613 - val_mae: 606.5239 - val_mse: 1024474.1875 - lr: 1.0000e-05 - 39ms/epoch - 13ms/step\n",
            "Epoch 273/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659328.4375 - val_loss: 1.7613 - val_mae: 606.5261 - val_mse: 1024471.9375 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 274/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9678 - mse: 659330.1875 - val_loss: 1.7613 - val_mae: 606.5286 - val_mse: 1024469.6875 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 275/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9678 - mse: 659329.9375 - val_loss: 1.7613 - val_mae: 606.5274 - val_mse: 1024471.1250 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 276/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659328.4375 - val_loss: 1.7613 - val_mae: 606.5254 - val_mse: 1024473.2500 - lr: 1.0000e-05 - 39ms/epoch - 13ms/step\n",
            "Epoch 277/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9674 - mse: 659327.5625 - val_loss: 1.7613 - val_mae: 606.5250 - val_mse: 1024474.1875 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 278/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9673 - mse: 659327.8125 - val_loss: 1.7613 - val_mae: 606.5259 - val_mse: 1024473.1875 - lr: 1.0000e-05 - 38ms/epoch - 13ms/step\n",
            "Epoch 279/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659328.7500 - val_loss: 1.7613 - val_mae: 606.5276 - val_mse: 1024471.5625 - lr: 1.0000e-05 - 43ms/epoch - 14ms/step\n",
            "Epoch 280/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659330.2500 - val_loss: 1.7613 - val_mae: 606.5301 - val_mse: 1024469.5000 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 281/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659329.8750 - val_loss: 1.7613 - val_mae: 606.5286 - val_mse: 1024471.1250 - lr: 1.0000e-05 - 55ms/epoch - 18ms/step\n",
            "Epoch 282/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9674 - mse: 659329.1875 - val_loss: 1.7613 - val_mae: 606.5283 - val_mse: 1024471.1250 - lr: 1.0000e-05 - 72ms/epoch - 24ms/step\n",
            "Epoch 283/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9674 - mse: 659329.5000 - val_loss: 1.7613 - val_mae: 606.5296 - val_mse: 1024470.3750 - lr: 1.0000e-05 - 62ms/epoch - 21ms/step\n",
            "Epoch 284/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659330.6875 - val_loss: 1.7613 - val_mae: 606.5316 - val_mse: 1024468.9375 - lr: 1.0000e-05 - 47ms/epoch - 16ms/step\n",
            "Epoch 285/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659332.1875 - val_loss: 1.7613 - val_mae: 606.5338 - val_mse: 1024466.0625 - lr: 1.0000e-05 - 46ms/epoch - 15ms/step\n",
            "Epoch 286/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659332.0000 - val_loss: 1.7613 - val_mae: 606.5322 - val_mse: 1024467.9375 - lr: 1.0000e-05 - 51ms/epoch - 17ms/step\n",
            "Epoch 287/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659331.1875 - val_loss: 1.7613 - val_mae: 606.5321 - val_mse: 1024468.0625 - lr: 1.0000e-05 - 66ms/epoch - 22ms/step\n",
            "Epoch 288/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659331.5000 - val_loss: 1.7613 - val_mae: 606.5333 - val_mse: 1024467.3125 - lr: 1.0000e-05 - 46ms/epoch - 15ms/step\n",
            "Epoch 289/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659332.6875 - val_loss: 1.7613 - val_mae: 606.5351 - val_mse: 1024465.4375 - lr: 1.0000e-05 - 67ms/epoch - 22ms/step\n",
            "Epoch 290/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659332.0000 - val_loss: 1.7613 - val_mae: 606.5334 - val_mse: 1024467.7500 - lr: 1.0000e-05 - 53ms/epoch - 18ms/step\n",
            "Epoch 291/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659331.3125 - val_loss: 1.7613 - val_mae: 606.5331 - val_mse: 1024467.7500 - lr: 1.0000e-05 - 63ms/epoch - 21ms/step\n",
            "Epoch 292/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9674 - mse: 659331.5000 - val_loss: 1.7613 - val_mae: 606.5342 - val_mse: 1024467.5000 - lr: 1.0000e-05 - 54ms/epoch - 18ms/step\n",
            "Epoch 293/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659332.5625 - val_loss: 1.7613 - val_mae: 606.5358 - val_mse: 1024465.5000 - lr: 1.0000e-05 - 64ms/epoch - 21ms/step\n",
            "Epoch 294/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659334.0000 - val_loss: 1.7613 - val_mae: 606.5381 - val_mse: 1024463.6250 - lr: 1.0000e-05 - 50ms/epoch - 17ms/step\n",
            "Epoch 295/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659333.5625 - val_loss: 1.7613 - val_mae: 606.5364 - val_mse: 1024465.2500 - lr: 1.0000e-05 - 60ms/epoch - 20ms/step\n",
            "Epoch 296/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659332.6250 - val_loss: 1.7613 - val_mae: 606.5366 - val_mse: 1024466.4375 - lr: 1.0000e-05 - 50ms/epoch - 17ms/step\n",
            "Epoch 297/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9675 - mse: 659333.0000 - val_loss: 1.7613 - val_mae: 606.5375 - val_mse: 1024464.8750 - lr: 1.0000e-05 - 61ms/epoch - 20ms/step\n",
            "Epoch 298/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9676 - mse: 659334.1250 - val_loss: 1.7613 - val_mae: 606.5392 - val_mse: 1024462.8750 - lr: 1.0000e-05 - 73ms/epoch - 24ms/step\n",
            "Epoch 299/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9678 - mse: 659335.5625 - val_loss: 1.7613 - val_mae: 606.5417 - val_mse: 1024460.7500 - lr: 1.0000e-05 - 61ms/epoch - 20ms/step\n",
            "Epoch 300/300\n",
            "3/3 - 0s - loss: 2.5698 - mae: 355.9677 - mse: 659335.2500 - val_loss: 1.7613 - val_mae: 606.5402 - val_mse: 1024462.6250 - lr: 1.0000e-05 - 63ms/epoch - 21ms/step\n",
            "Optimizing model by reducing: mae for epochs: 300, num_iter: 2, model: dense_model\n",
            "Epoch 1/300\n",
            "3/3 - 1s - loss: 510.8044 - mae: 510.8044 - mse: 913004.6250 - val_loss: 1358.8966 - val_mae: 1358.8966 - val_mse: 3023526.5000 - lr: 0.0010 - 913ms/epoch - 304ms/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 1213.5562 - mae: 1213.5562 - mse: 4923986.0000 - val_loss: 2689.1924 - val_mae: 2689.1924 - val_mse: 9785247.0000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 617.5662 - mae: 617.5662 - mse: 1268856.1250 - val_loss: 610.1757 - val_mae: 610.1757 - val_mse: 1044108.1250 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 663.1617 - mae: 663.1617 - mse: 1897074.0000 - val_loss: 1755.0059 - val_mae: 1755.0059 - val_mse: 4561036.5000 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 699.6887 - mae: 699.6887 - mse: 1830795.2500 - val_loss: 1137.8402 - val_mae: 1137.8402 - val_mse: 2217183.2500 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 367.5055 - mae: 367.5055 - mse: 662519.7500 - val_loss: 803.1308 - val_mae: 803.1308 - val_mse: 1438043.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 540.7050 - mae: 540.7050 - mse: 1204330.7500 - val_loss: 1224.1323 - val_mae: 1224.1323 - val_mse: 2582328.0000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 440.7530 - mae: 440.7530 - mse: 821060.0625 - val_loss: 635.6922 - val_mae: 635.6922 - val_mse: 1104007.7500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 406.9383 - mae: 406.9383 - mse: 824952.7500 - val_loss: 851.9355 - val_mae: 851.9355 - val_mse: 1450428.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 429.0439 - mae: 429.0439 - mse: 858327.8125 - val_loss: 679.2230 - val_mae: 679.2230 - val_mse: 1105997.1250 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 364.3852 - mae: 364.3852 - mse: 671795.3125 - val_loss: 702.4365 - val_mae: 702.4365 - val_mse: 1233163.1250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 392.5447 - mae: 392.5447 - mse: 726281.8750 - val_loss: 657.1974 - val_mae: 657.1974 - val_mse: 1146027.7500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 357.9951 - mae: 357.9951 - mse: 668119.5000 - val_loss: 641.0724 - val_mae: 641.0724 - val_mse: 1048268.9375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 373.1623 - mae: 373.1623 - mse: 709034.1250 - val_loss: 619.6519 - val_mae: 619.6519 - val_mse: 1025486.3750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 359.4017 - mae: 359.4017 - mse: 659847.0625 - val_loss: 628.7884 - val_mae: 628.7884 - val_mse: 1090541.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 359.5243 - mae: 359.5243 - mse: 657306.3125 - val_loss: 602.8502 - val_mae: 602.8502 - val_mse: 1023343.1875 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 362.1042 - mae: 362.1042 - mse: 681588.8125 - val_loss: 624.3425 - val_mae: 624.3425 - val_mse: 1027461.4375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 354.2325 - mae: 354.2325 - mse: 654872.2500 - val_loss: 607.4502 - val_mae: 607.4502 - val_mse: 1039087.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 362.2131 - mae: 362.2131 - mse: 662461.8125 - val_loss: 610.9005 - val_mae: 610.9005 - val_mse: 1046097.4375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 355.9507 - mae: 355.9507 - mse: 662315.0000 - val_loss: 615.7829 - val_mae: 615.7829 - val_mse: 1018809.6875 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 355.9053 - mae: 355.9053 - mse: 660981.8125 - val_loss: 604.2932 - val_mae: 604.2932 - val_mse: 1018585.1875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 356.5472 - mae: 356.5472 - mse: 653844.3125 - val_loss: 610.1017 - val_mae: 610.1017 - val_mse: 1039628.1875 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 353.8144 - mae: 353.8144 - mse: 653283.5625 - val_loss: 606.2535 - val_mae: 606.2535 - val_mse: 1015270.4375 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 354.0602 - mae: 354.0602 - mse: 654960.4375 - val_loss: 605.0327 - val_mae: 605.0327 - val_mse: 1018852.1875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 354.3969 - mae: 354.3969 - mse: 650959.5000 - val_loss: 605.7930 - val_mae: 605.7930 - val_mse: 1024418.8125 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 354.0204 - mae: 354.0204 - mse: 654721.0000 - val_loss: 606.0580 - val_mae: 606.0580 - val_mse: 1015458.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 353.3314 - mae: 353.3314 - mse: 651682.6250 - val_loss: 605.9210 - val_mae: 605.9210 - val_mse: 1024528.2500 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 354.2455 - mae: 354.2455 - mse: 650778.1875 - val_loss: 605.2899 - val_mae: 605.2899 - val_mse: 1020224.6250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 354.9087 - mae: 354.9087 - mse: 658652.5000 - val_loss: 607.1037 - val_mae: 607.1037 - val_mse: 1015238.8750 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 353.1082 - mae: 353.1082 - mse: 650817.6875 - val_loss: 606.5870 - val_mae: 606.5870 - val_mse: 1028457.6875 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 353.8141 - mae: 353.8141 - mse: 650213.0625 - val_loss: 605.0796 - val_mae: 605.0796 - val_mse: 1018400.6875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 355.2850 - mae: 355.2850 - mse: 660092.1250 - val_loss: 606.8387 - val_mae: 606.8387 - val_mse: 1015553.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 353.2905 - mae: 353.2905 - mse: 649757.6875 - val_loss: 605.4121 - val_mae: 605.4121 - val_mse: 1024488.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 353.1670 - mae: 353.1670 - mse: 651701.8750 - val_loss: 604.7410 - val_mae: 604.7410 - val_mse: 1019248.3125 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 352.7466 - mae: 352.7466 - mse: 649692.6875 - val_loss: 604.7598 - val_mae: 604.7598 - val_mse: 1022196.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 352.7261 - mae: 352.7261 - mse: 650896.3750 - val_loss: 604.6530 - val_mae: 604.6530 - val_mse: 1021639.8750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 352.7989 - mae: 352.7989 - mse: 650350.6250 - val_loss: 604.5817 - val_mae: 604.5817 - val_mse: 1022095.0000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 353.0525 - mae: 353.0525 - mse: 652593.3750 - val_loss: 604.5118 - val_mae: 604.5118 - val_mse: 1021450.8125 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 353.0439 - mae: 353.0439 - mse: 650133.5625 - val_loss: 604.5096 - val_mae: 604.5096 - val_mse: 1019534.6250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 354.0131 - mae: 354.0131 - mse: 656204.7500 - val_loss: 604.9135 - val_mae: 604.9135 - val_mse: 1017455.1250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 353.4282 - mae: 353.4282 - mse: 649719.2500 - val_loss: 604.6804 - val_mae: 604.6804 - val_mse: 1021726.8750 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 354.1271 - mae: 354.1271 - mse: 656614.8125 - val_loss: 605.2421 - val_mae: 605.2421 - val_mse: 1016985.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 353.2604 - mae: 353.2604 - mse: 649677.3750 - val_loss: 604.9939 - val_mae: 604.9939 - val_mse: 1022890.8125 - lr: 0.0010 - 41ms/epoch - 14ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 353.5984 - mae: 353.5984 - mse: 654710.9375 - val_loss: 605.1909 - val_mae: 605.1909 - val_mse: 1017490.7500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 352.9467 - mae: 352.9467 - mse: 649424.5000 - val_loss: 604.8871 - val_mae: 604.8871 - val_mse: 1019815.4375 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 353.9022 - mae: 353.9022 - mse: 655630.7500 - val_loss: 605.0204 - val_mae: 605.0204 - val_mse: 1018098.1250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 353.7095 - mae: 353.7095 - mse: 649750.6875 - val_loss: 604.8278 - val_mae: 604.8278 - val_mse: 1020101.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 355.1589 - mae: 355.1589 - mse: 660484.1250 - val_loss: 605.0040 - val_mae: 605.0040 - val_mse: 1018024.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 355.9669 - mae: 355.9669 - mse: 653075.3125 - val_loss: 605.0378 - val_mae: 605.0378 - val_mse: 1023695.1875 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 357.2658 - mae: 357.2658 - mse: 667110.8125 - val_loss: 605.2750 - val_mae: 605.2750 - val_mse: 1017065.6250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 358.3098 - mae: 358.3098 - mse: 656609.8125 - val_loss: 604.8565 - val_mae: 604.8565 - val_mse: 1021817.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 362.8746 - mae: 362.8746 - mse: 683426.3125 - val_loss: 606.1490 - val_mae: 606.1490 - val_mse: 1016092.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 365.1766 - mae: 365.1766 - mse: 669705.8750 - val_loss: 607.8948 - val_mae: 607.8948 - val_mse: 1033800.8750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 364.8332 - mae: 364.8332 - mse: 689758.0625 - val_loss: 609.2639 - val_mae: 609.2639 - val_mse: 1015733.6875 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 363.0272 - mae: 363.0272 - mse: 665357.7500 - val_loss: 609.3633 - val_mae: 609.3633 - val_mse: 1037139.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 360.4320 - mae: 360.4320 - mse: 676757.4375 - val_loss: 607.8538 - val_mae: 607.8538 - val_mse: 1015535.1875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 358.9904 - mae: 358.9904 - mse: 658084.4375 - val_loss: 605.2977 - val_mae: 605.2977 - val_mse: 1023031.1250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 362.9579 - mae: 362.9579 - mse: 683737.2500 - val_loss: 605.8126 - val_mae: 605.8126 - val_mse: 1016455.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 366.6848 - mae: 366.6848 - mse: 672786.9375 - val_loss: 608.9525 - val_mae: 608.9525 - val_mse: 1036013.8750 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 365.5255 - mae: 365.5255 - mse: 691748.8750 - val_loss: 608.6219 - val_mae: 608.6219 - val_mse: 1015647.5000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 364.9908 - mae: 364.9908 - mse: 669242.4375 - val_loss: 609.1657 - val_mae: 609.1657 - val_mse: 1036355.8750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 363.1497 - mae: 363.1497 - mse: 684734.4375 - val_loss: 607.9401 - val_mae: 607.9401 - val_mse: 1015564.5625 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 362.9964 - mae: 362.9964 - mse: 665525.5000 - val_loss: 607.0585 - val_mae: 607.0585 - val_mse: 1030743.1875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 363.5345 - mae: 363.5345 - mse: 685736.5000 - val_loss: 607.5278 - val_mae: 607.5278 - val_mse: 1015646.4375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 364.0205 - mae: 364.0205 - mse: 667373.5000 - val_loss: 607.4697 - val_mae: 607.4697 - val_mse: 1032080.2500 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 364.2148 - mae: 364.2148 - mse: 687769.0000 - val_loss: 607.7923 - val_mae: 607.7923 - val_mse: 1015805.1875 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 364.4017 - mae: 364.4017 - mse: 667782.7500 - val_loss: 607.4823 - val_mae: 607.4823 - val_mse: 1031943.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 364.8921 - mae: 364.8921 - mse: 689804.1250 - val_loss: 607.1474 - val_mae: 607.1474 - val_mse: 1015760.8125 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 366.4357 - mae: 366.4357 - mse: 672470.5000 - val_loss: 609.5728 - val_mae: 609.5728 - val_mse: 1037259.9375 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 364.9104 - mae: 364.9104 - mse: 690180.6250 - val_loss: 607.1716 - val_mae: 607.1716 - val_mse: 1015695.4375 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 366.7068 - mae: 366.7068 - mse: 673052.4375 - val_loss: 609.8798 - val_mae: 609.8798 - val_mse: 1037687.0625 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 364.9238 - mae: 364.9238 - mse: 689918.2500 - val_loss: 607.8677 - val_mae: 607.8677 - val_mse: 1015564.3750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 365.5392 - mae: 365.5392 - mse: 670425.9375 - val_loss: 608.3478 - val_mae: 608.3478 - val_mse: 1033376.6875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 365.6715 - mae: 365.6715 - mse: 691988.0000 - val_loss: 607.3809 - val_mae: 607.3809 - val_mse: 1015803.3125 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 367.3666 - mae: 367.3666 - mse: 674312.4375 - val_loss: 610.9095 - val_mae: 610.9095 - val_mse: 1039915.9375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 364.5525 - mae: 364.5525 - mse: 689124.7500 - val_loss: 609.2162 - val_mae: 609.2162 - val_mse: 1015686.4375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 363.1871 - mae: 363.1871 - mse: 665936.3125 - val_loss: 607.3037 - val_mae: 607.3037 - val_mse: 1030498.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 364.3697 - mae: 364.3697 - mse: 688267.6250 - val_loss: 607.4850 - val_mae: 607.4850 - val_mse: 1015716.4375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 365.2522 - mae: 365.2522 - mse: 669904.5000 - val_loss: 607.6639 - val_mae: 607.6639 - val_mse: 1031876.0625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 366.5144 - mae: 366.5144 - mse: 694547.9375 - val_loss: 607.0272 - val_mae: 607.0272 - val_mse: 1015922.5625 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 368.9865 - mae: 368.9865 - mse: 678107.7500 - val_loss: 612.3251 - val_mae: 612.3251 - val_mse: 1043884.4375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 364.7786 - mae: 364.7786 - mse: 689851.3750 - val_loss: 609.0776 - val_mae: 609.0776 - val_mse: 1015730.3750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 363.6953 - mae: 363.6953 - mse: 666952.1250 - val_loss: 607.7013 - val_mae: 607.7013 - val_mse: 1031663.9375 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 364.3473 - mae: 364.3473 - mse: 688251.0625 - val_loss: 607.6317 - val_mae: 607.6317 - val_mse: 1015752.7500 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 365.0743 - mae: 365.0743 - mse: 669613.5625 - val_loss: 607.8967 - val_mae: 607.8967 - val_mse: 1032109.7500 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 366.0466 - mae: 366.0466 - mse: 693227.6250 - val_loss: 606.6971 - val_mae: 606.6971 - val_mse: 1016116.5625 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 369.1004 - mae: 369.1004 - mse: 678242.5625 - val_loss: 611.7667 - val_mae: 611.7667 - val_mse: 1042249.9375 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 365.6603 - mae: 365.6603 - mse: 692256.0625 - val_loss: 608.8417 - val_mae: 608.8417 - val_mse: 1015731.5625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 365.3400 - mae: 365.3400 - mse: 670456.6250 - val_loss: 608.2017 - val_mae: 608.2017 - val_mse: 1033057.3750 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 365.5048 - mae: 365.5048 - mse: 691568.5625 - val_loss: 606.8264 - val_mae: 606.8264 - val_mse: 1015926.0625 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 368.4211 - mae: 368.4211 - mse: 676965.8125 - val_loss: 610.7895 - val_mae: 610.7895 - val_mse: 1039087.0000 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 366.4397 - mae: 366.4397 - mse: 694603.6250 - val_loss: 607.5845 - val_mae: 607.5845 - val_mse: 1015758.5625 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 368.2904 - mae: 368.2904 - mse: 676582.5000 - val_loss: 610.6690 - val_mae: 610.6690 - val_mse: 1038627.0000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 366.4929 - mae: 366.4929 - mse: 694576.6875 - val_loss: 606.5496 - val_mae: 606.5496 - val_mse: 1015908.3750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 370.6541 - mae: 370.6541 - mse: 682145.8125 - val_loss: 611.8458 - val_mae: 611.8458 - val_mse: 1041732.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 368.3098 - mae: 368.3098 - mse: 699944.0625 - val_loss: 606.2026 - val_mae: 606.2026 - val_mse: 1016131.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 374.4922 - mae: 374.4922 - mse: 691425.4375 - val_loss: 614.8434 - val_mae: 614.8434 - val_mse: 1049330.8750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 369.6360 - mae: 369.6360 - mse: 703812.1875 - val_loss: 608.3973 - val_mae: 608.3973 - val_mse: 1015703.7500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 370.6887 - mae: 370.6887 - mse: 682068.0625 - val_loss: 616.0219 - val_mae: 616.0219 - val_mse: 1051912.0000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 363.6074 - mae: 363.6074 - mse: 686288.3125 - val_loss: 611.0957 - val_mae: 611.0957 - val_mse: 1016409.2500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 359.0135 - mae: 359.0135 - mse: 657400.5000 - val_loss: 606.2266 - val_mae: 606.2266 - val_mse: 1026105.9375 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 360.8007 - mae: 360.8007 - mse: 677541.6250 - val_loss: 605.6094 - val_mae: 605.6094 - val_mse: 1016996.1250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 364.9669 - mae: 364.9669 - mse: 669603.0000 - val_loss: 606.4722 - val_mae: 606.4722 - val_mse: 1026871.6875 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 369.3179 - mae: 369.3179 - mse: 701974.0625 - val_loss: 605.3359 - val_mae: 605.3359 - val_mse: 1018013.4375 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 381.2556 - mae: 381.2556 - mse: 707240.7500 - val_loss: 615.5731 - val_mae: 615.5731 - val_mse: 1050824.1250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 377.6742 - mae: 377.6742 - mse: 727356.6875 - val_loss: 617.8208 - val_mae: 617.8208 - val_mse: 1020562.7500 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 368.7299 - mae: 368.7299 - mse: 678117.3125 - val_loss: 620.1118 - val_mae: 620.1118 - val_mse: 1061192.8750 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 358.7096 - mae: 358.7096 - mse: 671200.0625 - val_loss: 608.9668 - val_mae: 608.9668 - val_mse: 1015321.9375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 355.4652 - mae: 355.4652 - mse: 651565.8750 - val_loss: 606.3865 - val_mae: 606.3865 - val_mse: 1026901.3750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 355.3300 - mae: 355.3300 - mse: 661447.3125 - val_loss: 605.4265 - val_mae: 605.4265 - val_mse: 1017324.5625 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 355.8604 - mae: 355.8604 - mse: 652645.5625 - val_loss: 605.4960 - val_mae: 605.4960 - val_mse: 1022235.4375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 358.0348 - mae: 358.0348 - mse: 669571.6250 - val_loss: 605.2185 - val_mae: 605.2185 - val_mse: 1019409.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 363.3883 - mae: 363.3883 - mse: 665528.7500 - val_loss: 605.5518 - val_mae: 605.5518 - val_mse: 1022526.3125 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 371.2862 - mae: 371.2862 - mse: 707080.9375 - val_loss: 605.2445 - val_mae: 605.2445 - val_mse: 1017498.7500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 384.0861 - mae: 384.0861 - mse: 714843.0625 - val_loss: 614.5631 - val_mae: 614.5631 - val_mse: 1048397.7500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 382.9871 - mae: 382.9871 - mse: 743562.0000 - val_loss: 625.4985 - val_mae: 625.4985 - val_mse: 1026774.3750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 354.6104 - mae: 354.6104 - mse: 656277.7500 - val_loss: 608.4660 - val_mae: 608.4660 - val_mse: 1015094.5000 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 352.2381 - mae: 352.2381 - mse: 648392.9375 - val_loss: 605.7355 - val_mae: 605.7355 - val_mse: 1025124.3125 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 353.5547 - mae: 353.5547 - mse: 648155.7500 - val_loss: 611.3644 - val_mae: 611.3644 - val_mse: 1041118.8750 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 354.3420 - mae: 354.3420 - mse: 648770.1250 - val_loss: 610.7607 - val_mae: 610.7607 - val_mse: 1039598.8750 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 353.1293 - mae: 353.1293 - mse: 647842.1875 - val_loss: 605.9582 - val_mae: 605.9582 - val_mse: 1025940.7500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 352.2845 - mae: 352.2845 - mse: 648756.1250 - val_loss: 605.1709 - val_mae: 605.1709 - val_mse: 1017036.7500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 352.5654 - mae: 352.5654 - mse: 651024.5625 - val_loss: 606.8843 - val_mae: 606.8843 - val_mse: 1015285.6250 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 352.6992 - mae: 352.6992 - mse: 651593.5625 - val_loss: 606.6047 - val_mae: 606.6047 - val_mse: 1015403.6250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 352.3791 - mae: 352.3791 - mse: 650305.8125 - val_loss: 605.2616 - val_mae: 605.2616 - val_mse: 1017010.7500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 352.0992 - mae: 352.0992 - mse: 648771.3750 - val_loss: 605.1672 - val_mae: 605.1672 - val_mse: 1020952.3750 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 352.1896 - mae: 352.1896 - mse: 648088.1250 - val_loss: 605.7728 - val_mae: 605.7728 - val_mse: 1024412.6875 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 352.2769 - mae: 352.2769 - mse: 648055.5625 - val_loss: 605.6974 - val_mae: 605.6974 - val_mse: 1023988.0625 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 352.1725 - mae: 352.1725 - mse: 648331.5625 - val_loss: 605.2625 - val_mae: 605.2625 - val_mse: 1021309.1250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 352.1302 - mae: 352.1302 - mse: 648788.3750 - val_loss: 605.0910 - val_mae: 605.0910 - val_mse: 1019294.8750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 352.1484 - mae: 352.1484 - mse: 649084.5000 - val_loss: 605.0923 - val_mae: 605.0923 - val_mse: 1018726.8750 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 352.1228 - mae: 352.1228 - mse: 648962.3750 - val_loss: 605.0720 - val_mae: 605.0720 - val_mse: 1019438.3125 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 352.0840 - mae: 352.0840 - mse: 648582.8125 - val_loss: 605.1461 - val_mae: 605.1461 - val_mse: 1020764.4375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 352.1112 - mae: 352.1112 - mse: 648380.6875 - val_loss: 605.2333 - val_mae: 605.2333 - val_mse: 1021448.1875 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 352.1170 - mae: 352.1170 - mse: 648403.2500 - val_loss: 605.1685 - val_mae: 605.1685 - val_mse: 1021032.3750 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 352.1018 - mae: 352.1018 - mse: 648585.8750 - val_loss: 605.0729 - val_mae: 605.0729 - val_mse: 1020311.9375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 352.0983 - mae: 352.0983 - mse: 648669.1875 - val_loss: 605.0394 - val_mae: 605.0394 - val_mse: 1020123.8750 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 352.0885 - mae: 352.0885 - mse: 648611.3125 - val_loss: 605.0474 - val_mae: 605.0474 - val_mse: 1020388.1875 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 352.0895 - mae: 352.0895 - mse: 648540.5625 - val_loss: 605.0751 - val_mae: 605.0751 - val_mse: 1020720.7500 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 352.0904 - mae: 352.0904 - mse: 648519.8750 - val_loss: 605.0807 - val_mae: 605.0807 - val_mse: 1020865.8125 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 352.0900 - mae: 352.0900 - mse: 648508.4375 - val_loss: 605.0514 - val_mae: 605.0514 - val_mse: 1020747.8125 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 352.0908 - mae: 352.0908 - mse: 648584.1875 - val_loss: 605.0062 - val_mae: 605.0062 - val_mse: 1020501.5625 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 352.0845 - mae: 352.0845 - mse: 648587.8125 - val_loss: 604.9839 - val_mae: 604.9839 - val_mse: 1020525.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 352.0848 - mae: 352.0848 - mse: 648538.2500 - val_loss: 604.9692 - val_mae: 604.9692 - val_mse: 1020636.2500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 352.0873 - mae: 352.0873 - mse: 648542.6875 - val_loss: 604.9457 - val_mae: 604.9457 - val_mse: 1020597.1875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 352.0828 - mae: 352.0828 - mse: 648587.0625 - val_loss: 604.9173 - val_mae: 604.9173 - val_mse: 1020451.3750 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 352.0825 - mae: 352.0825 - mse: 648650.6250 - val_loss: 604.9030 - val_mae: 604.9030 - val_mse: 1020501.5000 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 352.0733 - mae: 352.0733 - mse: 648584.3125 - val_loss: 604.9116 - val_mae: 604.9116 - val_mse: 1020886.5625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 352.0669 - mae: 352.0669 - mse: 648495.8125 - val_loss: 604.9186 - val_mae: 604.9186 - val_mse: 1021315.0000 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 352.0770 - mae: 352.0770 - mse: 648402.8750 - val_loss: 604.8784 - val_mae: 604.8784 - val_mse: 1021345.9375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 352.0748 - mae: 352.0748 - mse: 648520.9375 - val_loss: 604.7802 - val_mae: 604.7802 - val_mse: 1020716.2500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 352.0686 - mae: 352.0686 - mse: 648659.3750 - val_loss: 604.7351 - val_mae: 604.7351 - val_mse: 1020404.2500 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 352.0594 - mae: 352.0594 - mse: 648646.3125 - val_loss: 604.7198 - val_mae: 604.7198 - val_mse: 1020612.4375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 352.0562 - mae: 352.0562 - mse: 648608.3750 - val_loss: 604.7299 - val_mae: 604.7299 - val_mse: 1021089.3750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 352.0591 - mae: 352.0591 - mse: 648495.2500 - val_loss: 604.7495 - val_mae: 604.7495 - val_mse: 1021538.5625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 352.0620 - mae: 352.0620 - mse: 648516.6250 - val_loss: 604.7216 - val_mae: 604.7216 - val_mse: 1021479.6250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 352.0595 - mae: 352.0595 - mse: 648554.0625 - val_loss: 604.6716 - val_mae: 604.6716 - val_mse: 1021223.8750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 352.0574 - mae: 352.0574 - mse: 648601.1875 - val_loss: 604.6287 - val_mae: 604.6287 - val_mse: 1020896.3750 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 352.0553 - mae: 352.0553 - mse: 648673.9375 - val_loss: 604.6102 - val_mae: 604.6102 - val_mse: 1020837.7500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 352.0518 - mae: 352.0518 - mse: 648604.5625 - val_loss: 604.6103 - val_mae: 604.6103 - val_mse: 1021120.8125 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 352.0533 - mae: 352.0533 - mse: 648588.5000 - val_loss: 604.6040 - val_mae: 604.6040 - val_mse: 1021136.6250 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 352.0542 - mae: 352.0542 - mse: 648668.6875 - val_loss: 604.5891 - val_mae: 604.5891 - val_mse: 1020978.9375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 352.0496 - mae: 352.0496 - mse: 648664.0000 - val_loss: 604.5842 - val_mae: 604.5842 - val_mse: 1021124.9375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 352.0450 - mae: 352.0450 - mse: 648595.2500 - val_loss: 604.5800 - val_mae: 604.5800 - val_mse: 1021348.5625 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 352.0521 - mae: 352.0521 - mse: 648557.4375 - val_loss: 604.5579 - val_mae: 604.5579 - val_mse: 1021250.3750 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 352.0510 - mae: 352.0510 - mse: 648635.6875 - val_loss: 604.5380 - val_mae: 604.5380 - val_mse: 1020916.0625 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 352.0495 - mae: 352.0495 - mse: 648719.2500 - val_loss: 604.5332 - val_mae: 604.5332 - val_mse: 1020806.6875 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 352.0437 - mae: 352.0437 - mse: 648697.2500 - val_loss: 604.5369 - val_mae: 604.5369 - val_mse: 1021044.3750 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 352.0407 - mae: 352.0407 - mse: 648609.1875 - val_loss: 604.5431 - val_mae: 604.5431 - val_mse: 1021311.6250 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 352.0441 - mae: 352.0441 - mse: 648625.0000 - val_loss: 604.5375 - val_mae: 604.5375 - val_mse: 1021281.5625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 352.0413 - mae: 352.0413 - mse: 648629.0000 - val_loss: 604.5235 - val_mae: 604.5235 - val_mse: 1021128.3750 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 352.0434 - mae: 352.0434 - mse: 648703.2500 - val_loss: 604.5131 - val_mae: 604.5131 - val_mse: 1020985.3125 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 352.0386 - mae: 352.0386 - mse: 648677.5000 - val_loss: 604.5119 - val_mae: 604.5119 - val_mse: 1021187.8125 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 352.0423 - mae: 352.0423 - mse: 648591.8750 - val_loss: 604.5128 - val_mae: 604.5128 - val_mse: 1021401.0000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 352.0440 - mae: 352.0440 - mse: 648608.3125 - val_loss: 604.5055 - val_mae: 604.5055 - val_mse: 1021244.1250 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 352.0423 - mae: 352.0423 - mse: 648693.2500 - val_loss: 604.5007 - val_mae: 604.5007 - val_mse: 1021088.6875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 352.0371 - mae: 352.0371 - mse: 648672.8750 - val_loss: 604.4996 - val_mae: 604.4996 - val_mse: 1021198.6250 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 352.0348 - mae: 352.0348 - mse: 648635.6250 - val_loss: 604.4918 - val_mae: 604.4918 - val_mse: 1021207.7500 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 352.0362 - mae: 352.0362 - mse: 648679.1250 - val_loss: 604.4839 - val_mae: 604.4839 - val_mse: 1021172.8750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 352.0349 - mae: 352.0349 - mse: 648635.1250 - val_loss: 604.4788 - val_mae: 604.4788 - val_mse: 1021233.1875 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 352.0346 - mae: 352.0346 - mse: 648677.9375 - val_loss: 604.4670 - val_mae: 604.4670 - val_mse: 1021104.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 352.0324 - mae: 352.0324 - mse: 648688.7500 - val_loss: 604.4571 - val_mae: 604.4571 - val_mse: 1021054.1875 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 352.0331 - mae: 352.0331 - mse: 648708.3125 - val_loss: 604.4593 - val_mae: 604.4593 - val_mse: 1021184.8750 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 352.0290 - mae: 352.0290 - mse: 648639.1250 - val_loss: 604.4634 - val_mae: 604.4634 - val_mse: 1021395.5000 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 352.0336 - mae: 352.0336 - mse: 648661.8750 - val_loss: 604.4596 - val_mae: 604.4596 - val_mse: 1021374.3125 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 352.0309 - mae: 352.0309 - mse: 648661.5000 - val_loss: 604.4492 - val_mae: 604.4492 - val_mse: 1021350.1875 - lr: 1.0000e-04 - 46ms/epoch - 15ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 352.0320 - mae: 352.0320 - mse: 648649.5000 - val_loss: 604.4353 - val_mae: 604.4353 - val_mse: 1021219.1250 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 352.0312 - mae: 352.0312 - mse: 648717.5625 - val_loss: 604.4316 - val_mae: 604.4316 - val_mse: 1021048.2500 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 352.0307 - mae: 352.0307 - mse: 648733.2500 - val_loss: 604.4492 - val_mae: 604.4492 - val_mse: 1021294.9375 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 352.0277 - mae: 352.0277 - mse: 648617.4375 - val_loss: 604.4685 - val_mae: 604.4685 - val_mse: 1021712.6250 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 352.0317 - mae: 352.0317 - mse: 648580.8125 - val_loss: 604.4557 - val_mae: 604.4557 - val_mse: 1021383.5625 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 352.0343 - mae: 352.0343 - mse: 648766.8125 - val_loss: 604.4360 - val_mae: 604.4360 - val_mse: 1020759.5625 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 352.0262 - mae: 352.0262 - mse: 648774.1875 - val_loss: 604.4385 - val_mae: 604.4385 - val_mse: 1020862.6250 - lr: 1.0000e-04 - 57ms/epoch - 19ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 352.0249 - mae: 352.0249 - mse: 648646.6875 - val_loss: 604.4506 - val_mae: 604.4506 - val_mse: 1021192.8750 - lr: 1.0000e-04 - 70ms/epoch - 23ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 352.0282 - mae: 352.0282 - mse: 648638.3125 - val_loss: 604.4573 - val_mae: 604.4573 - val_mse: 1021160.6875 - lr: 1.0000e-04 - 73ms/epoch - 24ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 352.0263 - mae: 352.0263 - mse: 648711.9375 - val_loss: 604.4622 - val_mae: 604.4622 - val_mse: 1021080.3125 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 352.0211 - mae: 352.0211 - mse: 648688.0625 - val_loss: 604.4660 - val_mae: 604.4660 - val_mse: 1021213.6875 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 352.0192 - mae: 352.0192 - mse: 648647.3750 - val_loss: 604.4611 - val_mae: 604.4611 - val_mse: 1021320.1875 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 352.0251 - mae: 352.0251 - mse: 648605.6250 - val_loss: 604.4517 - val_mae: 604.4517 - val_mse: 1021282.1875 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 352.0243 - mae: 352.0243 - mse: 648662.9375 - val_loss: 604.4468 - val_mae: 604.4468 - val_mse: 1021045.6875 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 352.0243 - mae: 352.0243 - mse: 648740.7500 - val_loss: 604.4512 - val_mae: 604.4512 - val_mse: 1020954.5625 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 352.0193 - mae: 352.0193 - mse: 648716.6875 - val_loss: 604.4577 - val_mae: 604.4577 - val_mse: 1021163.4375 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 352.0141 - mae: 352.0141 - mse: 648627.3125 - val_loss: 604.4640 - val_mae: 604.4640 - val_mse: 1021497.3125 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 352.0244 - mae: 352.0244 - mse: 648550.6250 - val_loss: 604.4569 - val_mae: 604.4569 - val_mse: 1021496.7500 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 352.0235 - mae: 352.0235 - mse: 648641.0000 - val_loss: 604.4429 - val_mae: 604.4429 - val_mse: 1021037.0625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 352.0226 - mae: 352.0226 - mse: 648774.3750 - val_loss: 604.4432 - val_mae: 604.4432 - val_mse: 1020813.3750 - lr: 1.0000e-04 - 68ms/epoch - 23ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 352.0150 - mae: 352.0150 - mse: 648729.0000 - val_loss: 604.4537 - val_mae: 604.4537 - val_mse: 1021110.6875 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 352.0102 - mae: 352.0102 - mse: 648622.1250 - val_loss: 604.4641 - val_mae: 604.4641 - val_mse: 1021401.7500 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 352.0137 - mae: 352.0137 - mse: 648621.4375 - val_loss: 604.4660 - val_mae: 604.4660 - val_mse: 1021442.8750 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 352.0142 - mae: 352.0142 - mse: 648599.8750 - val_loss: 604.4608 - val_mae: 604.4608 - val_mse: 1021353.6250 - lr: 1.0000e-04 - 65ms/epoch - 22ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 352.0151 - mae: 352.0151 - mse: 648667.1875 - val_loss: 604.4545 - val_mae: 604.4545 - val_mse: 1021178.9375 - lr: 1.0000e-04 - 43ms/epoch - 14ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 352.0101 - mae: 352.0101 - mse: 648649.1250 - val_loss: 604.4549 - val_mae: 604.4549 - val_mse: 1021194.5625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 352.0113 - mae: 352.0113 - mse: 648664.5000 - val_loss: 604.4622 - val_mae: 604.4622 - val_mse: 1021299.2500 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 352.0075 - mae: 352.0075 - mse: 648599.7500 - val_loss: 604.4641 - val_mae: 604.4641 - val_mse: 1021331.5625 - lr: 1.0000e-04 - 81ms/epoch - 27ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 352.0129 - mae: 352.0129 - mse: 648687.1875 - val_loss: 604.4558 - val_mae: 604.4558 - val_mse: 1021079.3125 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 216: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 352.0071 - mae: 352.0071 - mse: 648677.7500 - val_loss: 604.4531 - val_mae: 604.4531 - val_mse: 1021158.7500 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Optimizing model by reducing: mse for epochs: 300, num_iter: 2, model: dense_model\n",
            "Epoch 1/300\n",
            "3/3 - 1s - loss: 901630.9375 - mae: 506.8621 - mse: 901630.9375 - val_loss: 2507489.5000 - val_mae: 1202.4956 - val_mse: 2507489.5000 - lr: 0.0010 - 799ms/epoch - 266ms/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 4660065.0000 - mae: 1161.0094 - mse: 4660065.0000 - val_loss: 7638731.0000 - val_mae: 2343.5627 - val_mse: 7638731.0000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 824450.7500 - mae: 471.0634 - mse: 824450.7500 - val_loss: 1057637.7500 - val_mae: 648.0699 - val_mse: 1057637.7500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 1856354.7500 - mae: 659.8539 - mse: 1856354.7500 - val_loss: 4463061.0000 - val_mae: 1733.8517 - val_mse: 4463061.0000 - lr: 0.0010 - 42ms/epoch - 14ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 1976496.6250 - mae: 726.4050 - mse: 1976496.6250 - val_loss: 2853470.7500 - val_mae: 1332.6010 - val_mse: 2853470.7500 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 769575.8750 - mae: 412.4407 - mse: 769575.8750 - val_loss: 1033386.6875 - val_mae: 604.2247 - val_mse: 1033386.6875 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 928227.5625 - mae: 452.9696 - mse: 928227.5625 - val_loss: 2473285.2500 - val_mae: 1189.9031 - val_mse: 2473285.2500 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 1224825.2500 - mae: 557.0370 - mse: 1224825.2500 - val_loss: 2462240.5000 - val_mae: 1186.2632 - val_mse: 2462240.5000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 791912.5625 - mae: 428.4917 - mse: 791912.5625 - val_loss: 1172467.8750 - val_mae: 670.2704 - val_mse: 1172467.8750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 702001.6250 - mae: 368.2559 - mse: 702001.6250 - val_loss: 1199933.8750 - val_mae: 732.1370 - val_mse: 1199933.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 882055.2500 - mae: 431.1595 - mse: 882055.2500 - val_loss: 1366778.2500 - val_mae: 814.3789 - val_mse: 1366778.2500 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 738937.5000 - mae: 388.2049 - mse: 738937.5000 - val_loss: 1037070.5625 - val_mae: 630.1107 - val_mse: 1037070.5625 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 666807.8750 - mae: 362.1969 - mse: 666807.8750 - val_loss: 1209708.8750 - val_mae: 689.5032 - val_mse: 1209708.8750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 745932.1250 - mae: 399.2819 - mse: 745932.1875 - val_loss: 1358695.7500 - val_mae: 764.7994 - val_mse: 1358695.7500 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 691018.1250 - mae: 379.5493 - mse: 691018.1250 - val_loss: 1101470.6250 - val_mae: 633.4737 - val_mse: 1101470.6250 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 660495.2500 - mae: 355.9482 - mse: 660495.2500 - val_loss: 1026929.9375 - val_mae: 620.3859 - val_mse: 1026929.9375 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 693072.3125 - mae: 366.8405 - mse: 693072.3125 - val_loss: 1044943.2500 - val_mae: 637.9852 - val_mse: 1044943.2500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 664351.6250 - mae: 357.8086 - mse: 664351.6250 - val_loss: 1022391.6875 - val_mae: 602.6988 - val_mse: 1022391.6875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 658774.6250 - mae: 359.8043 - mse: 658774.6250 - val_loss: 1098919.7500 - val_mae: 633.2118 - val_mse: 1098919.7500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 669203.7500 - mae: 367.1903 - mse: 669203.8125 - val_loss: 1096843.8750 - val_mae: 632.5529 - val_mse: 1096843.8750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 653953.5625 - mae: 357.7691 - mse: 653953.5625 - val_loss: 1028590.9375 - val_mae: 603.3370 - val_mse: 1028590.9375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 657456.3750 - mae: 355.8122 - mse: 657456.4375 - val_loss: 1016237.8750 - val_mae: 607.8753 - val_mse: 1016237.8750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 656856.6875 - mae: 355.8161 - mse: 656856.6875 - val_loss: 1017294.6250 - val_mae: 603.5918 - val_mse: 1017294.6250 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 651426.6875 - mae: 355.4040 - mse: 651426.6875 - val_loss: 1042819.4375 - val_mae: 610.1104 - val_mse: 1042819.4375 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 654516.4375 - mae: 358.6680 - mse: 654516.4375 - val_loss: 1053639.6250 - val_mae: 615.4155 - val_mse: 1053639.6250 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 651347.8750 - mae: 356.5770 - mse: 651347.8750 - val_loss: 1029180.3750 - val_mae: 606.1223 - val_mse: 1029180.3750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 651328.0625 - mae: 354.8282 - mse: 651328.0625 - val_loss: 1017341.5625 - val_mae: 604.5337 - val_mse: 1017341.5625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 651406.0625 - mae: 354.7386 - mse: 651406.1250 - val_loss: 1020199.0000 - val_mae: 604.8733 - val_mse: 1020199.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 650200.8750 - mae: 355.1306 - mse: 650200.8750 - val_loss: 1032924.5000 - val_mae: 608.7076 - val_mse: 1032924.5000 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 650802.2500 - mae: 356.2007 - mse: 650802.2500 - val_loss: 1035820.8750 - val_mae: 610.1509 - val_mse: 1035820.8750 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 649778.6250 - mae: 355.0706 - mse: 649778.5625 - val_loss: 1025747.9375 - val_mae: 606.6509 - val_mse: 1025747.9375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 650106.8750 - mae: 354.1161 - mse: 650106.8125 - val_loss: 1021478.0000 - val_mae: 605.8050 - val_mse: 1021478.0000 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 649895.2500 - mae: 354.0822 - mse: 649895.2500 - val_loss: 1026016.5000 - val_mae: 607.1440 - val_mse: 1026016.5000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 649740.8125 - mae: 354.8202 - mse: 649740.7500 - val_loss: 1032027.3750 - val_mae: 609.5024 - val_mse: 1032027.3750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 649569.8750 - mae: 355.0881 - mse: 649569.9375 - val_loss: 1029484.3125 - val_mae: 608.7204 - val_mse: 1029484.3125 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 649360.8750 - mae: 354.6973 - mse: 649360.8750 - val_loss: 1024260.4375 - val_mae: 607.2942 - val_mse: 1024260.4375 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 649299.3125 - mae: 354.5248 - mse: 649299.3125 - val_loss: 1024127.6250 - val_mae: 607.4407 - val_mse: 1024127.6250 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 648880.2500 - mae: 354.6238 - mse: 648880.2500 - val_loss: 1027838.5625 - val_mae: 608.7336 - val_mse: 1027838.5625 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 648680.3125 - mae: 354.7532 - mse: 648680.3125 - val_loss: 1029157.1250 - val_mae: 609.3500 - val_mse: 1029157.1250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 648195.3125 - mae: 354.3978 - mse: 648195.3125 - val_loss: 1026408.0000 - val_mae: 608.4937 - val_mse: 1026408.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 648068.9375 - mae: 354.0867 - mse: 648068.8750 - val_loss: 1024555.2500 - val_mae: 608.0046 - val_mse: 1024555.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 648302.0625 - mae: 354.1790 - mse: 648302.0625 - val_loss: 1025100.1250 - val_mae: 608.2742 - val_mse: 1025100.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 648054.1875 - mae: 354.2502 - mse: 648054.1875 - val_loss: 1026356.0000 - val_mae: 608.8041 - val_mse: 1026356.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 648006.0625 - mae: 354.3708 - mse: 648006.1250 - val_loss: 1027025.8125 - val_mae: 609.1165 - val_mse: 1027025.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 648049.2500 - mae: 354.4250 - mse: 648049.2500 - val_loss: 1026782.2500 - val_mae: 609.1440 - val_mse: 1026782.2500 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 648030.9375 - mae: 354.4220 - mse: 648030.9375 - val_loss: 1026589.0000 - val_mae: 609.1303 - val_mse: 1026589.0000 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 647947.4375 - mae: 354.4278 - mse: 647947.4375 - val_loss: 1026754.1875 - val_mae: 609.2079 - val_mse: 1026754.1875 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 647854.7500 - mae: 354.3806 - mse: 647854.7500 - val_loss: 1026623.3750 - val_mae: 609.2180 - val_mse: 1026623.3750 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 647809.0625 - mae: 354.2504 - mse: 647809.0625 - val_loss: 1026145.6875 - val_mae: 609.0839 - val_mse: 1026145.6875 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 647893.0625 - mae: 354.1836 - mse: 647893.0625 - val_loss: 1025835.9375 - val_mae: 609.0010 - val_mse: 1025835.9375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 647824.2500 - mae: 354.2031 - mse: 647824.2500 - val_loss: 1025958.4375 - val_mae: 609.0764 - val_mse: 1025958.4375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 647766.1875 - mae: 354.2960 - mse: 647766.2500 - val_loss: 1026576.6875 - val_mae: 609.3202 - val_mse: 1026576.6875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 647743.7500 - mae: 354.3493 - mse: 647743.7500 - val_loss: 1026880.4375 - val_mae: 609.4415 - val_mse: 1026880.4375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 647724.3125 - mae: 354.3665 - mse: 647724.3125 - val_loss: 1026613.6250 - val_mae: 609.3713 - val_mse: 1026613.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 647668.6250 - mae: 354.3550 - mse: 647668.6250 - val_loss: 1026325.8750 - val_mae: 609.2874 - val_mse: 1026325.8750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 647637.0625 - mae: 354.3468 - mse: 647637.0625 - val_loss: 1026150.2500 - val_mae: 609.2399 - val_mse: 1026150.2500 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 647654.5000 - mae: 354.2664 - mse: 647654.5000 - val_loss: 1025881.6250 - val_mae: 609.1655 - val_mse: 1025881.6250 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 647629.1875 - mae: 354.2239 - mse: 647629.1875 - val_loss: 1025819.8750 - val_mae: 609.1690 - val_mse: 1025819.8750 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 647595.5000 - mae: 354.2964 - mse: 647595.4375 - val_loss: 1026458.8750 - val_mae: 609.4219 - val_mse: 1026458.8750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 647564.6250 - mae: 354.3784 - mse: 647564.5625 - val_loss: 1026894.3750 - val_mae: 609.6059 - val_mse: 1026894.3750 - lr: 0.0010 - 30ms/epoch - 10ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 647541.3750 - mae: 354.4105 - mse: 647541.3125 - val_loss: 1026560.8750 - val_mae: 609.5291 - val_mse: 1026560.8750 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 647492.2500 - mae: 354.3495 - mse: 647492.2500 - val_loss: 1026275.1250 - val_mae: 609.4768 - val_mse: 1026275.1250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 647449.8125 - mae: 354.2821 - mse: 647449.8125 - val_loss: 1026421.5000 - val_mae: 609.5826 - val_mse: 1026421.5000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 647428.1875 - mae: 354.2233 - mse: 647428.0625 - val_loss: 1026342.0000 - val_mae: 609.6071 - val_mse: 1026342.0000 - lr: 0.0010 - 43ms/epoch - 14ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 647398.8125 - mae: 354.2381 - mse: 647398.7500 - val_loss: 1026068.9375 - val_mae: 609.5483 - val_mse: 1026068.9375 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 647370.1250 - mae: 354.2835 - mse: 647370.1250 - val_loss: 1026308.3125 - val_mae: 609.6599 - val_mse: 1026308.3125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 647350.1250 - mae: 354.2489 - mse: 647350.1875 - val_loss: 1026300.6875 - val_mae: 609.6603 - val_mse: 1026300.6875 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 647329.2500 - mae: 354.2630 - mse: 647329.2500 - val_loss: 1026151.5625 - val_mae: 609.6168 - val_mse: 1026151.5625 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 647296.5000 - mae: 354.2728 - mse: 647296.4375 - val_loss: 1026447.0000 - val_mae: 609.7506 - val_mse: 1026447.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 647247.6250 - mae: 354.2248 - mse: 647247.6250 - val_loss: 1026585.8125 - val_mae: 609.8293 - val_mse: 1026585.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 647245.3125 - mae: 354.1513 - mse: 647245.3125 - val_loss: 1026246.0625 - val_mae: 609.7417 - val_mse: 1026246.0625 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 647207.6250 - mae: 354.1883 - mse: 647207.6250 - val_loss: 1026217.0000 - val_mae: 609.7563 - val_mse: 1026217.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 647171.0000 - mae: 354.2018 - mse: 647171.0000 - val_loss: 1026813.7500 - val_mae: 609.9879 - val_mse: 1026813.7500 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 647129.9375 - mae: 354.2116 - mse: 647130.0000 - val_loss: 1026836.3750 - val_mae: 610.0170 - val_mse: 1026836.3750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 647102.1875 - mae: 354.1656 - mse: 647102.1875 - val_loss: 1026324.1250 - val_mae: 609.8391 - val_mse: 1026324.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 647087.5000 - mae: 354.1626 - mse: 647087.5000 - val_loss: 1026506.9375 - val_mae: 609.8909 - val_mse: 1026506.9375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 647046.7500 - mae: 354.2089 - mse: 647046.7500 - val_loss: 1026926.3125 - val_mae: 610.0399 - val_mse: 1026926.3125 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 647013.5000 - mae: 354.1869 - mse: 647013.5000 - val_loss: 1026452.2500 - val_mae: 609.8598 - val_mse: 1026452.2500 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 647043.5000 - mae: 354.0460 - mse: 647043.5000 - val_loss: 1025954.8125 - val_mae: 609.6702 - val_mse: 1025954.8125 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 646986.5625 - mae: 354.0998 - mse: 646986.5625 - val_loss: 1026427.1875 - val_mae: 609.8574 - val_mse: 1026427.1875 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 646944.7500 - mae: 354.2116 - mse: 646944.7500 - val_loss: 1026921.6875 - val_mae: 610.1060 - val_mse: 1026921.6875 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 646906.8750 - mae: 354.1596 - mse: 646906.8750 - val_loss: 1026460.0000 - val_mae: 610.0212 - val_mse: 1026460.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 646885.6875 - mae: 354.1118 - mse: 646885.6250 - val_loss: 1026441.0000 - val_mae: 610.0707 - val_mse: 1026441.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 646868.0625 - mae: 354.2178 - mse: 646868.0625 - val_loss: 1026834.8750 - val_mae: 610.2164 - val_mse: 1026834.8750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 646824.0625 - mae: 354.1781 - mse: 646824.0625 - val_loss: 1026813.8125 - val_mae: 610.2016 - val_mse: 1026813.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 646783.0625 - mae: 354.1009 - mse: 646783.0625 - val_loss: 1026559.9375 - val_mae: 610.1243 - val_mse: 1026559.9375 - lr: 0.0010 - 57ms/epoch - 19ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 646778.2500 - mae: 354.1549 - mse: 646778.1875 - val_loss: 1026739.0625 - val_mae: 610.2308 - val_mse: 1026739.0625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 646732.5625 - mae: 354.1633 - mse: 646732.5625 - val_loss: 1027046.8750 - val_mae: 610.4171 - val_mse: 1027046.8750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 646713.9375 - mae: 354.1296 - mse: 646713.9375 - val_loss: 1026705.0000 - val_mae: 610.3492 - val_mse: 1026705.0000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 646696.0000 - mae: 354.1669 - mse: 646696.0000 - val_loss: 1026621.1250 - val_mae: 610.3472 - val_mse: 1026621.1250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 646665.0625 - mae: 354.2423 - mse: 646665.0000 - val_loss: 1027009.3750 - val_mae: 610.5192 - val_mse: 1027009.3750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 646642.3125 - mae: 354.1483 - mse: 646642.3125 - val_loss: 1026618.9375 - val_mae: 610.3885 - val_mse: 1026618.9375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 646621.2500 - mae: 354.1365 - mse: 646621.2500 - val_loss: 1026647.0000 - val_mae: 610.4277 - val_mse: 1026647.0000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 646567.8125 - mae: 354.1982 - mse: 646567.8125 - val_loss: 1027283.2500 - val_mae: 610.7258 - val_mse: 1027283.2500 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 646556.7500 - mae: 354.1895 - mse: 646556.7500 - val_loss: 1026821.6250 - val_mae: 610.5847 - val_mse: 1026821.6250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 646547.4375 - mae: 354.1273 - mse: 646547.5000 - val_loss: 1026769.5000 - val_mae: 610.5551 - val_mse: 1026769.5000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 646485.8750 - mae: 354.1920 - mse: 646485.8750 - val_loss: 1027434.9375 - val_mae: 610.8365 - val_mse: 1027434.9375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 646473.5000 - mae: 354.1490 - mse: 646473.5000 - val_loss: 1026617.5625 - val_mae: 610.5856 - val_mse: 1026617.5625 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 646469.8125 - mae: 354.1885 - mse: 646469.7500 - val_loss: 1026618.5000 - val_mae: 610.6221 - val_mse: 1026618.5000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 646420.1250 - mae: 354.2219 - mse: 646420.1250 - val_loss: 1027537.6875 - val_mae: 611.0069 - val_mse: 1027537.6875 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 646406.2500 - mae: 354.1573 - mse: 646406.2500 - val_loss: 1026983.8125 - val_mae: 610.8132 - val_mse: 1026983.8125 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 646379.8125 - mae: 354.1319 - mse: 646379.8125 - val_loss: 1026773.5000 - val_mae: 610.7200 - val_mse: 1026773.5000 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 646325.4375 - mae: 354.2177 - mse: 646325.4375 - val_loss: 1027553.1250 - val_mae: 611.0593 - val_mse: 1027553.1250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 646321.5000 - mae: 354.2033 - mse: 646321.5625 - val_loss: 1026841.8125 - val_mae: 610.8397 - val_mse: 1026841.8125 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 646315.3125 - mae: 354.1259 - mse: 646315.3125 - val_loss: 1026811.6875 - val_mae: 610.8421 - val_mse: 1026811.6875 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 646260.8125 - mae: 354.2084 - mse: 646260.8750 - val_loss: 1027829.6875 - val_mae: 611.2855 - val_mse: 1027829.6875 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 646248.4375 - mae: 354.1630 - mse: 646248.5000 - val_loss: 1026921.4375 - val_mae: 610.9917 - val_mse: 1026921.4375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 646223.2500 - mae: 354.1852 - mse: 646223.2500 - val_loss: 1027018.5625 - val_mae: 611.0106 - val_mse: 1027018.5625 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 646189.6875 - mae: 354.2935 - mse: 646189.6875 - val_loss: 1027833.8750 - val_mae: 611.2931 - val_mse: 1027833.8750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 646160.6875 - mae: 354.1153 - mse: 646160.6875 - val_loss: 1026913.7500 - val_mae: 610.8762 - val_mse: 1026913.7500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 646117.7500 - mae: 354.0702 - mse: 646117.7500 - val_loss: 1027137.8125 - val_mae: 610.8087 - val_mse: 1027137.8125 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 646064.5000 - mae: 354.1312 - mse: 646064.5000 - val_loss: 1027557.7500 - val_mae: 610.8876 - val_mse: 1027557.7500 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 646060.4375 - mae: 354.0381 - mse: 646060.4375 - val_loss: 1026545.0000 - val_mae: 610.4610 - val_mse: 1026545.0000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 646043.9375 - mae: 354.0163 - mse: 646044.0000 - val_loss: 1027126.5000 - val_mae: 610.6984 - val_mse: 1027126.5000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 645990.1250 - mae: 354.1602 - mse: 645990.1250 - val_loss: 1027727.0000 - val_mae: 611.0862 - val_mse: 1027727.0000 - lr: 0.0010 - 45ms/epoch - 15ms/step\n",
            "Epoch 116/300\n",
            "3/3 - 0s - loss: 645981.3750 - mae: 354.0654 - mse: 645981.3750 - val_loss: 1026528.0625 - val_mae: 610.7679 - val_mse: 1026528.0625 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 645958.8750 - mae: 354.1538 - mse: 645958.8750 - val_loss: 1027448.8750 - val_mae: 611.1251 - val_mse: 1027448.8750 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 645926.3750 - mae: 354.1736 - mse: 645926.3750 - val_loss: 1027875.6250 - val_mae: 611.2933 - val_mse: 1027875.6250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 645893.6875 - mae: 354.0814 - mse: 645893.8125 - val_loss: 1026642.1250 - val_mae: 610.8127 - val_mse: 1026642.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 645884.7500 - mae: 354.0957 - mse: 645884.7500 - val_loss: 1027482.4375 - val_mae: 611.0802 - val_mse: 1027482.4375 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 645832.1250 - mae: 354.2076 - mse: 645832.1250 - val_loss: 1027859.9375 - val_mae: 611.2722 - val_mse: 1027859.9375 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 122: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 645827.1250 - mae: 354.0028 - mse: 645827.1250 - val_loss: 1026583.1875 - val_mae: 610.8375 - val_mse: 1026583.1875 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 645778.6250 - mae: 354.0435 - mse: 645778.5625 - val_loss: 1026775.7500 - val_mae: 610.9098 - val_mse: 1026775.7500 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 645765.0000 - mae: 354.0674 - mse: 645765.0000 - val_loss: 1027061.8125 - val_mae: 611.0049 - val_mse: 1027061.8125 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 645761.9375 - mae: 354.0985 - mse: 645761.8750 - val_loss: 1027353.8125 - val_mae: 611.1054 - val_mse: 1027353.8125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 645758.5625 - mae: 354.1170 - mse: 645758.5625 - val_loss: 1027575.2500 - val_mae: 611.1873 - val_mse: 1027575.2500 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 645753.2500 - mae: 354.1219 - mse: 645753.1875 - val_loss: 1027682.3125 - val_mae: 611.2307 - val_mse: 1027682.3125 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 645746.3125 - mae: 354.1131 - mse: 645746.2500 - val_loss: 1027669.7500 - val_mae: 611.2330 - val_mse: 1027669.7500 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 645739.3750 - mae: 354.0930 - mse: 645739.4375 - val_loss: 1027571.8750 - val_mae: 611.2014 - val_mse: 1027571.8750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 645737.5625 - mae: 354.0803 - mse: 645737.5625 - val_loss: 1027436.5000 - val_mae: 611.1484 - val_mse: 1027436.5000 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 645733.6875 - mae: 354.0750 - mse: 645733.6875 - val_loss: 1027319.1250 - val_mae: 611.1024 - val_mse: 1027319.1250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 645730.0625 - mae: 354.0673 - mse: 645730.0625 - val_loss: 1027254.2500 - val_mae: 611.0802 - val_mse: 1027254.2500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 645727.3125 - mae: 354.0684 - mse: 645727.3125 - val_loss: 1027242.4375 - val_mae: 611.0725 - val_mse: 1027242.4375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 645728.0625 - mae: 354.0768 - mse: 645728.1250 - val_loss: 1027280.8750 - val_mae: 611.0847 - val_mse: 1027280.8750 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 645723.6250 - mae: 354.0814 - mse: 645723.7500 - val_loss: 1027349.5000 - val_mae: 611.1154 - val_mse: 1027349.5000 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 645719.0000 - mae: 354.0806 - mse: 645719.0000 - val_loss: 1027422.9375 - val_mae: 611.1514 - val_mse: 1027422.9375 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 645720.4375 - mae: 354.0747 - mse: 645720.4375 - val_loss: 1027460.5000 - val_mae: 611.1722 - val_mse: 1027460.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 645719.0000 - mae: 354.0753 - mse: 645719.0000 - val_loss: 1027435.3125 - val_mae: 611.1644 - val_mse: 1027435.3125 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 645712.6250 - mae: 354.0786 - mse: 645712.6250 - val_loss: 1027386.3750 - val_mae: 611.1495 - val_mse: 1027386.3750 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 645707.6250 - mae: 354.0765 - mse: 645707.6250 - val_loss: 1027362.1250 - val_mae: 611.1511 - val_mse: 1027362.1250 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 645706.6250 - mae: 354.0727 - mse: 645706.6250 - val_loss: 1027358.6250 - val_mae: 611.1609 - val_mse: 1027358.6250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 645705.6250 - mae: 354.0765 - mse: 645705.6875 - val_loss: 1027367.3750 - val_mae: 611.1639 - val_mse: 1027367.3750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 645701.7500 - mae: 354.0846 - mse: 645701.8125 - val_loss: 1027389.9375 - val_mae: 611.1694 - val_mse: 1027389.9375 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 645696.5625 - mae: 354.0830 - mse: 645696.5625 - val_loss: 1027422.1250 - val_mae: 611.1867 - val_mse: 1027422.1250 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 645695.3750 - mae: 354.0783 - mse: 645695.3750 - val_loss: 1027447.6875 - val_mae: 611.2033 - val_mse: 1027447.6875 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 645693.1250 - mae: 354.0823 - mse: 645693.0625 - val_loss: 1027452.7500 - val_mae: 611.2032 - val_mse: 1027452.7500 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 645690.5000 - mae: 354.0899 - mse: 645690.5000 - val_loss: 1027445.8125 - val_mae: 611.1968 - val_mse: 1027445.8125 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 645685.4375 - mae: 354.0861 - mse: 645685.4375 - val_loss: 1027437.3125 - val_mae: 611.1982 - val_mse: 1027437.3125 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 645683.7500 - mae: 354.0765 - mse: 645683.7500 - val_loss: 1027427.7500 - val_mae: 611.1999 - val_mse: 1027427.7500 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 645683.4375 - mae: 354.0747 - mse: 645683.4375 - val_loss: 1027417.2500 - val_mae: 611.1926 - val_mse: 1027417.2500 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 645678.2500 - mae: 354.0837 - mse: 645678.3125 - val_loss: 1027418.4375 - val_mae: 611.1896 - val_mse: 1027418.4375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 645673.8750 - mae: 354.0832 - mse: 645673.8125 - val_loss: 1027435.5000 - val_mae: 611.1992 - val_mse: 1027435.5000 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 645672.9375 - mae: 354.0761 - mse: 645673.0000 - val_loss: 1027446.0625 - val_mae: 611.2081 - val_mse: 1027446.0625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 645671.1250 - mae: 354.0757 - mse: 645671.1250 - val_loss: 1027421.3125 - val_mae: 611.2021 - val_mse: 1027421.3125 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 645667.3125 - mae: 354.0791 - mse: 645667.3125 - val_loss: 1027395.0000 - val_mae: 611.1979 - val_mse: 1027395.0000 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 645661.8750 - mae: 354.0774 - mse: 645661.8750 - val_loss: 1027398.9375 - val_mae: 611.2077 - val_mse: 1027398.9375 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 645660.3125 - mae: 354.0858 - mse: 645660.3125 - val_loss: 1027420.0000 - val_mae: 611.2213 - val_mse: 1027420.0000 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 645657.1250 - mae: 354.0846 - mse: 645657.1250 - val_loss: 1027447.4375 - val_mae: 611.2382 - val_mse: 1027447.4375 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 645654.0000 - mae: 354.0878 - mse: 645653.9375 - val_loss: 1027469.8750 - val_mae: 611.2452 - val_mse: 1027469.8750 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 645655.0625 - mae: 354.0945 - mse: 645655.0625 - val_loss: 1027487.6875 - val_mae: 611.2495 - val_mse: 1027487.6875 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 645648.9375 - mae: 354.0945 - mse: 645649.0000 - val_loss: 1027503.5000 - val_mae: 611.2604 - val_mse: 1027503.5000 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 645643.5000 - mae: 354.0911 - mse: 645643.5000 - val_loss: 1027507.6250 - val_mae: 611.2720 - val_mse: 1027507.6250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 645647.1250 - mae: 354.0837 - mse: 645647.1250 - val_loss: 1027493.8125 - val_mae: 611.2736 - val_mse: 1027493.8125 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 645646.1875 - mae: 354.0827 - mse: 645646.2500 - val_loss: 1027463.3750 - val_mae: 611.2581 - val_mse: 1027463.3750 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 645637.9375 - mae: 354.0867 - mse: 645638.0000 - val_loss: 1027441.1875 - val_mae: 611.2393 - val_mse: 1027441.1875 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 645639.7500 - mae: 354.0963 - mse: 645639.7500 - val_loss: 1027449.1250 - val_mae: 611.2307 - val_mse: 1027449.1250 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 645636.6875 - mae: 354.0965 - mse: 645636.6875 - val_loss: 1027474.7500 - val_mae: 611.2393 - val_mse: 1027474.7500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 645630.6250 - mae: 354.0948 - mse: 645630.6250 - val_loss: 1027483.4375 - val_mae: 611.2557 - val_mse: 1027483.4375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 645623.0000 - mae: 354.0874 - mse: 645622.9375 - val_loss: 1027487.3125 - val_mae: 611.2761 - val_mse: 1027487.3125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 645629.0000 - mae: 354.0779 - mse: 645629.0000 - val_loss: 1027498.8750 - val_mae: 611.2921 - val_mse: 1027498.8750 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 645627.5000 - mae: 354.0786 - mse: 645627.5000 - val_loss: 1027500.0625 - val_mae: 611.2903 - val_mse: 1027500.0625 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 645619.6875 - mae: 354.0861 - mse: 645619.6875 - val_loss: 1027486.2500 - val_mae: 611.2749 - val_mse: 1027486.2500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 645618.2500 - mae: 354.0985 - mse: 645618.2500 - val_loss: 1027457.4375 - val_mae: 611.2601 - val_mse: 1027457.4375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 645616.1250 - mae: 354.1020 - mse: 645616.1250 - val_loss: 1027449.6875 - val_mae: 611.2656 - val_mse: 1027449.6875 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 645609.2500 - mae: 354.1020 - mse: 645609.3125 - val_loss: 1027478.9375 - val_mae: 611.2907 - val_mse: 1027478.9375 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 645603.1875 - mae: 354.0970 - mse: 645603.1875 - val_loss: 1027520.4375 - val_mae: 611.3226 - val_mse: 1027520.4375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 645606.4375 - mae: 354.0884 - mse: 645606.3750 - val_loss: 1027547.8750 - val_mae: 611.3435 - val_mse: 1027547.8750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 645606.3125 - mae: 354.0901 - mse: 645606.3125 - val_loss: 1027540.6875 - val_mae: 611.3387 - val_mse: 1027540.6875 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 645599.0000 - mae: 354.0929 - mse: 645599.0000 - val_loss: 1027518.1875 - val_mae: 611.3206 - val_mse: 1027518.1875 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 645596.0625 - mae: 354.1021 - mse: 645596.0625 - val_loss: 1027506.4375 - val_mae: 611.3076 - val_mse: 1027506.4375 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 645592.7500 - mae: 354.1031 - mse: 645592.8125 - val_loss: 1027517.1875 - val_mae: 611.3110 - val_mse: 1027517.1875 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 645587.2500 - mae: 354.1011 - mse: 645587.2500 - val_loss: 1027532.2500 - val_mae: 611.3243 - val_mse: 1027532.2500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 645582.3750 - mae: 354.0934 - mse: 645582.3750 - val_loss: 1027517.9375 - val_mae: 611.3339 - val_mse: 1027517.9375 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 645581.8750 - mae: 354.0958 - mse: 645581.8750 - val_loss: 1027488.9375 - val_mae: 611.3330 - val_mse: 1027488.9375 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 645579.5625 - mae: 354.1017 - mse: 645579.5625 - val_loss: 1027484.2500 - val_mae: 611.3364 - val_mse: 1027484.2500 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 645575.0000 - mae: 354.0975 - mse: 645575.0000 - val_loss: 1027505.9375 - val_mae: 611.3496 - val_mse: 1027505.9375 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 645571.6250 - mae: 354.1024 - mse: 645571.6250 - val_loss: 1027539.9375 - val_mae: 611.3636 - val_mse: 1027539.9375 - lr: 1.0000e-04 - 63ms/epoch - 21ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 645569.3750 - mae: 354.0981 - mse: 645569.4375 - val_loss: 1027570.9375 - val_mae: 611.3770 - val_mse: 1027570.9375 - lr: 1.0000e-04 - 46ms/epoch - 15ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 645566.3750 - mae: 354.1006 - mse: 645566.3750 - val_loss: 1027583.9375 - val_mae: 611.3756 - val_mse: 1027583.9375 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 645564.8750 - mae: 354.1059 - mse: 645564.9375 - val_loss: 1027568.1875 - val_mae: 611.3653 - val_mse: 1027568.1875 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 645560.6250 - mae: 354.1022 - mse: 645560.5625 - val_loss: 1027535.8125 - val_mae: 611.3610 - val_mse: 1027535.8125 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 645554.6250 - mae: 354.0957 - mse: 645554.6250 - val_loss: 1027525.1250 - val_mae: 611.3641 - val_mse: 1027525.1250 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 645553.6250 - mae: 354.0962 - mse: 645553.6250 - val_loss: 1027532.5625 - val_mae: 611.3661 - val_mse: 1027532.5625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 645551.3750 - mae: 354.1043 - mse: 645551.3125 - val_loss: 1027557.8125 - val_mae: 611.3739 - val_mse: 1027557.8125 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 645546.9375 - mae: 354.1036 - mse: 645547.0625 - val_loss: 1027597.0625 - val_mae: 611.3932 - val_mse: 1027597.0625 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 645542.2500 - mae: 354.1006 - mse: 645542.2500 - val_loss: 1027636.2500 - val_mae: 611.4153 - val_mse: 1027636.2500 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 645539.8125 - mae: 354.1035 - mse: 645539.8125 - val_loss: 1027631.3125 - val_mae: 611.4213 - val_mse: 1027631.3125 - lr: 1.0000e-04 - 60ms/epoch - 20ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 645536.1875 - mae: 354.1131 - mse: 645536.1875 - val_loss: 1027610.0625 - val_mae: 611.4238 - val_mse: 1027610.0625 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 645530.5625 - mae: 354.1130 - mse: 645530.5625 - val_loss: 1027612.5000 - val_mae: 611.4374 - val_mse: 1027612.5000 - lr: 1.0000e-04 - 66ms/epoch - 22ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 645524.9375 - mae: 354.1057 - mse: 645524.9375 - val_loss: 1027628.6875 - val_mae: 611.4534 - val_mse: 1027628.6875 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 645519.8750 - mae: 354.1080 - mse: 645519.8750 - val_loss: 1027644.9375 - val_mae: 611.4592 - val_mse: 1027644.9375 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 645517.4375 - mae: 354.1133 - mse: 645517.4375 - val_loss: 1027659.3750 - val_mae: 611.4620 - val_mse: 1027659.3750 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 645508.5000 - mae: 354.1119 - mse: 645508.5625 - val_loss: 1027671.1250 - val_mae: 611.4713 - val_mse: 1027671.1250 - lr: 1.0000e-04 - 46ms/epoch - 15ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 645502.7500 - mae: 354.1038 - mse: 645502.7500 - val_loss: 1027654.5625 - val_mae: 611.4736 - val_mse: 1027654.5625 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 645497.8750 - mae: 354.1022 - mse: 645497.8750 - val_loss: 1027608.6250 - val_mae: 611.4626 - val_mse: 1027608.6250 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 645494.3750 - mae: 354.1083 - mse: 645494.3125 - val_loss: 1027588.0000 - val_mae: 611.4590 - val_mse: 1027588.0000 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 645486.6250 - mae: 354.1097 - mse: 645486.6250 - val_loss: 1027599.2500 - val_mae: 611.4731 - val_mse: 1027599.2500 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 645480.6250 - mae: 354.1051 - mse: 645480.6250 - val_loss: 1027623.0625 - val_mae: 611.4902 - val_mse: 1027623.0625 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 645474.3750 - mae: 354.1063 - mse: 645474.3750 - val_loss: 1027643.6875 - val_mae: 611.4984 - val_mse: 1027643.6875 - lr: 1.0000e-04 - 58ms/epoch - 19ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 645468.5625 - mae: 354.1108 - mse: 645468.5625 - val_loss: 1027645.3750 - val_mae: 611.5018 - val_mse: 1027645.3750 - lr: 1.0000e-04 - 46ms/epoch - 15ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 645459.3125 - mae: 354.1070 - mse: 645459.3125 - val_loss: 1027633.6250 - val_mae: 611.5137 - val_mse: 1027633.6250 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 645448.3125 - mae: 354.1014 - mse: 645448.3125 - val_loss: 1027646.1250 - val_mae: 611.5338 - val_mse: 1027646.1250 - lr: 1.0000e-04 - 64ms/epoch - 21ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 645440.2500 - mae: 354.1069 - mse: 645440.3125 - val_loss: 1027659.1875 - val_mae: 611.5442 - val_mse: 1027659.1875 - lr: 1.0000e-04 - 68ms/epoch - 23ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 645432.7500 - mae: 354.1146 - mse: 645432.7500 - val_loss: 1027668.5625 - val_mae: 611.5499 - val_mse: 1027668.5625 - lr: 1.0000e-04 - 65ms/epoch - 22ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 645421.6250 - mae: 354.1121 - mse: 645421.6250 - val_loss: 1027673.1875 - val_mae: 611.5601 - val_mse: 1027673.1875 - lr: 1.0000e-04 - 63ms/epoch - 21ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 645413.6250 - mae: 354.1027 - mse: 645413.6250 - val_loss: 1027664.3125 - val_mae: 611.5657 - val_mse: 1027664.3125 - lr: 1.0000e-04 - 75ms/epoch - 25ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 645403.7500 - mae: 354.1026 - mse: 645403.7500 - val_loss: 1027628.2500 - val_mae: 611.5578 - val_mse: 1027628.2500 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 645397.2500 - mae: 354.1073 - mse: 645397.2500 - val_loss: 1027576.8750 - val_mae: 611.5486 - val_mse: 1027576.8750 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 645386.6875 - mae: 354.1046 - mse: 645386.6875 - val_loss: 1027566.1250 - val_mae: 611.5596 - val_mse: 1027566.1250 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 645376.8125 - mae: 354.0977 - mse: 645376.8125 - val_loss: 1027587.5000 - val_mae: 611.5819 - val_mse: 1027587.5000 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 645368.7500 - mae: 354.1024 - mse: 645368.7500 - val_loss: 1027614.6250 - val_mae: 611.5980 - val_mse: 1027614.6250 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 222: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 645363.1250 - mae: 354.1113 - mse: 645363.1875 - val_loss: 1027633.3125 - val_mae: 611.6083 - val_mse: 1027633.3125 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Optimizing model by reducing: mape for epochs: 300, num_iter: 2, model: dense_model\n",
            "Epoch 1/300\n",
            "3/3 - 1s - loss: 4.2369 - mae: 532.0794 - mse: 961417.6250 - val_loss: 2.9735 - val_mae: 973.1241 - val_mse: 1752014.5000 - lr: 0.0010 - 802ms/epoch - 267ms/step\n",
            "Epoch 2/300\n",
            "3/3 - 0s - loss: 3.8931 - mae: 490.9465 - mse: 1013378.8750 - val_loss: 1.8159 - val_mae: 620.9298 - val_mse: 1026469.2500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 3/300\n",
            "3/3 - 0s - loss: 3.0796 - mae: 467.0605 - mse: 965462.0000 - val_loss: 3.0608 - val_mae: 1037.2366 - val_mse: 2019687.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 4/300\n",
            "3/3 - 0s - loss: 3.1374 - mae: 374.9992 - mse: 701410.5000 - val_loss: 2.6530 - val_mae: 871.4979 - val_mse: 1495157.1250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 5/300\n",
            "3/3 - 0s - loss: 3.1887 - mae: 404.4227 - mse: 770345.5625 - val_loss: 1.7575 - val_mae: 606.0767 - val_mse: 1022783.1250 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 6/300\n",
            "3/3 - 0s - loss: 2.7255 - mae: 391.5528 - mse: 734460.8125 - val_loss: 2.1733 - val_mae: 747.6204 - val_mse: 1322995.3750 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 7/300\n",
            "3/3 - 0s - loss: 2.7739 - mae: 365.8639 - mse: 690484.3125 - val_loss: 2.2060 - val_mae: 734.8130 - val_mse: 1204565.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 8/300\n",
            "3/3 - 0s - loss: 2.7811 - mae: 366.0692 - mse: 677522.2500 - val_loss: 1.7688 - val_mae: 613.3359 - val_mse: 1058338.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 9/300\n",
            "3/3 - 0s - loss: 2.6410 - mae: 365.3801 - mse: 667485.3125 - val_loss: 1.7557 - val_mae: 608.7408 - val_mse: 1046784.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 10/300\n",
            "3/3 - 0s - loss: 2.6103 - mae: 366.3891 - mse: 693905.6875 - val_loss: 1.9354 - val_mae: 654.5051 - val_mse: 1067366.6250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 11/300\n",
            "3/3 - 0s - loss: 2.5968 - mae: 356.3089 - mse: 655048.8125 - val_loss: 1.8602 - val_mae: 643.8663 - val_mse: 1118315.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 12/300\n",
            "3/3 - 0s - loss: 2.6215 - mae: 355.6216 - mse: 657864.1250 - val_loss: 1.8648 - val_mae: 635.3862 - val_mse: 1038920.1875 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 13/300\n",
            "3/3 - 0s - loss: 2.5827 - mae: 355.3102 - mse: 657626.6875 - val_loss: 1.7593 - val_mae: 609.8370 - val_mse: 1043639.6875 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 14/300\n",
            "3/3 - 0s - loss: 2.5656 - mae: 353.6712 - mse: 652486.8750 - val_loss: 1.7595 - val_mae: 606.8712 - val_mse: 1014812.8125 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 15/300\n",
            "3/3 - 0s - loss: 2.5479 - mae: 355.0559 - mse: 659939.5625 - val_loss: 1.7542 - val_mae: 605.8780 - val_mse: 1015627.7500 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 16/300\n",
            "3/3 - 0s - loss: 2.5396 - mae: 353.0187 - mse: 651247.0625 - val_loss: 1.7507 - val_mae: 605.8112 - val_mse: 1020785.6250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 17/300\n",
            "3/3 - 0s - loss: 2.5443 - mae: 354.7283 - mse: 659763.6875 - val_loss: 1.7740 - val_mae: 610.9994 - val_mse: 1014418.4375 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 18/300\n",
            "3/3 - 0s - loss: 2.5411 - mae: 353.1210 - mse: 651686.4375 - val_loss: 1.7564 - val_mae: 608.4498 - val_mse: 1030152.1250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 19/300\n",
            "3/3 - 0s - loss: 2.5491 - mae: 354.8795 - mse: 659914.1250 - val_loss: 1.7875 - val_mae: 614.6512 - val_mse: 1016507.1875 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 20/300\n",
            "3/3 - 0s - loss: 2.5450 - mae: 353.1618 - mse: 650989.3125 - val_loss: 1.7617 - val_mae: 610.3436 - val_mse: 1036379.6250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 21/300\n",
            "3/3 - 0s - loss: 2.5516 - mae: 355.1428 - mse: 660983.8750 - val_loss: 1.7972 - val_mae: 617.2942 - val_mse: 1019102.0000 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 22/300\n",
            "3/3 - 0s - loss: 2.5477 - mae: 353.4201 - mse: 651570.3125 - val_loss: 1.7645 - val_mae: 611.3369 - val_mse: 1041243.8125 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 23/300\n",
            "3/3 - 0s - loss: 2.5597 - mae: 356.4055 - mse: 665233.3750 - val_loss: 1.8051 - val_mae: 619.4832 - val_mse: 1021269.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 24/300\n",
            "3/3 - 0s - loss: 2.5501 - mae: 353.9233 - mse: 651880.1250 - val_loss: 1.7662 - val_mae: 611.9738 - val_mse: 1043778.0000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 25/300\n",
            "3/3 - 0s - loss: 2.5649 - mae: 356.9675 - mse: 666855.9375 - val_loss: 1.7973 - val_mae: 617.2947 - val_mse: 1019569.4375 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 26/300\n",
            "3/3 - 0s - loss: 2.5486 - mae: 354.0248 - mse: 652067.4375 - val_loss: 1.7605 - val_mae: 610.0643 - val_mse: 1039129.6250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 27/300\n",
            "3/3 - 0s - loss: 2.5677 - mae: 357.9819 - mse: 669788.5625 - val_loss: 1.7950 - val_mae: 616.7564 - val_mse: 1019034.5625 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 28/300\n",
            "3/3 - 0s - loss: 2.5536 - mae: 355.2340 - mse: 653544.6250 - val_loss: 1.7652 - val_mae: 611.7048 - val_mse: 1042045.3125 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 29/300\n",
            "3/3 - 0s - loss: 2.5799 - mae: 360.3580 - mse: 676161.9375 - val_loss: 1.8018 - val_mae: 618.7797 - val_mse: 1020487.0625 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 30/300\n",
            "3/3 - 0s - loss: 2.5650 - mae: 357.0901 - mse: 655746.5000 - val_loss: 1.7676 - val_mae: 612.4904 - val_mse: 1042602.0000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 31/300\n",
            "3/3 - 0s - loss: 2.5872 - mae: 362.3976 - mse: 682061.2500 - val_loss: 1.8064 - val_mae: 620.1163 - val_mse: 1021362.3750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 32/300\n",
            "3/3 - 0s - loss: 2.5750 - mae: 358.9748 - mse: 659992.4375 - val_loss: 1.7751 - val_mae: 615.1119 - val_mse: 1049360.6250 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 33/300\n",
            "3/3 - 0s - loss: 2.6039 - mae: 365.0410 - mse: 689517.3125 - val_loss: 1.8138 - val_mae: 622.3538 - val_mse: 1023316.0625 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 34/300\n",
            "3/3 - 0s - loss: 2.5869 - mae: 361.3226 - mse: 662646.1250 - val_loss: 1.7877 - val_mae: 619.4431 - val_mse: 1059580.3750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 35/300\n",
            "3/3 - 0s - loss: 2.6253 - mae: 368.5726 - mse: 699064.1250 - val_loss: 1.8301 - val_mae: 627.0131 - val_mse: 1028143.9375 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 36/300\n",
            "3/3 - 0s - loss: 2.6049 - mae: 364.3737 - mse: 668789.6250 - val_loss: 1.8147 - val_mae: 628.2067 - val_mse: 1078894.5000 - lr: 0.0010 - 52ms/epoch - 17ms/step\n",
            "Epoch 37/300\n",
            "3/3 - 0s - loss: 2.6464 - mae: 371.9601 - mse: 709417.0000 - val_loss: 1.8572 - val_mae: 634.4512 - val_mse: 1036553.1875 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 38/300\n",
            "3/3 - 0s - loss: 2.6260 - mae: 367.8864 - mse: 678243.8125 - val_loss: 1.8596 - val_mae: 642.8959 - val_mse: 1108678.6250 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 39/300\n",
            "3/3 - 0s - loss: 2.6773 - mae: 376.0070 - mse: 721374.9375 - val_loss: 1.9199 - val_mae: 651.9261 - val_mse: 1060940.3750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 40/300\n",
            "3/3 - 0s - loss: 2.6537 - mae: 370.5171 - mse: 683759.2500 - val_loss: 1.9386 - val_mae: 669.1089 - val_mse: 1159386.3750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 41/300\n",
            "3/3 - 0s - loss: 2.6992 - mae: 376.5916 - mse: 724477.5000 - val_loss: 2.0016 - val_mae: 675.4287 - val_mse: 1099074.1250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 42/300\n",
            "3/3 - 0s - loss: 2.6702 - mae: 368.8855 - mse: 680772.0625 - val_loss: 2.0025 - val_mae: 689.8938 - val_mse: 1202265.5000 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 43/300\n",
            "3/3 - 0s - loss: 2.7100 - mae: 373.9079 - mse: 716750.3750 - val_loss: 2.0586 - val_mae: 692.3005 - val_mse: 1127814.8750 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 44/300\n",
            "3/3 - 0s - loss: 2.6744 - mae: 365.8933 - mse: 674331.1875 - val_loss: 2.0193 - val_mae: 695.4136 - val_mse: 1214310.1250 - lr: 0.0010 - 50ms/epoch - 17ms/step\n",
            "Epoch 45/300\n",
            "3/3 - 0s - loss: 2.7057 - mae: 370.7740 - mse: 707308.6875 - val_loss: 2.0557 - val_mae: 691.3551 - val_mse: 1126268.8750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 46/300\n",
            "3/3 - 0s - loss: 2.6682 - mae: 364.0309 - mse: 669470.7500 - val_loss: 2.0062 - val_mae: 691.2380 - val_mse: 1205749.7500 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 47/300\n",
            "3/3 - 0s - loss: 2.6940 - mae: 369.1560 - mse: 702686.1875 - val_loss: 2.0537 - val_mae: 690.7694 - val_mse: 1125189.0000 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 48/300\n",
            "3/3 - 0s - loss: 2.6633 - mae: 362.6831 - mse: 667128.1250 - val_loss: 1.9965 - val_mae: 688.0765 - val_mse: 1199451.2500 - lr: 0.0010 - 54ms/epoch - 18ms/step\n",
            "Epoch 49/300\n",
            "3/3 - 0s - loss: 2.6894 - mae: 368.5572 - mse: 701142.0000 - val_loss: 2.0454 - val_mae: 688.3506 - val_mse: 1121059.8750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 50/300\n",
            "3/3 - 0s - loss: 2.6597 - mae: 362.4397 - mse: 666447.6250 - val_loss: 1.9928 - val_mae: 686.8517 - val_mse: 1196383.2500 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 51/300\n",
            "3/3 - 0s - loss: 2.6842 - mae: 367.7531 - mse: 698560.5000 - val_loss: 2.0544 - val_mae: 691.0652 - val_mse: 1125698.7500 - lr: 0.0010 - 44ms/epoch - 15ms/step\n",
            "Epoch 52/300\n",
            "3/3 - 0s - loss: 2.6636 - mae: 362.3407 - mse: 666120.0625 - val_loss: 2.0176 - val_mae: 694.8594 - val_mse: 1212828.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 53/300\n",
            "3/3 - 0s - loss: 2.6914 - mae: 366.1071 - mse: 692888.5625 - val_loss: 2.0558 - val_mae: 691.5529 - val_mse: 1126274.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 54/300\n",
            "3/3 - 0s - loss: 2.6583 - mae: 360.4761 - mse: 662058.3125 - val_loss: 1.9760 - val_mae: 681.2766 - val_mse: 1185146.7500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 55/300\n",
            "3/3 - 0s - loss: 2.6719 - mae: 364.7426 - mse: 689001.3125 - val_loss: 2.0335 - val_mae: 684.8896 - val_mse: 1114755.1250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 56/300\n",
            "3/3 - 0s - loss: 2.6481 - mae: 359.9052 - mse: 661149.8750 - val_loss: 1.9602 - val_mae: 676.1300 - val_mse: 1174360.3750 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 57/300\n",
            "3/3 - 0s - loss: 2.6645 - mae: 365.0136 - mse: 690199.0625 - val_loss: 2.0278 - val_mae: 683.1823 - val_mse: 1112251.7500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 58/300\n",
            "3/3 - 0s - loss: 2.6478 - mae: 360.3566 - mse: 661747.0000 - val_loss: 1.9645 - val_mae: 677.5346 - val_mse: 1176937.6250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 59/300\n",
            "3/3 - 0s - loss: 2.6637 - mae: 364.5363 - mse: 688388.0000 - val_loss: 2.0341 - val_mae: 685.0862 - val_mse: 1115678.0000 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 60/300\n",
            "3/3 - 0s - loss: 2.6479 - mae: 359.5188 - mse: 660010.0000 - val_loss: 1.9556 - val_mae: 674.6489 - val_mse: 1171231.0000 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 61/300\n",
            "3/3 - 0s - loss: 2.6605 - mae: 364.1725 - mse: 687424.6875 - val_loss: 2.0251 - val_mae: 682.4221 - val_mse: 1110685.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 62/300\n",
            "3/3 - 0s - loss: 2.6419 - mae: 359.1740 - mse: 659674.3125 - val_loss: 1.9462 - val_mae: 671.5726 - val_mse: 1165046.3750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 63/300\n",
            "3/3 - 0s - loss: 2.6603 - mae: 364.9108 - mse: 689957.9375 - val_loss: 2.0260 - val_mae: 682.7230 - val_mse: 1111143.5000 - lr: 0.0010 - 46ms/epoch - 15ms/step\n",
            "Epoch 64/300\n",
            "3/3 - 0s - loss: 2.6413 - mae: 359.5309 - mse: 660415.0000 - val_loss: 1.9546 - val_mae: 674.3088 - val_mse: 1170639.3750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 65/300\n",
            "3/3 - 0s - loss: 2.6652 - mae: 365.1800 - mse: 690676.1875 - val_loss: 2.0296 - val_mae: 683.8262 - val_mse: 1113135.7500 - lr: 0.0010 - 53ms/epoch - 18ms/step\n",
            "Epoch 66/300\n",
            "3/3 - 0s - loss: 2.6428 - mae: 359.5782 - mse: 660379.9375 - val_loss: 1.9549 - val_mae: 674.4406 - val_mse: 1170786.2500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 67/300\n",
            "3/3 - 0s - loss: 2.6649 - mae: 365.1809 - mse: 690771.5625 - val_loss: 2.0245 - val_mae: 682.3199 - val_mse: 1110578.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 68/300\n",
            "3/3 - 0s - loss: 2.6409 - mae: 359.6840 - mse: 660793.3125 - val_loss: 1.9566 - val_mae: 674.9661 - val_mse: 1171705.5000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 69/300\n",
            "3/3 - 0s - loss: 2.6666 - mae: 365.5132 - mse: 691859.8750 - val_loss: 2.0331 - val_mae: 684.8553 - val_mse: 1115057.3750 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 70/300\n",
            "3/3 - 0s - loss: 2.6436 - mae: 359.5843 - mse: 660517.0000 - val_loss: 1.9640 - val_mae: 677.3474 - val_mse: 1176461.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 71/300\n",
            "3/3 - 0s - loss: 2.6671 - mae: 364.6646 - mse: 688935.3750 - val_loss: 2.0278 - val_mae: 683.3592 - val_mse: 1112398.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 72/300\n",
            "3/3 - 0s - loss: 2.6415 - mae: 359.4244 - mse: 660132.1875 - val_loss: 1.9598 - val_mae: 676.0215 - val_mse: 1173768.6250 - lr: 0.0010 - 40ms/epoch - 13ms/step\n",
            "Epoch 73/300\n",
            "3/3 - 0s - loss: 2.6659 - mae: 364.6416 - mse: 688715.3750 - val_loss: 2.0301 - val_mae: 684.0854 - val_mse: 1113730.1250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 74/300\n",
            "3/3 - 0s - loss: 2.6419 - mae: 359.1843 - mse: 659346.6875 - val_loss: 1.9544 - val_mae: 674.2698 - val_mse: 1169878.1250 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 75/300\n",
            "3/3 - 0s - loss: 2.6617 - mae: 364.2258 - mse: 687433.6250 - val_loss: 2.0257 - val_mae: 682.8137 - val_mse: 1111579.8750 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 76/300\n",
            "3/3 - 0s - loss: 2.6393 - mae: 358.8437 - mse: 659085.7500 - val_loss: 1.9495 - val_mae: 672.6316 - val_mse: 1166304.2500 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 77/300\n",
            "3/3 - 0s - loss: 2.6593 - mae: 364.1830 - mse: 687571.3125 - val_loss: 2.0173 - val_mae: 680.2881 - val_mse: 1107167.5000 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 78/300\n",
            "3/3 - 0s - loss: 2.6356 - mae: 358.7091 - mse: 659192.7500 - val_loss: 1.9480 - val_mae: 672.0776 - val_mse: 1164997.6250 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 79/300\n",
            "3/3 - 0s - loss: 2.6575 - mae: 363.9018 - mse: 686619.8125 - val_loss: 2.0187 - val_mae: 680.6818 - val_mse: 1107802.8750 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 80/300\n",
            "3/3 - 0s - loss: 2.6347 - mae: 358.3847 - mse: 658357.5625 - val_loss: 1.9418 - val_mae: 670.0675 - val_mse: 1161088.6250 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 81/300\n",
            "3/3 - 0s - loss: 2.6553 - mae: 363.8243 - mse: 686236.0000 - val_loss: 2.0181 - val_mae: 680.5201 - val_mse: 1107574.3750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 82/300\n",
            "3/3 - 0s - loss: 2.6352 - mae: 358.4825 - mse: 658266.1250 - val_loss: 1.9446 - val_mae: 671.0198 - val_mse: 1162939.7500 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 83/300\n",
            "3/3 - 0s - loss: 2.6556 - mae: 363.7852 - mse: 686155.0625 - val_loss: 2.0194 - val_mae: 680.9139 - val_mse: 1108305.5000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 84/300\n",
            "3/3 - 0s - loss: 2.6360 - mae: 358.5534 - mse: 658739.5625 - val_loss: 1.9454 - val_mae: 671.2175 - val_mse: 1163562.5000 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 85/300\n",
            "3/3 - 0s - loss: 2.6570 - mae: 364.0159 - mse: 686867.0000 - val_loss: 2.0241 - val_mae: 682.3091 - val_mse: 1110757.5000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 86/300\n",
            "3/3 - 0s - loss: 2.6385 - mae: 358.9522 - mse: 659183.1250 - val_loss: 1.9537 - val_mae: 673.9735 - val_mse: 1168931.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 87/300\n",
            "3/3 - 0s - loss: 2.6599 - mae: 363.8827 - mse: 686147.2500 - val_loss: 2.0280 - val_mae: 683.4597 - val_mse: 1112766.8750 - lr: 0.0010 - 31ms/epoch - 10ms/step\n",
            "Epoch 88/300\n",
            "3/3 - 0s - loss: 2.6392 - mae: 358.8147 - mse: 659064.7500 - val_loss: 1.9587 - val_mae: 675.5804 - val_mse: 1172312.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 89/300\n",
            "3/3 - 0s - loss: 2.6637 - mae: 364.2850 - mse: 687391.5625 - val_loss: 2.0352 - val_mae: 685.6046 - val_mse: 1116570.7500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 90/300\n",
            "3/3 - 0s - loss: 2.6422 - mae: 358.9566 - mse: 659025.0625 - val_loss: 1.9607 - val_mae: 676.2395 - val_mse: 1173469.3750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 91/300\n",
            "3/3 - 0s - loss: 2.6637 - mae: 364.2904 - mse: 687529.5625 - val_loss: 2.0331 - val_mae: 684.9792 - val_mse: 1115499.7500 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 92/300\n",
            "3/3 - 0s - loss: 2.6416 - mae: 358.8288 - mse: 659263.8125 - val_loss: 1.9628 - val_mae: 676.9304 - val_mse: 1174974.2500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 93/300\n",
            "3/3 - 0s - loss: 2.6641 - mae: 364.0117 - mse: 686741.2500 - val_loss: 2.0289 - val_mae: 683.7711 - val_mse: 1113356.1250 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 94/300\n",
            "3/3 - 0s - loss: 2.6390 - mae: 358.7025 - mse: 658884.3750 - val_loss: 1.9587 - val_mae: 675.6036 - val_mse: 1172136.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 95/300\n",
            "3/3 - 0s - loss: 2.6622 - mae: 363.9761 - mse: 686501.3750 - val_loss: 2.0280 - val_mae: 683.5020 - val_mse: 1113017.5000 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 96/300\n",
            "3/3 - 0s - loss: 2.6384 - mae: 358.6258 - mse: 658504.7500 - val_loss: 1.9554 - val_mae: 674.5546 - val_mse: 1169983.1250 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 97/300\n",
            "3/3 - 0s - loss: 2.6598 - mae: 363.3866 - mse: 684536.0625 - val_loss: 2.0189 - val_mae: 680.8458 - val_mse: 1108264.1250 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 98/300\n",
            "3/3 - 0s - loss: 2.6334 - mae: 358.0970 - mse: 657671.8125 - val_loss: 1.9463 - val_mae: 671.6010 - val_mse: 1164106.8750 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 99/300\n",
            "3/3 - 0s - loss: 2.6564 - mae: 363.2378 - mse: 684151.8750 - val_loss: 2.0123 - val_mae: 678.9298 - val_mse: 1104887.1250 - lr: 0.0010 - 49ms/epoch - 16ms/step\n",
            "Epoch 100/300\n",
            "3/3 - 0s - loss: 2.6303 - mae: 358.0798 - mse: 657524.1250 - val_loss: 1.9375 - val_mae: 668.7153 - val_mse: 1158186.5000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 101/300\n",
            "3/3 - 0s - loss: 2.6521 - mae: 363.4702 - mse: 685150.2500 - val_loss: 2.0086 - val_mae: 677.8561 - val_mse: 1103181.5000 - lr: 0.0010 - 48ms/epoch - 16ms/step\n",
            "Epoch 102/300\n",
            "3/3 - 0s - loss: 2.6287 - mae: 357.9586 - mse: 657459.1250 - val_loss: 1.9342 - val_mae: 667.6265 - val_mse: 1156121.2500 - lr: 0.0010 - 32ms/epoch - 11ms/step\n",
            "Epoch 103/300\n",
            "3/3 - 0s - loss: 2.6510 - mae: 363.8098 - mse: 686348.0625 - val_loss: 2.0095 - val_mae: 678.0923 - val_mse: 1103784.7500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 104/300\n",
            "3/3 - 0s - loss: 2.6305 - mae: 358.4252 - mse: 658157.0000 - val_loss: 1.9365 - val_mae: 668.4355 - val_mse: 1157659.0000 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 105/300\n",
            "3/3 - 0s - loss: 2.6512 - mae: 363.7903 - mse: 686166.3125 - val_loss: 2.0125 - val_mae: 678.9496 - val_mse: 1105074.0000 - lr: 0.0010 - 51ms/epoch - 17ms/step\n",
            "Epoch 106/300\n",
            "3/3 - 0s - loss: 2.6310 - mae: 358.3392 - mse: 658287.0625 - val_loss: 1.9382 - val_mae: 668.9619 - val_mse: 1158968.0000 - lr: 0.0010 - 39ms/epoch - 13ms/step\n",
            "Epoch 107/300\n",
            "3/3 - 0s - loss: 2.6543 - mae: 364.2649 - mse: 687849.7500 - val_loss: 2.0135 - val_mae: 679.2791 - val_mse: 1105487.3750 - lr: 0.0010 - 36ms/epoch - 12ms/step\n",
            "Epoch 108/300\n",
            "3/3 - 0s - loss: 2.6313 - mae: 358.5809 - mse: 658898.5625 - val_loss: 1.9425 - val_mae: 670.3508 - val_mse: 1161820.3750 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 109/300\n",
            "3/3 - 0s - loss: 2.6563 - mae: 364.3737 - mse: 688118.3125 - val_loss: 2.0189 - val_mae: 680.8280 - val_mse: 1108313.2500 - lr: 0.0010 - 33ms/epoch - 11ms/step\n",
            "Epoch 110/300\n",
            "3/3 - 0s - loss: 2.6338 - mae: 358.6539 - mse: 658786.0625 - val_loss: 1.9471 - val_mae: 671.8915 - val_mse: 1164863.8750 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 111/300\n",
            "3/3 - 0s - loss: 2.6559 - mae: 363.9642 - mse: 686680.7500 - val_loss: 2.0205 - val_mae: 681.2634 - val_mse: 1109286.7500 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 112/300\n",
            "3/3 - 0s - loss: 2.6351 - mae: 358.4025 - mse: 658193.1875 - val_loss: 1.9442 - val_mae: 670.9536 - val_mse: 1163083.5000 - lr: 0.0010 - 37ms/epoch - 12ms/step\n",
            "Epoch 113/300\n",
            "3/3 - 0s - loss: 2.6544 - mae: 363.8522 - mse: 686469.9375 - val_loss: 2.0192 - val_mae: 680.8940 - val_mse: 1108446.6250 - lr: 0.0010 - 34ms/epoch - 11ms/step\n",
            "Epoch 114/300\n",
            "3/3 - 0s - loss: 2.6340 - mae: 358.4368 - mse: 658637.3750 - val_loss: 1.9469 - val_mae: 671.8420 - val_mse: 1165183.2500 - lr: 0.0010 - 47ms/epoch - 16ms/step\n",
            "Epoch 115/300\n",
            "3/3 - 0s - loss: 2.6573 - mae: 363.9732 - mse: 686871.8125 - val_loss: 2.0242 - val_mae: 682.4156 - val_mse: 1110891.0000 - lr: 0.0010 - 35ms/epoch - 12ms/step\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "3/3 - 0s - loss: 2.6355 - mae: 358.6977 - mse: 658808.7500 - val_loss: 1.9555 - val_mae: 674.6801 - val_mse: 1171060.3750 - lr: 0.0010 - 38ms/epoch - 13ms/step\n",
            "Epoch 117/300\n",
            "3/3 - 0s - loss: 2.7079 - mae: 363.5142 - mse: 662131.8125 - val_loss: 1.8122 - val_mae: 627.4127 - val_mse: 1076929.7500 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 118/300\n",
            "3/3 - 0s - loss: 2.5790 - mae: 353.0980 - mse: 650077.4375 - val_loss: 1.7679 - val_mae: 609.6355 - val_mse: 1016235.2500 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 119/300\n",
            "3/3 - 0s - loss: 2.5545 - mae: 357.9798 - mse: 669256.4375 - val_loss: 1.8723 - val_mae: 638.9065 - val_mse: 1042295.0625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 120/300\n",
            "3/3 - 0s - loss: 2.5994 - mae: 360.4886 - mse: 674098.4375 - val_loss: 1.8416 - val_mae: 630.2891 - val_mse: 1032239.3750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 121/300\n",
            "3/3 - 0s - loss: 2.5613 - mae: 354.3931 - mse: 657090.6250 - val_loss: 1.7573 - val_mae: 606.8903 - val_mse: 1016580.0625 - lr: 1.0000e-04 - 54ms/epoch - 18ms/step\n",
            "Epoch 122/300\n",
            "3/3 - 0s - loss: 2.5335 - mae: 352.3901 - mse: 649438.8125 - val_loss: 1.7592 - val_mae: 609.8384 - val_mse: 1035575.1250 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 123/300\n",
            "3/3 - 0s - loss: 2.5547 - mae: 353.7231 - mse: 649902.9375 - val_loss: 1.7619 - val_mae: 610.7855 - val_mse: 1038046.8125 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 124/300\n",
            "3/3 - 0s - loss: 2.5503 - mae: 352.7140 - mse: 649587.6250 - val_loss: 1.7504 - val_mae: 605.7733 - val_mse: 1021192.0625 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 125/300\n",
            "3/3 - 0s - loss: 2.5350 - mae: 352.8519 - mse: 653269.8750 - val_loss: 1.7705 - val_mae: 610.3463 - val_mse: 1016320.2500 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 126/300\n",
            "3/3 - 0s - loss: 2.5394 - mae: 354.1431 - mse: 657639.1250 - val_loss: 1.7824 - val_mae: 613.5941 - val_mse: 1017668.2500 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 127/300\n",
            "3/3 - 0s - loss: 2.5394 - mae: 353.5117 - mse: 655479.3750 - val_loss: 1.7636 - val_mae: 608.5361 - val_mse: 1016091.0000 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 128/300\n",
            "3/3 - 0s - loss: 2.5323 - mae: 352.4227 - mse: 651511.4375 - val_loss: 1.7510 - val_mae: 605.6946 - val_mse: 1019533.8750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 129/300\n",
            "3/3 - 0s - loss: 2.5340 - mae: 352.3005 - mse: 650157.6250 - val_loss: 1.7504 - val_mae: 605.9722 - val_mse: 1022268.7500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 130/300\n",
            "3/3 - 0s - loss: 2.5358 - mae: 352.3566 - mse: 650517.7500 - val_loss: 1.7515 - val_mae: 605.7510 - val_mse: 1019049.5000 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 131/300\n",
            "3/3 - 0s - loss: 2.5333 - mae: 352.5617 - mse: 652114.5000 - val_loss: 1.7581 - val_mae: 607.1150 - val_mse: 1016418.9375 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 132/300\n",
            "3/3 - 0s - loss: 2.5332 - mae: 352.9301 - mse: 653739.9375 - val_loss: 1.7633 - val_mae: 608.4714 - val_mse: 1016063.3750 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 133/300\n",
            "3/3 - 0s - loss: 2.5333 - mae: 352.8499 - mse: 653373.2500 - val_loss: 1.7588 - val_mae: 607.3018 - val_mse: 1016319.4375 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 134/300\n",
            "3/3 - 0s - loss: 2.5321 - mae: 352.5291 - mse: 651997.5000 - val_loss: 1.7539 - val_mae: 606.2076 - val_mse: 1017499.8125 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 135/300\n",
            "3/3 - 0s - loss: 2.5322 - mae: 352.3944 - mse: 651237.8125 - val_loss: 1.7524 - val_mae: 605.9264 - val_mse: 1018334.2500 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 136/300\n",
            "3/3 - 0s - loss: 2.5327 - mae: 352.4066 - mse: 651235.3750 - val_loss: 1.7531 - val_mae: 606.0490 - val_mse: 1017909.6250 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 137/300\n",
            "3/3 - 0s - loss: 2.5325 - mae: 352.5060 - mse: 651814.0625 - val_loss: 1.7556 - val_mae: 606.5579 - val_mse: 1016900.0000 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 138/300\n",
            "3/3 - 0s - loss: 2.5323 - mae: 352.6176 - mse: 652444.4375 - val_loss: 1.7571 - val_mae: 606.9090 - val_mse: 1016522.5625 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 139/300\n",
            "3/3 - 0s - loss: 2.5322 - mae: 352.6012 - mse: 652378.6875 - val_loss: 1.7561 - val_mae: 606.6689 - val_mse: 1016756.0625 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 140/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5146 - mse: 651932.8750 - val_loss: 1.7545 - val_mae: 606.3495 - val_mse: 1017261.0625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 141/300\n",
            "3/3 - 0s - loss: 2.5321 - mae: 352.4835 - mse: 651740.5625 - val_loss: 1.7543 - val_mae: 606.3059 - val_mse: 1017345.1875 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 142/300\n",
            "3/3 - 0s - loss: 2.5322 - mae: 352.5175 - mse: 651918.7500 - val_loss: 1.7552 - val_mae: 606.5038 - val_mse: 1016998.0625 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 143/300\n",
            "3/3 - 0s - loss: 2.5321 - mae: 352.5689 - mse: 652219.0625 - val_loss: 1.7561 - val_mae: 606.6949 - val_mse: 1016724.5625 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 144/300\n",
            "3/3 - 0s - loss: 2.5321 - mae: 352.5698 - mse: 652226.4375 - val_loss: 1.7557 - val_mae: 606.6175 - val_mse: 1016824.7500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 145/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5263 - mse: 651986.0000 - val_loss: 1.7550 - val_mae: 606.4569 - val_mse: 1017080.9375 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 146/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5088 - mse: 651881.8750 - val_loss: 1.7548 - val_mae: 606.4295 - val_mse: 1017129.3125 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 147/300\n",
            "3/3 - 0s - loss: 2.5321 - mae: 352.5290 - mse: 651998.4375 - val_loss: 1.7554 - val_mae: 606.5491 - val_mse: 1016930.0000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 148/300\n",
            "3/3 - 0s - loss: 2.5321 - mae: 352.5528 - mse: 652141.5000 - val_loss: 1.7558 - val_mae: 606.6339 - val_mse: 1016801.7500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 149/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5488 - mse: 652120.5625 - val_loss: 1.7556 - val_mae: 606.5922 - val_mse: 1016865.8750 - lr: 1.0000e-04 - 51ms/epoch - 17ms/step\n",
            "Epoch 150/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5262 - mse: 651989.1875 - val_loss: 1.7551 - val_mae: 606.5032 - val_mse: 1017011.3750 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 151/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5226 - mse: 651964.0625 - val_loss: 1.7552 - val_mae: 606.5154 - val_mse: 1016992.5625 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 152/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5372 - mse: 652049.3750 - val_loss: 1.7555 - val_mae: 606.5880 - val_mse: 1016877.9375 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 153/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5467 - mse: 652108.3750 - val_loss: 1.7556 - val_mae: 606.6122 - val_mse: 1016845.7500 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 154/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5370 - mse: 652057.1250 - val_loss: 1.7554 - val_mae: 606.5691 - val_mse: 1016915.6875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 155/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5315 - mse: 652026.0625 - val_loss: 1.7553 - val_mae: 606.5598 - val_mse: 1016935.0625 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 156/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5338 - mse: 652036.5000 - val_loss: 1.7554 - val_mae: 606.5809 - val_mse: 1016910.6875 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 157/300\n",
            "3/3 - 0s - loss: 2.5320 - mae: 352.5356 - mse: 652046.5625 - val_loss: 1.7555 - val_mae: 606.5966 - val_mse: 1016895.1875 - lr: 1.0000e-04 - 36ms/epoch - 12ms/step\n",
            "Epoch 158/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5356 - mse: 652044.1875 - val_loss: 1.7555 - val_mae: 606.6029 - val_mse: 1016892.3125 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 159/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5363 - mse: 652048.8750 - val_loss: 1.7555 - val_mae: 606.6092 - val_mse: 1016888.0625 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 160/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5368 - mse: 652055.5000 - val_loss: 1.7556 - val_mae: 606.6154 - val_mse: 1016882.6250 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 161/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5326 - mse: 652033.4375 - val_loss: 1.7554 - val_mae: 606.5859 - val_mse: 1016933.9375 - lr: 1.0000e-04 - 48ms/epoch - 16ms/step\n",
            "Epoch 162/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5271 - mse: 652000.3750 - val_loss: 1.7554 - val_mae: 606.5858 - val_mse: 1016941.8750 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 163/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5278 - mse: 652003.0000 - val_loss: 1.7555 - val_mae: 606.6075 - val_mse: 1016917.5000 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 164/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5369 - mse: 652048.3750 - val_loss: 1.7557 - val_mae: 606.6506 - val_mse: 1016860.1875 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 165/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5344 - mse: 652034.8125 - val_loss: 1.7555 - val_mae: 606.6211 - val_mse: 1016912.0000 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 166/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5250 - mse: 651987.3750 - val_loss: 1.7554 - val_mae: 606.5981 - val_mse: 1016953.6875 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 167/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5383 - mse: 652064.2500 - val_loss: 1.7557 - val_mae: 606.6597 - val_mse: 1016861.8125 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 168/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5333 - mse: 652040.3750 - val_loss: 1.7555 - val_mae: 606.6190 - val_mse: 1016927.2500 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 169/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5258 - mse: 651998.8750 - val_loss: 1.7555 - val_mae: 606.6221 - val_mse: 1016921.5625 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 170/300\n",
            "3/3 - 0s - loss: 2.5319 - mae: 352.5354 - mse: 652051.6250 - val_loss: 1.7557 - val_mae: 606.6618 - val_mse: 1016858.5625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 171/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5302 - mse: 652022.5000 - val_loss: 1.7554 - val_mae: 606.6199 - val_mse: 1016919.5000 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 172/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5305 - mse: 652025.3750 - val_loss: 1.7555 - val_mae: 606.6403 - val_mse: 1016882.1875 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 173/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5339 - mse: 652051.6250 - val_loss: 1.7555 - val_mae: 606.6400 - val_mse: 1016879.5000 - lr: 1.0000e-04 - 63ms/epoch - 21ms/step\n",
            "Epoch 174/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5272 - mse: 652018.5000 - val_loss: 1.7554 - val_mae: 606.6108 - val_mse: 1016921.1875 - lr: 1.0000e-04 - 68ms/epoch - 23ms/step\n",
            "Epoch 175/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5315 - mse: 652034.0625 - val_loss: 1.7556 - val_mae: 606.6551 - val_mse: 1016847.4375 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 176/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5290 - mse: 652024.2500 - val_loss: 1.7555 - val_mae: 606.6219 - val_mse: 1016892.6875 - lr: 1.0000e-04 - 62ms/epoch - 21ms/step\n",
            "Epoch 177/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5285 - mse: 652026.4375 - val_loss: 1.7555 - val_mae: 606.6335 - val_mse: 1016868.4375 - lr: 1.0000e-04 - 65ms/epoch - 22ms/step\n",
            "Epoch 178/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5310 - mse: 652043.5625 - val_loss: 1.7555 - val_mae: 606.6410 - val_mse: 1016851.7500 - lr: 1.0000e-04 - 41ms/epoch - 14ms/step\n",
            "Epoch 179/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5296 - mse: 652035.1875 - val_loss: 1.7555 - val_mae: 606.6329 - val_mse: 1016856.3125 - lr: 1.0000e-04 - 57ms/epoch - 19ms/step\n",
            "Epoch 180/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5362 - mse: 652071.3750 - val_loss: 1.7557 - val_mae: 606.6650 - val_mse: 1016798.5000 - lr: 1.0000e-04 - 55ms/epoch - 18ms/step\n",
            "Epoch 181/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5305 - mse: 652047.2500 - val_loss: 1.7554 - val_mae: 606.6161 - val_mse: 1016865.7500 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 182/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5277 - mse: 652032.3125 - val_loss: 1.7554 - val_mae: 606.6177 - val_mse: 1016858.2500 - lr: 1.0000e-04 - 38ms/epoch - 13ms/step\n",
            "Epoch 183/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5301 - mse: 652046.6250 - val_loss: 1.7556 - val_mae: 606.6398 - val_mse: 1016819.8750 - lr: 1.0000e-04 - 42ms/epoch - 14ms/step\n",
            "Epoch 184/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5278 - mse: 652035.6875 - val_loss: 1.7555 - val_mae: 606.6259 - val_mse: 1016836.6250 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 185/300\n",
            "3/3 - 0s - loss: 2.5318 - mae: 352.5339 - mse: 652062.4375 - val_loss: 1.7557 - val_mae: 606.6683 - val_mse: 1016765.5000 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 186/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5292 - mse: 652038.9375 - val_loss: 1.7555 - val_mae: 606.6248 - val_mse: 1016823.8750 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 187/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5276 - mse: 652035.1250 - val_loss: 1.7555 - val_mae: 606.6320 - val_mse: 1016803.3750 - lr: 1.0000e-04 - 45ms/epoch - 15ms/step\n",
            "Epoch 188/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5334 - mse: 652073.2500 - val_loss: 1.7556 - val_mae: 606.6448 - val_mse: 1016774.9375 - lr: 1.0000e-04 - 64ms/epoch - 21ms/step\n",
            "Epoch 189/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5214 - mse: 652017.2500 - val_loss: 1.7553 - val_mae: 606.5826 - val_mse: 1016862.8125 - lr: 1.0000e-04 - 43ms/epoch - 14ms/step\n",
            "Epoch 190/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5265 - mse: 652045.6250 - val_loss: 1.7555 - val_mae: 606.6262 - val_mse: 1016788.8750 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 191/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5409 - mse: 652125.1250 - val_loss: 1.7558 - val_mae: 606.6793 - val_mse: 1016704.4375 - lr: 1.0000e-04 - 52ms/epoch - 17ms/step\n",
            "Epoch 192/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5307 - mse: 652069.8750 - val_loss: 1.7555 - val_mae: 606.6221 - val_mse: 1016784.1875 - lr: 1.0000e-04 - 64ms/epoch - 21ms/step\n",
            "Epoch 193/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5306 - mse: 652066.9375 - val_loss: 1.7556 - val_mae: 606.6414 - val_mse: 1016746.0000 - lr: 1.0000e-04 - 46ms/epoch - 15ms/step\n",
            "Epoch 194/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5277 - mse: 652057.5000 - val_loss: 1.7554 - val_mae: 606.6051 - val_mse: 1016793.4375 - lr: 1.0000e-04 - 47ms/epoch - 16ms/step\n",
            "Epoch 195/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5249 - mse: 652046.9375 - val_loss: 1.7554 - val_mae: 606.6091 - val_mse: 1016779.9375 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 196/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5285 - mse: 652068.1250 - val_loss: 1.7555 - val_mae: 606.6299 - val_mse: 1016740.1250 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 197/300\n",
            "3/3 - 0s - loss: 2.5317 - mae: 352.5350 - mse: 652107.6875 - val_loss: 1.7556 - val_mae: 606.6481 - val_mse: 1016703.0000 - lr: 1.0000e-04 - 82ms/epoch - 27ms/step\n",
            "Epoch 198/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5221 - mse: 652041.3750 - val_loss: 1.7553 - val_mae: 606.5762 - val_mse: 1016808.0000 - lr: 1.0000e-04 - 59ms/epoch - 20ms/step\n",
            "Epoch 199/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5182 - mse: 652018.5000 - val_loss: 1.7554 - val_mae: 606.5940 - val_mse: 1016772.2500 - lr: 1.0000e-04 - 53ms/epoch - 18ms/step\n",
            "Epoch 200/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5270 - mse: 652069.1250 - val_loss: 1.7555 - val_mae: 606.6205 - val_mse: 1016722.8750 - lr: 1.0000e-04 - 44ms/epoch - 15ms/step\n",
            "Epoch 201/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5282 - mse: 652078.6875 - val_loss: 1.7555 - val_mae: 606.6167 - val_mse: 1016720.8125 - lr: 1.0000e-04 - 56ms/epoch - 19ms/step\n",
            "Epoch 202/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5272 - mse: 652071.8750 - val_loss: 1.7555 - val_mae: 606.6175 - val_mse: 1016710.1250 - lr: 1.0000e-04 - 66ms/epoch - 22ms/step\n",
            "Epoch 203/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5393 - mse: 652139.6250 - val_loss: 1.7557 - val_mae: 606.6732 - val_mse: 1016614.3750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 204/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5299 - mse: 652100.1250 - val_loss: 1.7553 - val_mae: 606.5856 - val_mse: 1016736.3125 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 205/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5125 - mse: 652004.0000 - val_loss: 1.7552 - val_mae: 606.5577 - val_mse: 1016771.6875 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 206/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5208 - mse: 652046.3750 - val_loss: 1.7554 - val_mae: 606.6035 - val_mse: 1016693.5000 - lr: 1.0000e-04 - 49ms/epoch - 16ms/step\n",
            "Epoch 207/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5281 - mse: 652090.3125 - val_loss: 1.7555 - val_mae: 606.6305 - val_mse: 1016646.8750 - lr: 1.0000e-04 - 40ms/epoch - 13ms/step\n",
            "Epoch 208/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5223 - mse: 652059.5000 - val_loss: 1.7553 - val_mae: 606.5881 - val_mse: 1016703.5625 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 209/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5156 - mse: 652027.5625 - val_loss: 1.7552 - val_mae: 606.5590 - val_mse: 1016741.3750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 210/300\n",
            "3/3 - 0s - loss: 2.5316 - mae: 352.5187 - mse: 652048.8125 - val_loss: 1.7553 - val_mae: 606.5831 - val_mse: 1016695.8125 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 211/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5222 - mse: 652068.0000 - val_loss: 1.7554 - val_mae: 606.5917 - val_mse: 1016679.0000 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 212/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5127 - mse: 652016.4375 - val_loss: 1.7552 - val_mae: 606.5603 - val_mse: 1016728.3750 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 213/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5123 - mse: 652007.8750 - val_loss: 1.7553 - val_mae: 606.5884 - val_mse: 1016679.0000 - lr: 1.0000e-04 - 30ms/epoch - 10ms/step\n",
            "Epoch 214/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5219 - mse: 652061.5625 - val_loss: 1.7555 - val_mae: 606.6282 - val_mse: 1016610.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 215/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5175 - mse: 652045.8750 - val_loss: 1.7553 - val_mae: 606.5845 - val_mse: 1016671.6250 - lr: 1.0000e-04 - 39ms/epoch - 13ms/step\n",
            "Epoch 216/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5172 - mse: 652044.3125 - val_loss: 1.7554 - val_mae: 606.6035 - val_mse: 1016634.5000 - lr: 1.0000e-04 - 32ms/epoch - 11ms/step\n",
            "Epoch 217/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5295 - mse: 652114.3750 - val_loss: 1.7556 - val_mae: 606.6495 - val_mse: 1016558.5625 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 218/300\n",
            "3/3 - 0s - loss: 2.5314 - mae: 352.5106 - mse: 652019.2500 - val_loss: 1.7551 - val_mae: 606.5340 - val_mse: 1016734.8125 - lr: 1.0000e-04 - 35ms/epoch - 12ms/step\n",
            "Epoch 219/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5051 - mse: 651981.3125 - val_loss: 1.7554 - val_mae: 606.5903 - val_mse: 1016638.5625 - lr: 1.0000e-04 - 50ms/epoch - 17ms/step\n",
            "Epoch 220/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5344 - mse: 652139.4375 - val_loss: 1.7558 - val_mae: 606.6825 - val_mse: 1016493.7500 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 221/300\n",
            "3/3 - 0s - loss: 2.5314 - mae: 352.5255 - mse: 652101.8750 - val_loss: 1.7554 - val_mae: 606.6066 - val_mse: 1016600.8125 - lr: 1.0000e-04 - 34ms/epoch - 11ms/step\n",
            "Epoch 222/300\n",
            "3/3 - 0s - loss: 2.5314 - mae: 352.5049 - mse: 651993.6250 - val_loss: 1.7550 - val_mae: 606.5284 - val_mse: 1016726.1875 - lr: 1.0000e-04 - 33ms/epoch - 11ms/step\n",
            "Epoch 223/300\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5026 - mse: 651977.3125 - val_loss: 1.7553 - val_mae: 606.5796 - val_mse: 1016645.7500 - lr: 1.0000e-04 - 31ms/epoch - 10ms/step\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 224: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "3/3 - 0s - loss: 2.5315 - mae: 352.5267 - mse: 652108.3750 - val_loss: 1.7557 - val_mae: 606.6706 - val_mse: 1016509.5000 - lr: 1.0000e-04 - 37ms/epoch - 12ms/step\n",
            "Epoch 225/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5106 - mse: 652034.3750 - val_loss: 1.7557 - val_mae: 606.6655 - val_mse: 1016515.6875 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 226/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5075 - mse: 652018.5625 - val_loss: 1.7556 - val_mae: 606.6512 - val_mse: 1016537.2500 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 227/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5029 - mse: 651994.9375 - val_loss: 1.7555 - val_mae: 606.6332 - val_mse: 1016563.5625 - lr: 1.0000e-05 - 30ms/epoch - 10ms/step\n",
            "Epoch 228/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4990 - mse: 651975.3125 - val_loss: 1.7555 - val_mae: 606.6205 - val_mse: 1016581.4375 - lr: 1.0000e-05 - 50ms/epoch - 17ms/step\n",
            "Epoch 229/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4978 - mse: 651969.9375 - val_loss: 1.7555 - val_mae: 606.6183 - val_mse: 1016584.0625 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 230/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4989 - mse: 651978.2500 - val_loss: 1.7555 - val_mae: 606.6241 - val_mse: 1016572.9375 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 231/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5011 - mse: 651992.8125 - val_loss: 1.7555 - val_mae: 606.6307 - val_mse: 1016560.8125 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 232/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5019 - mse: 651998.8125 - val_loss: 1.7555 - val_mae: 606.6318 - val_mse: 1016557.3750 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 233/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5012 - mse: 651995.3750 - val_loss: 1.7555 - val_mae: 606.6262 - val_mse: 1016564.2500 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 234/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4994 - mse: 651986.1875 - val_loss: 1.7555 - val_mae: 606.6207 - val_mse: 1016570.5000 - lr: 1.0000e-05 - 31ms/epoch - 10ms/step\n",
            "Epoch 235/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4991 - mse: 651984.9375 - val_loss: 1.7555 - val_mae: 606.6218 - val_mse: 1016567.9375 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 236/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5004 - mse: 651992.0000 - val_loss: 1.7555 - val_mae: 606.6268 - val_mse: 1016559.5625 - lr: 1.0000e-05 - 32ms/epoch - 11ms/step\n",
            "Epoch 237/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5009 - mse: 651995.3125 - val_loss: 1.7555 - val_mae: 606.6262 - val_mse: 1016560.0000 - lr: 1.0000e-05 - 51ms/epoch - 17ms/step\n",
            "Epoch 238/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5002 - mse: 651991.8125 - val_loss: 1.7555 - val_mae: 606.6225 - val_mse: 1016564.2500 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 239/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4993 - mse: 651987.5625 - val_loss: 1.7555 - val_mae: 606.6212 - val_mse: 1016564.8750 - lr: 1.0000e-05 - 32ms/epoch - 11ms/step\n",
            "Epoch 240/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5000 - mse: 651991.0625 - val_loss: 1.7555 - val_mae: 606.6243 - val_mse: 1016559.7500 - lr: 1.0000e-05 - 50ms/epoch - 17ms/step\n",
            "Epoch 241/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5008 - mse: 651995.1250 - val_loss: 1.7555 - val_mae: 606.6266 - val_mse: 1016555.5000 - lr: 1.0000e-05 - 41ms/epoch - 14ms/step\n",
            "Epoch 242/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5010 - mse: 651996.6875 - val_loss: 1.7555 - val_mae: 606.6259 - val_mse: 1016555.9375 - lr: 1.0000e-05 - 31ms/epoch - 10ms/step\n",
            "Epoch 243/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5001 - mse: 651992.4375 - val_loss: 1.7555 - val_mae: 606.6217 - val_mse: 1016560.8125 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 244/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4993 - mse: 651987.3750 - val_loss: 1.7555 - val_mae: 606.6210 - val_mse: 1016561.7500 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 245/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4997 - mse: 651989.5625 - val_loss: 1.7555 - val_mae: 606.6229 - val_mse: 1016558.0625 - lr: 1.0000e-05 - 51ms/epoch - 17ms/step\n",
            "Epoch 246/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4998 - mse: 651990.2500 - val_loss: 1.7555 - val_mae: 606.6224 - val_mse: 1016558.3750 - lr: 1.0000e-05 - 48ms/epoch - 16ms/step\n",
            "Epoch 247/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4994 - mse: 651987.7500 - val_loss: 1.7555 - val_mae: 606.6204 - val_mse: 1016560.3125 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 248/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4992 - mse: 651987.1875 - val_loss: 1.7555 - val_mae: 606.6210 - val_mse: 1016558.6875 - lr: 1.0000e-05 - 51ms/epoch - 17ms/step\n",
            "Epoch 249/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4999 - mse: 651991.0625 - val_loss: 1.7555 - val_mae: 606.6232 - val_mse: 1016554.0000 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 250/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4992 - mse: 651987.6250 - val_loss: 1.7555 - val_mae: 606.6165 - val_mse: 1016563.6250 - lr: 1.0000e-05 - 31ms/epoch - 10ms/step\n",
            "Epoch 251/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4969 - mse: 651974.3750 - val_loss: 1.7554 - val_mae: 606.6110 - val_mse: 1016571.0625 - lr: 1.0000e-05 - 31ms/epoch - 10ms/step\n",
            "Epoch 252/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4970 - mse: 651973.6875 - val_loss: 1.7555 - val_mae: 606.6149 - val_mse: 1016565.8750 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 253/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4991 - mse: 651985.7500 - val_loss: 1.7555 - val_mae: 606.6245 - val_mse: 1016550.8750 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 254/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.5007 - mse: 651994.7500 - val_loss: 1.7555 - val_mae: 606.6279 - val_mse: 1016546.1250 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 255/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4998 - mse: 651990.3125 - val_loss: 1.7555 - val_mae: 606.6219 - val_mse: 1016554.6875 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 256/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4974 - mse: 651977.3125 - val_loss: 1.7555 - val_mae: 606.6157 - val_mse: 1016564.4375 - lr: 1.0000e-05 - 57ms/epoch - 19ms/step\n",
            "Epoch 257/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4974 - mse: 651977.1250 - val_loss: 1.7555 - val_mae: 606.6186 - val_mse: 1016559.6875 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 258/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4983 - mse: 651983.1250 - val_loss: 1.7555 - val_mae: 606.6204 - val_mse: 1016557.3750 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 259/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4981 - mse: 651982.0625 - val_loss: 1.7555 - val_mae: 606.6204 - val_mse: 1016556.8750 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 260/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4983 - mse: 651982.7500 - val_loss: 1.7555 - val_mae: 606.6198 - val_mse: 1016557.5000 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 261/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4980 - mse: 651981.1875 - val_loss: 1.7555 - val_mae: 606.6212 - val_mse: 1016555.3125 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 262/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4985 - mse: 651984.5625 - val_loss: 1.7555 - val_mae: 606.6210 - val_mse: 1016556.3125 - lr: 1.0000e-05 - 50ms/epoch - 17ms/step\n",
            "Epoch 263/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4978 - mse: 651981.1250 - val_loss: 1.7555 - val_mae: 606.6200 - val_mse: 1016557.9375 - lr: 1.0000e-05 - 48ms/epoch - 16ms/step\n",
            "Epoch 264/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4977 - mse: 651980.4375 - val_loss: 1.7555 - val_mae: 606.6189 - val_mse: 1016560.5000 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 265/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4972 - mse: 651978.2500 - val_loss: 1.7555 - val_mae: 606.6189 - val_mse: 1016560.3125 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 266/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4973 - mse: 651978.6875 - val_loss: 1.7555 - val_mae: 606.6199 - val_mse: 1016558.5000 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 267/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4976 - mse: 651979.5625 - val_loss: 1.7555 - val_mae: 606.6203 - val_mse: 1016558.3125 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 268/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4976 - mse: 651980.0625 - val_loss: 1.7555 - val_mae: 606.6205 - val_mse: 1016558.8750 - lr: 1.0000e-05 - 32ms/epoch - 11ms/step\n",
            "Epoch 269/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4971 - mse: 651977.6875 - val_loss: 1.7555 - val_mae: 606.6191 - val_mse: 1016561.3750 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 270/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4969 - mse: 651976.6875 - val_loss: 1.7555 - val_mae: 606.6203 - val_mse: 1016560.7500 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 271/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4969 - mse: 651976.5625 - val_loss: 1.7555 - val_mae: 606.6199 - val_mse: 1016562.2500 - lr: 1.0000e-05 - 32ms/epoch - 11ms/step\n",
            "Epoch 272/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4967 - mse: 651975.5000 - val_loss: 1.7555 - val_mae: 606.6207 - val_mse: 1016560.6250 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 273/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4964 - mse: 651974.1875 - val_loss: 1.7555 - val_mae: 606.6179 - val_mse: 1016565.0625 - lr: 1.0000e-05 - 47ms/epoch - 16ms/step\n",
            "Epoch 274/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4964 - mse: 651974.0000 - val_loss: 1.7555 - val_mae: 606.6213 - val_mse: 1016559.8125 - lr: 1.0000e-05 - 41ms/epoch - 14ms/step\n",
            "Epoch 275/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4977 - mse: 651981.5000 - val_loss: 1.7555 - val_mae: 606.6242 - val_mse: 1016555.6875 - lr: 1.0000e-05 - 31ms/epoch - 10ms/step\n",
            "Epoch 276/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4975 - mse: 651980.7500 - val_loss: 1.7555 - val_mae: 606.6230 - val_mse: 1016559.0000 - lr: 1.0000e-05 - 39ms/epoch - 13ms/step\n",
            "Epoch 277/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4961 - mse: 651973.5625 - val_loss: 1.7555 - val_mae: 606.6191 - val_mse: 1016565.3750 - lr: 1.0000e-05 - 54ms/epoch - 18ms/step\n",
            "Epoch 278/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4961 - mse: 651972.8125 - val_loss: 1.7555 - val_mae: 606.6219 - val_mse: 1016562.5625 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Epoch 279/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4964 - mse: 651975.6250 - val_loss: 1.7555 - val_mae: 606.6224 - val_mse: 1016561.6250 - lr: 1.0000e-05 - 32ms/epoch - 11ms/step\n",
            "Epoch 280/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4960 - mse: 651972.7500 - val_loss: 1.7555 - val_mae: 606.6200 - val_mse: 1016565.3750 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 281/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4960 - mse: 651972.3750 - val_loss: 1.7555 - val_mae: 606.6226 - val_mse: 1016561.9375 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 282/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4966 - mse: 651975.8750 - val_loss: 1.7555 - val_mae: 606.6240 - val_mse: 1016561.0000 - lr: 1.0000e-05 - 33ms/epoch - 11ms/step\n",
            "Epoch 283/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4959 - mse: 651972.7500 - val_loss: 1.7555 - val_mae: 606.6222 - val_mse: 1016563.4375 - lr: 1.0000e-05 - 36ms/epoch - 12ms/step\n",
            "Epoch 284/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4962 - mse: 651973.8750 - val_loss: 1.7555 - val_mae: 606.6241 - val_mse: 1016561.4375 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 285/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4962 - mse: 651975.0000 - val_loss: 1.7555 - val_mae: 606.6219 - val_mse: 1016564.4375 - lr: 1.0000e-05 - 47ms/epoch - 16ms/step\n",
            "Epoch 286/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4952 - mse: 651970.0625 - val_loss: 1.7555 - val_mae: 606.6198 - val_mse: 1016567.6875 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 287/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4958 - mse: 651973.3750 - val_loss: 1.7555 - val_mae: 606.6246 - val_mse: 1016560.9375 - lr: 1.0000e-05 - 37ms/epoch - 12ms/step\n",
            "Epoch 288/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4963 - mse: 651976.3750 - val_loss: 1.7555 - val_mae: 606.6242 - val_mse: 1016561.3125 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 289/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4957 - mse: 651973.0625 - val_loss: 1.7555 - val_mae: 606.6223 - val_mse: 1016564.0625 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 290/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4952 - mse: 651970.3125 - val_loss: 1.7555 - val_mae: 606.6218 - val_mse: 1016565.3750 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 291/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4955 - mse: 651972.0000 - val_loss: 1.7555 - val_mae: 606.6246 - val_mse: 1016561.5625 - lr: 1.0000e-05 - 51ms/epoch - 17ms/step\n",
            "Epoch 292/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4963 - mse: 651976.3125 - val_loss: 1.7555 - val_mae: 606.6260 - val_mse: 1016559.7500 - lr: 1.0000e-05 - 49ms/epoch - 16ms/step\n",
            "Epoch 293/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4957 - mse: 651974.1250 - val_loss: 1.7555 - val_mae: 606.6228 - val_mse: 1016563.8750 - lr: 1.0000e-05 - 46ms/epoch - 15ms/step\n",
            "Epoch 294/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4951 - mse: 651970.9375 - val_loss: 1.7555 - val_mae: 606.6223 - val_mse: 1016563.3750 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 295/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4960 - mse: 651975.8750 - val_loss: 1.7555 - val_mae: 606.6248 - val_mse: 1016559.5625 - lr: 1.0000e-05 - 34ms/epoch - 11ms/step\n",
            "Epoch 296/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4960 - mse: 651975.8125 - val_loss: 1.7555 - val_mae: 606.6237 - val_mse: 1016562.0000 - lr: 1.0000e-05 - 49ms/epoch - 16ms/step\n",
            "Epoch 297/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4949 - mse: 651970.4375 - val_loss: 1.7555 - val_mae: 606.6231 - val_mse: 1016565.0000 - lr: 1.0000e-05 - 32ms/epoch - 11ms/step\n",
            "Epoch 298/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4957 - mse: 651974.3750 - val_loss: 1.7555 - val_mae: 606.6277 - val_mse: 1016558.5625 - lr: 1.0000e-05 - 43ms/epoch - 14ms/step\n",
            "Epoch 299/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4958 - mse: 651975.6875 - val_loss: 1.7555 - val_mae: 606.6265 - val_mse: 1016560.5625 - lr: 1.0000e-05 - 41ms/epoch - 14ms/step\n",
            "Epoch 300/300\n",
            "3/3 - 0s - loss: 2.5313 - mae: 352.4947 - mse: 651969.6250 - val_loss: 1.7555 - val_mae: 606.6229 - val_mse: 1016565.6875 - lr: 1.0000e-05 - 35ms/epoch - 12ms/step\n",
            "Optimizing model by reducing: mae for epochs: 300, num_iter: 3, model: NBEATS_model\n",
            "Epoch 1/300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate train pred"
      ],
      "metadata": {
        "id": "-nbwE3JDYzRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create AR model\n",
        "AR_model = get_AR_model(len(train_windows), WINDOW_SIZE_WEEK + 1)\n",
        "\n",
        "# Create NBEATS model\n",
        "NBEATS_model = get_NBEATS_model()\n",
        "\n",
        "# Create LNRNN model\n",
        "LNRNN_model = get_LNRNN_model()\n",
        "\n",
        "# Create LSTM model\n",
        "LSTM_model_obj = LSTMModel()\n",
        "LSTM_model = LSTM_model_obj.get_model()\n",
        "\n",
        "# Create Dense model\n",
        "dense_model_obj = DenseModel()\n",
        "dense_model = dense_model_obj.get_model()\n",
        "\n",
        "train_models = [NBEATS_model, LSTM_model, dense_model]\n",
        "# train_models = [NBEATS_model, LNRNN_model, LSTM_model, dense_model]\n",
        "# train_models = [LNRNN_model]\n",
        "\n",
        "# Obtain list of trained ensemble models\n",
        "ensemble_models = get_ensemble_models(models=train_models, train_data=train_dataset, test_data=test_dataset,\n",
        "                                      num_iter=ENSEMBLE_NUM_ITER, num_epochs=ENSEMBLE_NUM_EPOCHS)\n",
        "\n",
        "# Generate model summaries\n",
        "get_ensemble_models_summary(models=train_models)\n",
        "\n",
        "# Plot the ensemble models\n",
        "# plot_model(AR_model)\n",
        "\n",
        "# plot_model(NBEATS_model)\n",
        "\n",
        "# plot_model(LNRNN_model)\n",
        "\n",
        "# plot_model(LSTM_model)\n",
        "\n",
        "# plot_model(dense_model)\n",
        "\n",
        "# Generate ensemble predictions\n",
        "ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models, input_data=test_dataset)\n",
        "\n",
        "# Evaluate ensemble model predictions\n",
        "ensemble_results = evaluate_preds(y_true=test_labels, y_pred=np.median(ensemble_preds, axis=0))\n",
        "print(ensemble_results)\n",
        "\n",
        "# Obtain the upper and lower bounds of the 95% confidence levels\n",
        "lower, upper = get_upper_lower_confidence(preds=ensemble_preds)\n",
        "\n",
        "# Get the median values of the ensemble preds\n",
        "ensemble_median = np.median(ensemble_preds, axis=0)\n",
        "\n",
        "# Plot the confidence interval\n",
        "# plot_confidence_interval(test_windows, test_labels, ensemble_median, lower=lower, upper=upper, offset=300)\n",
        "\n",
        "# Make forecasts into future of the price of bitcoin\n",
        "future_forecast = make_future_forecast(models=ensemble_models, values=dataset_full_labels_br,\n",
        "                                       into_future=INTO_FUTURE_2_WEEK, window_size=WINDOW_SIZE_WEEK + 1)\n",
        "\n",
        "# plot_future_forecast(df_btc_price=df_btc_price_closing, future_forecast=future_forecast)\n",
        "print(f\"Future forecast: {future_forecast}\")"
      ],
      "metadata": {
        "id": "t5DzqmzcY1Tp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}